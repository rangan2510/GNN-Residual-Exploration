{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "class args:\n",
    "    num_layers = 5\n",
    "    device = 'cuda:0'\n",
    "    log_steps = 1\n",
    "    hidden_channels = 64*4\n",
    "    dropout = 0.2\n",
    "    lr = 0.01\n",
    "    epochs = 1000\n",
    "    eval_steps = 10\n",
    "    runs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import humanize\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\r\n",
      "  Downloading ogb-1.2.1-py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 1.2 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.1)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.18.5)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.0.3)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.1)\r\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Installing collected packages: ogb\r\n",
      "Successfully installed ogb-1.2.1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-scatter==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 24.9 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\r\n",
      "Successfully installed torch-scatter-2.0.5\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-sparse==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.6 MB 12.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==latest+cu101) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==latest+cu101) (1.18.5)\r\n",
      "Installing collected packages: torch-sparse\r\n",
      "Successfully installed torch-sparse-0.6.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-cluster==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.3 MB 12.8 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-cluster\r\n",
      "Successfully installed torch-cluster-1.5.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-spline-conv==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 13.3 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\r\n",
      "Successfully installed torch-spline-conv-1.2.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-1.6.0.tar.gz (172 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 172 kB 7.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.5.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.18.5)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (4.45.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.4.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.23.1)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.48.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.23.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.0.3)\r\n",
      "Collecting rdflib\r\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 231 kB 15.7 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.10.0)\r\n",
      "Collecting googledrivedownloader\r\n",
      "  Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB)\r\n",
      "Collecting ase\r\n",
      "  Downloading ase-3.19.2-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 21.0 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.11.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torch-geometric) (0.18.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->torch-geometric) (4.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (0.14.1)\r\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (0.31.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (1.24.3)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2020.6.20)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (3.0.4)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2.8.1)\r\n",
      "Collecting isodate\r\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.6 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (1.14.0)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (2.4.7)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ase->torch-geometric) (3.2.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric) (1.1.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (0.10.0)\r\n",
      "Building wheels for collected packages: torch-geometric\r\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-1.6.0-py3-none-any.whl size=296336 sha256=52914d4bcd15222b616a1fbe4233c9ac2c20c58835c63bb1e4aecd69ab326074\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/69/d6/b8ed45222466048e1efc27af604aded6825217f0caa6dff569\r\n",
      "Successfully built torch-geometric\r\n",
      "Installing collected packages: isodate, rdflib, googledrivedownloader, ase, torch-geometric\r\n",
      "Successfully installed ase-3.19.2 googledrivedownloader-0.4 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# install Open Graph Benchmark\n",
    "! pip install ogb\n",
    "\n",
    "# install PyTorch Geometric\n",
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR,CosineAnnealingLR\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://snap.stanford.edu/ogb/data/nodeproppred/proteinfunc.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:14<00:00, 14.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/proteinfunc.zip\n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 384.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n",
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = f'{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-proteins',\n",
    "                                 transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "\n",
    "# Move edge features to node features.\n",
    "data.x = data.adj_t.mean(dim=1)\n",
    "data.adj_t.set_value_(None)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train'].to(device)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 minutes\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            GCNConv(in_channels, hidden_channels, normalize=False))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, normalize=False))\n",
    "        self.convs.append(\n",
    "            GCNConv(hidden_channels*2, out_channels, normalize=False))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        x = self.convs[0](x, adj_t)\n",
    "        x_ = x\n",
    "        for conv in self.convs[1:-1]:\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = torch.cat([x_,x],1)    \n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = criterion(out, data.y[train_idx].to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(data.x, data.adj_t)\n",
    "\n",
    "    train_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['rocauc']\n",
    "    valid_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['rocauc']\n",
    "    test_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['rocauc']\n",
    "\n",
    "    return train_rocauc, valid_rocauc, test_rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed =  0\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5319\t Train: 37.78%\t Valid: 35.44%\tTest: 37.91%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4211\t Train: 36.71%\t Valid: 33.45%\tTest: 35.85%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4018\t Train: 39.05%\t Valid: 33.44%\tTest: 34.93%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3900\t Train: 43.59%\t Valid: 34.99%\tTest: 35.17%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3792\t Train: 48.62%\t Valid: 36.14%\tTest: 35.75%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3733\t Train: 52.07%\t Valid: 38.32%\tTest: 37.14%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3670\t Train: 54.18%\t Valid: 40.72%\tTest: 38.41%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3628\t Train: 56.26%\t Valid: 44.23%\tTest: 40.50%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3552\t Train: 58.44%\t Valid: 48.41%\tTest: 42.91%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3517\t Train: 59.78%\t Valid: 51.96%\tTest: 43.58%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3484\t Train: 61.41%\t Valid: 52.65%\tTest: 45.82%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3418\t Train: 62.88%\t Valid: 55.85%\tTest: 46.93%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3369\t Train: 64.94%\t Valid: 60.43%\tTest: 49.33%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3333\t Train: 65.23%\t Valid: 60.67%\tTest: 49.45%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3316\t Train: 66.65%\t Valid: 62.49%\tTest: 51.22%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3291\t Train: 68.49%\t Valid: 65.17%\tTest: 53.79%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3268\t Train: 68.62%\t Valid: 65.45%\tTest: 54.04%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3247\t Train: 69.17%\t Valid: 67.05%\tTest: 55.05%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3244\t Train: 67.90%\t Valid: 62.96%\tTest: 53.57%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3227\t Train: 69.95%\t Valid: 67.90%\tTest: 56.44%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3205\t Train: 69.11%\t Valid: 66.49%\tTest: 55.83%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3189\t Train: 70.88%\t Valid: 69.71%\tTest: 58.54%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3175\t Train: 70.60%\t Valid: 68.99%\tTest: 58.84%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3169\t Train: 71.85%\t Valid: 70.69%\tTest: 60.23%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3152\t Train: 71.89%\t Valid: 70.66%\tTest: 60.38%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3131\t Train: 72.68%\t Valid: 71.94%\tTest: 61.79%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3111\t Train: 72.22%\t Valid: 71.14%\tTest: 61.55%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3114\t Train: 72.94%\t Valid: 72.23%\tTest: 62.27%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3085\t Train: 74.31%\t Valid: 73.39%\tTest: 64.20%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3088\t Train: 73.85%\t Valid: 72.56%\tTest: 63.37%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3104\t Train: 73.00%\t Valid: 71.54%\tTest: 62.13%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3077\t Train: 73.98%\t Valid: 72.41%\tTest: 63.64%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3066\t Train: 75.31%\t Valid: 74.19%\tTest: 65.54%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3033\t Train: 75.44%\t Valid: 74.33%\tTest: 65.83%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3018\t Train: 75.81%\t Valid: 74.82%\tTest: 66.67%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3032\t Train: 76.15%\t Valid: 74.93%\tTest: 66.68%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3000\t Train: 76.37%\t Valid: 75.26%\tTest: 67.67%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2981\t Train: 76.98%\t Valid: 75.81%\tTest: 67.98%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2995\t Train: 75.96%\t Valid: 75.56%\tTest: 68.08%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2974\t Train: 77.20%\t Valid: 76.22%\tTest: 68.77%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2952\t Train: 77.67%\t Valid: 76.34%\tTest: 68.85%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2957\t Train: 77.99%\t Valid: 76.51%\tTest: 69.01%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2948\t Train: 78.30%\t Valid: 76.87%\tTest: 69.93%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2926\t Train: 78.23%\t Valid: 76.85%\tTest: 69.66%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2909\t Train: 78.81%\t Valid: 77.30%\tTest: 70.31%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2898\t Train: 79.04%\t Valid: 77.54%\tTest: 70.67%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2880\t Train: 79.61%\t Valid: 78.00%\tTest: 71.46%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2909\t Train: 79.24%\t Valid: 77.96%\tTest: 71.49%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2875\t Train: 79.27%\t Valid: 77.81%\tTest: 70.90%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2863\t Train: 80.18%\t Valid: 78.54%\tTest: 72.09%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2844\t Train: 80.35%\t Valid: 78.51%\tTest: 71.88%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2872\t Train: 79.37%\t Valid: 77.53%\tTest: 71.14%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2847\t Train: 80.42%\t Valid: 78.67%\tTest: 72.61%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2824\t Train: 80.77%\t Valid: 78.87%\tTest: 72.70%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2839\t Train: 80.13%\t Valid: 78.32%\tTest: 71.87%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2800\t Train: 80.88%\t Valid: 78.72%\tTest: 72.41%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2801\t Train: 80.67%\t Valid: 78.70%\tTest: 72.05%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2792\t Train: 81.62%\t Valid: 79.22%\tTest: 72.91%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2794\t Train: 81.51%\t Valid: 79.54%\tTest: 73.35%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2772\t Train: 81.74%\t Valid: 79.31%\tTest: 72.16%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2797\t Train: 81.56%\t Valid: 78.88%\tTest: 72.22%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2784\t Train: 81.73%\t Valid: 79.13%\tTest: 72.79%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2751\t Train: 81.82%\t Valid: 79.57%\tTest: 73.10%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2778\t Train: 81.95%\t Valid: 79.10%\tTest: 71.50%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2757\t Train: 82.12%\t Valid: 79.59%\tTest: 72.28%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2742\t Train: 81.95%\t Valid: 78.94%\tTest: 71.30%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2736\t Train: 82.46%\t Valid: 80.15%\tTest: 73.02%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2736\t Train: 82.19%\t Valid: 79.18%\tTest: 71.37%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2722\t Train: 82.46%\t Valid: 79.60%\tTest: 71.97%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2709\t Train: 82.96%\t Valid: 80.08%\tTest: 72.54%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2704\t Train: 82.76%\t Valid: 79.73%\tTest: 71.29%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2749\t Train: 82.49%\t Valid: 79.26%\tTest: 71.24%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2742\t Train: 82.88%\t Valid: 79.59%\tTest: 71.03%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2690\t Train: 82.83%\t Valid: 79.62%\tTest: 70.84%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2688\t Train: 83.33%\t Valid: 80.09%\tTest: 71.47%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2693\t Train: 82.98%\t Valid: 79.73%\tTest: 70.88%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2677\t Train: 83.35%\t Valid: 80.45%\tTest: 71.91%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2668\t Train: 83.63%\t Valid: 80.31%\tTest: 70.69%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2867\t Train: 81.96%\t Valid: 79.36%\tTest: 72.18%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2734\t Train: 82.80%\t Valid: 80.04%\tTest: 73.26%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2692\t Train: 82.93%\t Valid: 80.36%\tTest: 73.46%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2666\t Train: 83.58%\t Valid: 80.39%\tTest: 72.74%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2658\t Train: 83.34%\t Valid: 80.03%\tTest: 71.75%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2657\t Train: 83.70%\t Valid: 80.55%\tTest: 71.83%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2659\t Train: 83.60%\t Valid: 80.24%\tTest: 70.99%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2655\t Train: 83.90%\t Valid: 80.48%\tTest: 70.80%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2641\t Train: 83.77%\t Valid: 80.06%\tTest: 70.22%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2635\t Train: 84.05%\t Valid: 80.38%\tTest: 70.24%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2637\t Train: 84.36%\t Valid: 80.58%\tTest: 70.05%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2634\t Train: 84.01%\t Valid: 79.95%\tTest: 69.24%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2613\t Train: 83.95%\t Valid: 80.55%\tTest: 71.23%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2623\t Train: 84.26%\t Valid: 80.60%\tTest: 70.71%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2628\t Train: 84.13%\t Valid: 80.78%\tTest: 71.37%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2602\t Train: 84.15%\t Valid: 80.63%\tTest: 70.95%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2626\t Train: 84.49%\t Valid: 81.18%\tTest: 71.77%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2634\t Train: 84.44%\t Valid: 80.38%\tTest: 70.05%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2603\t Train: 84.27%\t Valid: 80.37%\tTest: 70.34%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2606\t Train: 84.56%\t Valid: 79.84%\tTest: 68.85%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2594\t Train: 84.35%\t Valid: 80.02%\tTest: 70.15%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2593\t Train: 84.84%\t Valid: 80.68%\tTest: 70.27%\n",
      "Seed =  1\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4699\t Train: 37.76%\t Valid: 35.28%\tTest: 37.98%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4221\t Train: 37.52%\t Valid: 33.41%\tTest: 35.57%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3990\t Train: 39.67%\t Valid: 33.73%\tTest: 34.94%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3905\t Train: 42.35%\t Valid: 34.86%\tTest: 35.16%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3808\t Train: 46.76%\t Valid: 35.85%\tTest: 35.13%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3721\t Train: 53.88%\t Valid: 39.53%\tTest: 37.34%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3646\t Train: 54.12%\t Valid: 41.17%\tTest: 38.25%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3572\t Train: 55.99%\t Valid: 44.63%\tTest: 40.43%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3529\t Train: 56.87%\t Valid: 45.57%\tTest: 41.44%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3462\t Train: 61.48%\t Valid: 50.81%\tTest: 44.42%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3383\t Train: 64.84%\t Valid: 57.02%\tTest: 48.68%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3392\t Train: 65.51%\t Valid: 57.93%\tTest: 49.73%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3320\t Train: 67.22%\t Valid: 60.09%\tTest: 51.45%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3275\t Train: 68.83%\t Valid: 64.01%\tTest: 53.91%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3266\t Train: 67.17%\t Valid: 60.96%\tTest: 51.74%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3235\t Train: 69.01%\t Valid: 64.92%\tTest: 54.90%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3204\t Train: 70.05%\t Valid: 67.69%\tTest: 57.14%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3185\t Train: 71.10%\t Valid: 69.46%\tTest: 58.56%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3163\t Train: 71.19%\t Valid: 68.82%\tTest: 58.13%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3320\t Train: 70.55%\t Valid: 68.27%\tTest: 58.50%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3303\t Train: 67.84%\t Valid: 64.07%\tTest: 53.19%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3378\t Train: 66.45%\t Valid: 61.18%\tTest: 51.77%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3224\t Train: 70.55%\t Valid: 66.71%\tTest: 56.26%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3187\t Train: 70.40%\t Valid: 66.77%\tTest: 56.08%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3170\t Train: 71.22%\t Valid: 67.79%\tTest: 57.18%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3151\t Train: 70.89%\t Valid: 68.00%\tTest: 57.47%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3144\t Train: 70.93%\t Valid: 68.03%\tTest: 57.80%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3147\t Train: 72.78%\t Valid: 70.02%\tTest: 60.31%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3130\t Train: 72.88%\t Valid: 70.46%\tTest: 60.86%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3119\t Train: 72.94%\t Valid: 70.67%\tTest: 61.06%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3118\t Train: 73.14%\t Valid: 71.01%\tTest: 61.58%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3092\t Train: 72.80%\t Valid: 71.00%\tTest: 61.30%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3078\t Train: 72.68%\t Valid: 70.87%\tTest: 62.01%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3064\t Train: 74.36%\t Valid: 73.08%\tTest: 64.15%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3040\t Train: 74.64%\t Valid: 73.97%\tTest: 65.02%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3027\t Train: 75.33%\t Valid: 74.65%\tTest: 66.06%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3007\t Train: 75.59%\t Valid: 75.28%\tTest: 67.22%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2992\t Train: 76.37%\t Valid: 75.97%\tTest: 68.48%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3011\t Train: 75.47%\t Valid: 74.89%\tTest: 67.81%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2983\t Train: 76.52%\t Valid: 75.58%\tTest: 68.59%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2963\t Train: 76.95%\t Valid: 76.12%\tTest: 69.35%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2952\t Train: 77.59%\t Valid: 76.88%\tTest: 70.07%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2973\t Train: 77.14%\t Valid: 75.95%\tTest: 69.98%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2929\t Train: 77.85%\t Valid: 76.35%\tTest: 70.08%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2944\t Train: 78.18%\t Valid: 76.85%\tTest: 70.46%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2910\t Train: 78.81%\t Valid: 77.31%\tTest: 70.82%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2903\t Train: 78.38%\t Valid: 77.05%\tTest: 71.17%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2884\t Train: 78.83%\t Valid: 77.20%\tTest: 71.29%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2876\t Train: 79.36%\t Valid: 77.54%\tTest: 71.68%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2886\t Train: 79.30%\t Valid: 77.59%\tTest: 72.08%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2863\t Train: 79.34%\t Valid: 77.41%\tTest: 71.78%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2841\t Train: 80.11%\t Valid: 78.45%\tTest: 72.53%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2840\t Train: 80.36%\t Valid: 78.59%\tTest: 72.82%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2833\t Train: 80.23%\t Valid: 77.81%\tTest: 72.27%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2819\t Train: 80.58%\t Valid: 78.76%\tTest: 72.98%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2819\t Train: 80.71%\t Valid: 78.36%\tTest: 72.75%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2823\t Train: 81.15%\t Valid: 79.26%\tTest: 73.64%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2812\t Train: 81.18%\t Valid: 79.01%\tTest: 73.19%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2798\t Train: 80.94%\t Valid: 78.92%\tTest: 72.94%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2835\t Train: 81.24%\t Valid: 78.83%\tTest: 73.18%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2786\t Train: 81.45%\t Valid: 79.07%\tTest: 73.29%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2811\t Train: 81.29%\t Valid: 79.81%\tTest: 73.93%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2850\t Train: 80.99%\t Valid: 78.66%\tTest: 72.71%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2783\t Train: 81.48%\t Valid: 79.18%\tTest: 72.93%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2763\t Train: 81.83%\t Valid: 79.15%\tTest: 73.60%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2761\t Train: 81.93%\t Valid: 78.76%\tTest: 73.27%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2747\t Train: 82.14%\t Valid: 78.85%\tTest: 73.11%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2725\t Train: 82.36%\t Valid: 79.25%\tTest: 73.57%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2746\t Train: 82.36%\t Valid: 79.77%\tTest: 74.17%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2725\t Train: 82.71%\t Valid: 79.21%\tTest: 73.38%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2726\t Train: 82.73%\t Valid: 79.50%\tTest: 73.60%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2691\t Train: 82.91%\t Valid: 79.23%\tTest: 73.61%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2726\t Train: 82.90%\t Valid: 79.22%\tTest: 73.63%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2700\t Train: 83.24%\t Valid: 79.80%\tTest: 74.29%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2688\t Train: 83.00%\t Valid: 79.74%\tTest: 74.04%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2683\t Train: 83.04%\t Valid: 79.28%\tTest: 73.66%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2693\t Train: 82.94%\t Valid: 78.92%\tTest: 72.87%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2702\t Train: 83.30%\t Valid: 79.83%\tTest: 74.10%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2674\t Train: 83.53%\t Valid: 79.81%\tTest: 74.04%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2659\t Train: 83.08%\t Valid: 78.81%\tTest: 72.88%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2655\t Train: 83.64%\t Valid: 80.15%\tTest: 74.45%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2651\t Train: 83.69%\t Valid: 79.99%\tTest: 73.88%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2717\t Train: 83.77%\t Valid: 80.44%\tTest: 74.74%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2682\t Train: 83.68%\t Valid: 79.33%\tTest: 73.18%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2654\t Train: 83.53%\t Valid: 80.31%\tTest: 74.45%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2658\t Train: 83.93%\t Valid: 80.20%\tTest: 74.61%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2638\t Train: 84.15%\t Valid: 80.14%\tTest: 74.24%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2654\t Train: 84.13%\t Valid: 80.04%\tTest: 74.40%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2635\t Train: 84.13%\t Valid: 79.94%\tTest: 73.69%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2631\t Train: 83.86%\t Valid: 79.65%\tTest: 72.79%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2731\t Train: 83.08%\t Valid: 80.35%\tTest: 73.82%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2642\t Train: 84.12%\t Valid: 80.64%\tTest: 74.70%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2620\t Train: 84.24%\t Valid: 80.30%\tTest: 74.30%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2622\t Train: 84.16%\t Valid: 79.51%\tTest: 73.27%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2659\t Train: 83.95%\t Valid: 79.90%\tTest: 73.50%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2614\t Train: 84.38%\t Valid: 80.71%\tTest: 74.54%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2617\t Train: 84.48%\t Valid: 80.37%\tTest: 74.17%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2606\t Train: 84.47%\t Valid: 80.27%\tTest: 73.81%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2604\t Train: 84.43%\t Valid: 80.08%\tTest: 72.99%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2619\t Train: 84.47%\t Valid: 80.45%\tTest: 74.47%\n",
      "Seed =  2\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5307\t Train: 38.53%\t Valid: 36.25%\tTest: 38.29%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4164\t Train: 36.39%\t Valid: 33.46%\tTest: 35.85%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4037\t Train: 38.26%\t Valid: 33.65%\tTest: 35.31%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3930\t Train: 41.27%\t Valid: 34.84%\tTest: 35.15%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3857\t Train: 42.99%\t Valid: 35.08%\tTest: 34.77%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3790\t Train: 45.91%\t Valid: 36.33%\tTest: 35.06%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3705\t Train: 50.55%\t Valid: 38.78%\tTest: 36.34%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3631\t Train: 54.85%\t Valid: 42.18%\tTest: 38.63%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3557\t Train: 58.73%\t Valid: 47.68%\tTest: 41.91%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3498\t Train: 62.45%\t Valid: 53.37%\tTest: 45.49%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3437\t Train: 62.29%\t Valid: 53.20%\tTest: 45.19%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3444\t Train: 60.01%\t Valid: 51.06%\tTest: 44.56%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3377\t Train: 63.50%\t Valid: 54.94%\tTest: 46.93%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3344\t Train: 65.59%\t Valid: 58.16%\tTest: 49.44%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3313\t Train: 66.59%\t Valid: 59.83%\tTest: 50.92%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3350\t Train: 65.96%\t Valid: 59.04%\tTest: 50.54%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3259\t Train: 69.10%\t Valid: 64.08%\tTest: 55.23%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3235\t Train: 68.97%\t Valid: 64.22%\tTest: 55.12%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3216\t Train: 69.40%\t Valid: 65.41%\tTest: 56.03%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3190\t Train: 69.91%\t Valid: 66.61%\tTest: 57.16%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3174\t Train: 70.56%\t Valid: 67.78%\tTest: 58.27%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3172\t Train: 71.20%\t Valid: 68.59%\tTest: 59.30%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3145\t Train: 71.29%\t Valid: 68.99%\tTest: 59.47%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3128\t Train: 71.81%\t Valid: 69.99%\tTest: 60.28%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3138\t Train: 72.41%\t Valid: 70.01%\tTest: 60.70%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3109\t Train: 72.55%\t Valid: 70.52%\tTest: 61.27%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3096\t Train: 73.28%\t Valid: 71.93%\tTest: 62.64%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3080\t Train: 73.68%\t Valid: 72.40%\tTest: 63.12%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3071\t Train: 74.14%\t Valid: 72.85%\tTest: 63.73%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3061\t Train: 73.43%\t Valid: 71.98%\tTest: 62.93%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3053\t Train: 75.29%\t Valid: 74.28%\tTest: 65.46%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3062\t Train: 75.22%\t Valid: 74.06%\tTest: 65.25%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3029\t Train: 75.41%\t Valid: 74.23%\tTest: 65.54%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3035\t Train: 76.43%\t Valid: 75.57%\tTest: 67.12%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3017\t Train: 75.83%\t Valid: 74.06%\tTest: 65.77%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3025\t Train: 77.37%\t Valid: 76.11%\tTest: 68.20%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2991\t Train: 76.94%\t Valid: 75.43%\tTest: 67.30%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2974\t Train: 76.58%\t Valid: 75.00%\tTest: 67.07%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2965\t Train: 77.63%\t Valid: 76.31%\tTest: 68.58%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2944\t Train: 77.93%\t Valid: 76.48%\tTest: 68.83%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2965\t Train: 76.57%\t Valid: 75.58%\tTest: 68.19%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2934\t Train: 78.51%\t Valid: 76.66%\tTest: 69.24%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2929\t Train: 78.38%\t Valid: 76.90%\tTest: 69.30%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2927\t Train: 78.68%\t Valid: 76.94%\tTest: 69.49%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2897\t Train: 78.59%\t Valid: 76.83%\tTest: 69.54%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2891\t Train: 79.21%\t Valid: 77.32%\tTest: 70.48%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2966\t Train: 78.49%\t Valid: 77.16%\tTest: 70.56%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2918\t Train: 79.39%\t Valid: 77.46%\tTest: 70.73%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2890\t Train: 79.24%\t Valid: 76.99%\tTest: 70.23%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2864\t Train: 80.10%\t Valid: 77.71%\tTest: 71.58%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2857\t Train: 79.71%\t Valid: 77.36%\tTest: 71.02%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2859\t Train: 80.32%\t Valid: 78.23%\tTest: 72.08%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2828\t Train: 80.11%\t Valid: 77.45%\tTest: 70.95%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2836\t Train: 80.81%\t Valid: 78.55%\tTest: 72.82%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2827\t Train: 80.67%\t Valid: 78.05%\tTest: 72.30%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2860\t Train: 80.63%\t Valid: 78.05%\tTest: 72.20%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2813\t Train: 80.72%\t Valid: 78.30%\tTest: 72.28%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2784\t Train: 81.33%\t Valid: 78.52%\tTest: 72.70%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2869\t Train: 80.30%\t Valid: 77.88%\tTest: 71.06%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2816\t Train: 80.54%\t Valid: 78.46%\tTest: 71.91%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2781\t Train: 81.39%\t Valid: 78.63%\tTest: 72.61%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2802\t Train: 81.26%\t Valid: 79.00%\tTest: 73.45%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2766\t Train: 81.68%\t Valid: 78.81%\tTest: 73.19%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2766\t Train: 81.97%\t Valid: 79.02%\tTest: 72.95%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2748\t Train: 81.94%\t Valid: 78.95%\tTest: 72.67%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2736\t Train: 82.31%\t Valid: 78.72%\tTest: 72.34%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2741\t Train: 82.12%\t Valid: 78.22%\tTest: 71.06%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2731\t Train: 82.02%\t Valid: 79.21%\tTest: 71.71%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2737\t Train: 82.31%\t Valid: 78.98%\tTest: 72.67%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2804\t Train: 82.21%\t Valid: 78.31%\tTest: 70.25%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2715\t Train: 82.43%\t Valid: 78.23%\tTest: 71.49%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2719\t Train: 82.89%\t Valid: 79.31%\tTest: 72.84%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2695\t Train: 83.10%\t Valid: 78.32%\tTest: 71.48%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2705\t Train: 82.74%\t Valid: 77.85%\tTest: 71.57%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2781\t Train: 81.86%\t Valid: 79.80%\tTest: 72.92%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2716\t Train: 82.99%\t Valid: 79.40%\tTest: 72.49%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2721\t Train: 83.03%\t Valid: 79.10%\tTest: 71.80%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2682\t Train: 83.11%\t Valid: 79.05%\tTest: 71.37%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2673\t Train: 83.17%\t Valid: 79.15%\tTest: 71.48%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2694\t Train: 82.37%\t Valid: 78.51%\tTest: 70.74%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2700\t Train: 83.42%\t Valid: 78.78%\tTest: 70.44%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2669\t Train: 83.39%\t Valid: 78.66%\tTest: 70.75%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2681\t Train: 83.77%\t Valid: 79.16%\tTest: 70.83%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2666\t Train: 83.65%\t Valid: 78.98%\tTest: 70.79%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2657\t Train: 83.55%\t Valid: 78.93%\tTest: 70.94%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2652\t Train: 83.49%\t Valid: 79.51%\tTest: 71.01%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2642\t Train: 83.76%\t Valid: 78.61%\tTest: 69.76%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2646\t Train: 83.72%\t Valid: 79.11%\tTest: 69.23%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2629\t Train: 84.13%\t Valid: 78.64%\tTest: 70.15%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2643\t Train: 84.36%\t Valid: 79.42%\tTest: 70.60%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2657\t Train: 83.98%\t Valid: 78.85%\tTest: 69.80%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2605\t Train: 84.06%\t Valid: 78.50%\tTest: 69.48%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2612\t Train: 84.27%\t Valid: 79.90%\tTest: 70.29%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2651\t Train: 83.84%\t Valid: 79.25%\tTest: 68.41%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2612\t Train: 84.31%\t Valid: 78.56%\tTest: 69.23%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2615\t Train: 84.20%\t Valid: 78.35%\tTest: 69.14%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2590\t Train: 84.71%\t Valid: 79.42%\tTest: 70.04%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2579\t Train: 84.74%\t Valid: 79.01%\tTest: 69.37%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2869\t Train: 82.51%\t Valid: 79.02%\tTest: 69.51%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2706\t Train: 83.74%\t Valid: 78.90%\tTest: 70.73%\n",
      "Seed =  3\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5204\t Train: 38.43%\t Valid: 35.60%\tTest: 37.94%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4179\t Train: 36.87%\t Valid: 33.35%\tTest: 35.61%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4001\t Train: 38.91%\t Valid: 33.29%\tTest: 34.64%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3906\t Train: 42.69%\t Valid: 34.85%\tTest: 35.00%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3779\t Train: 50.95%\t Valid: 37.09%\tTest: 35.80%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3693\t Train: 53.41%\t Valid: 40.23%\tTest: 37.57%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3625\t Train: 55.53%\t Valid: 43.64%\tTest: 39.48%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3551\t Train: 58.34%\t Valid: 48.04%\tTest: 41.59%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3479\t Train: 57.49%\t Valid: 47.31%\tTest: 40.83%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3433\t Train: 63.80%\t Valid: 54.88%\tTest: 46.27%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3395\t Train: 66.18%\t Valid: 60.09%\tTest: 49.45%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3377\t Train: 66.67%\t Valid: 61.16%\tTest: 50.21%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3354\t Train: 67.24%\t Valid: 62.43%\tTest: 51.12%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3312\t Train: 67.77%\t Valid: 64.02%\tTest: 52.08%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3278\t Train: 67.28%\t Valid: 64.73%\tTest: 52.63%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3260\t Train: 67.93%\t Valid: 65.84%\tTest: 54.03%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3231\t Train: 69.18%\t Valid: 67.94%\tTest: 55.90%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3222\t Train: 69.05%\t Valid: 68.06%\tTest: 56.87%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3207\t Train: 69.17%\t Valid: 68.08%\tTest: 56.97%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3192\t Train: 70.67%\t Valid: 69.69%\tTest: 57.28%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3178\t Train: 71.37%\t Valid: 70.30%\tTest: 59.00%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3164\t Train: 70.94%\t Valid: 70.24%\tTest: 58.79%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3139\t Train: 71.92%\t Valid: 71.37%\tTest: 59.81%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3142\t Train: 71.39%\t Valid: 70.52%\tTest: 58.86%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3116\t Train: 72.54%\t Valid: 71.97%\tTest: 61.03%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3158\t Train: 73.17%\t Valid: 72.74%\tTest: 61.49%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3093\t Train: 73.47%\t Valid: 72.72%\tTest: 62.51%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3107\t Train: 73.53%\t Valid: 72.84%\tTest: 62.47%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3136\t Train: 72.71%\t Valid: 71.58%\tTest: 61.20%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3070\t Train: 74.16%\t Valid: 73.66%\tTest: 63.33%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3048\t Train: 74.86%\t Valid: 74.36%\tTest: 63.96%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3039\t Train: 74.86%\t Valid: 74.27%\tTest: 64.66%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3014\t Train: 76.23%\t Valid: 75.49%\tTest: 66.41%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3008\t Train: 76.20%\t Valid: 75.64%\tTest: 66.76%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2994\t Train: 77.24%\t Valid: 76.72%\tTest: 68.66%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3014\t Train: 77.71%\t Valid: 77.14%\tTest: 69.53%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2969\t Train: 76.97%\t Valid: 76.45%\tTest: 68.53%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2946\t Train: 76.89%\t Valid: 76.31%\tTest: 69.52%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2976\t Train: 77.28%\t Valid: 76.05%\tTest: 68.55%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2934\t Train: 77.73%\t Valid: 76.78%\tTest: 70.01%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2910\t Train: 78.72%\t Valid: 77.39%\tTest: 70.01%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2887\t Train: 79.01%\t Valid: 77.80%\tTest: 70.67%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2886\t Train: 78.56%\t Valid: 77.05%\tTest: 70.19%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2874\t Train: 79.56%\t Valid: 77.84%\tTest: 70.71%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2872\t Train: 78.94%\t Valid: 77.54%\tTest: 71.37%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2896\t Train: 80.12%\t Valid: 78.35%\tTest: 71.44%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2870\t Train: 79.95%\t Valid: 78.04%\tTest: 71.15%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2848\t Train: 80.69%\t Valid: 78.38%\tTest: 71.88%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2817\t Train: 80.63%\t Valid: 78.59%\tTest: 72.16%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2798\t Train: 80.93%\t Valid: 78.67%\tTest: 72.24%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2819\t Train: 81.43%\t Valid: 79.30%\tTest: 73.06%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2780\t Train: 81.04%\t Valid: 78.50%\tTest: 72.24%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2825\t Train: 80.98%\t Valid: 79.06%\tTest: 73.40%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2819\t Train: 80.14%\t Valid: 77.95%\tTest: 72.21%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2800\t Train: 81.00%\t Valid: 78.58%\tTest: 72.40%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2768\t Train: 81.34%\t Valid: 79.10%\tTest: 73.24%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2753\t Train: 81.86%\t Valid: 79.17%\tTest: 73.34%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2740\t Train: 81.43%\t Valid: 78.86%\tTest: 73.04%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2803\t Train: 81.31%\t Valid: 79.11%\tTest: 73.64%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2774\t Train: 81.93%\t Valid: 79.32%\tTest: 73.46%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2735\t Train: 82.37%\t Valid: 79.51%\tTest: 73.54%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2717\t Train: 82.17%\t Valid: 79.38%\tTest: 74.15%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2710\t Train: 82.67%\t Valid: 79.98%\tTest: 74.63%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2710\t Train: 82.58%\t Valid: 79.66%\tTest: 74.50%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2689\t Train: 82.78%\t Valid: 79.94%\tTest: 74.82%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2685\t Train: 82.79%\t Valid: 79.58%\tTest: 74.73%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2672\t Train: 82.89%\t Valid: 79.75%\tTest: 74.26%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2735\t Train: 82.06%\t Valid: 79.12%\tTest: 74.13%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2687\t Train: 83.07%\t Valid: 80.09%\tTest: 74.64%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2672\t Train: 83.07%\t Valid: 80.13%\tTest: 75.05%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2704\t Train: 83.25%\t Valid: 80.02%\tTest: 74.93%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2667\t Train: 82.93%\t Valid: 79.72%\tTest: 74.11%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2680\t Train: 83.08%\t Valid: 79.68%\tTest: 74.23%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2740\t Train: 83.48%\t Valid: 80.21%\tTest: 74.14%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2648\t Train: 83.25%\t Valid: 80.21%\tTest: 75.39%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2671\t Train: 83.45%\t Valid: 80.10%\tTest: 74.00%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2647\t Train: 83.72%\t Valid: 80.37%\tTest: 73.93%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2688\t Train: 82.86%\t Valid: 79.45%\tTest: 74.24%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2655\t Train: 83.71%\t Valid: 80.43%\tTest: 73.38%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2636\t Train: 83.73%\t Valid: 80.29%\tTest: 73.89%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2620\t Train: 84.09%\t Valid: 80.57%\tTest: 73.58%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2649\t Train: 84.06%\t Valid: 80.35%\tTest: 73.88%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2644\t Train: 83.61%\t Valid: 80.10%\tTest: 72.29%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2617\t Train: 84.21%\t Valid: 80.53%\tTest: 71.27%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2619\t Train: 83.89%\t Valid: 80.49%\tTest: 73.14%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2634\t Train: 83.89%\t Valid: 80.10%\tTest: 69.94%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2625\t Train: 84.35%\t Valid: 80.39%\tTest: 72.32%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2653\t Train: 84.18%\t Valid: 80.64%\tTest: 72.38%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2603\t Train: 84.37%\t Valid: 80.48%\tTest: 70.67%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2614\t Train: 84.17%\t Valid: 80.53%\tTest: 72.03%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2598\t Train: 84.51%\t Valid: 80.69%\tTest: 71.31%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2601\t Train: 84.15%\t Valid: 79.95%\tTest: 69.88%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2631\t Train: 84.48%\t Valid: 80.61%\tTest: 71.07%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2590\t Train: 84.39%\t Valid: 80.59%\tTest: 70.08%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2585\t Train: 84.67%\t Valid: 80.80%\tTest: 71.06%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2581\t Train: 84.67%\t Valid: 80.83%\tTest: 69.77%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2674\t Train: 84.28%\t Valid: 80.60%\tTest: 69.73%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2621\t Train: 84.28%\t Valid: 80.56%\tTest: 68.18%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2592\t Train: 84.71%\t Valid: 80.71%\tTest: 69.39%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2573\t Train: 84.77%\t Valid: 80.99%\tTest: 70.57%\n",
      "Seed =  4\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5280\t Train: 37.03%\t Valid: 35.81%\tTest: 38.24%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4168\t Train: 35.63%\t Valid: 33.25%\tTest: 35.87%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4028\t Train: 38.14%\t Valid: 33.51%\tTest: 35.16%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3936\t Train: 41.08%\t Valid: 34.56%\tTest: 35.13%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3845\t Train: 44.61%\t Valid: 34.97%\tTest: 34.78%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3751\t Train: 51.47%\t Valid: 37.50%\tTest: 36.13%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3681\t Train: 53.96%\t Valid: 40.37%\tTest: 38.00%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3609\t Train: 56.29%\t Valid: 44.18%\tTest: 40.18%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3589\t Train: 57.96%\t Valid: 47.29%\tTest: 41.76%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3488\t Train: 61.24%\t Valid: 51.45%\tTest: 44.70%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3438\t Train: 61.79%\t Valid: 51.92%\tTest: 44.67%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3382\t Train: 63.94%\t Valid: 55.90%\tTest: 46.96%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3349\t Train: 64.20%\t Valid: 56.48%\tTest: 47.36%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3324\t Train: 66.57%\t Valid: 60.70%\tTest: 50.72%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3336\t Train: 66.25%\t Valid: 59.46%\tTest: 50.13%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3289\t Train: 67.90%\t Valid: 62.92%\tTest: 52.80%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3264\t Train: 68.38%\t Valid: 64.55%\tTest: 53.98%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3244\t Train: 68.45%\t Valid: 65.19%\tTest: 54.58%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3224\t Train: 69.24%\t Valid: 66.37%\tTest: 55.85%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3220\t Train: 70.03%\t Valid: 67.58%\tTest: 57.16%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3187\t Train: 70.70%\t Valid: 68.62%\tTest: 58.05%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3173\t Train: 71.44%\t Valid: 69.63%\tTest: 59.13%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3160\t Train: 71.45%\t Valid: 69.73%\tTest: 59.09%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3144\t Train: 72.10%\t Valid: 69.92%\tTest: 59.44%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3142\t Train: 72.13%\t Valid: 70.47%\tTest: 60.27%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3121\t Train: 71.02%\t Valid: 68.44%\tTest: 57.93%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3136\t Train: 72.03%\t Valid: 69.95%\tTest: 60.75%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3107\t Train: 73.62%\t Valid: 71.83%\tTest: 62.30%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3085\t Train: 73.76%\t Valid: 71.66%\tTest: 61.97%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3069\t Train: 73.89%\t Valid: 71.70%\tTest: 61.64%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3057\t Train: 74.95%\t Valid: 72.80%\tTest: 63.07%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3040\t Train: 74.81%\t Valid: 72.52%\tTest: 62.52%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3028\t Train: 75.07%\t Valid: 72.32%\tTest: 61.98%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3024\t Train: 76.01%\t Valid: 73.33%\tTest: 63.91%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3010\t Train: 76.26%\t Valid: 73.21%\tTest: 63.46%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2990\t Train: 76.09%\t Valid: 73.18%\tTest: 63.50%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2999\t Train: 76.81%\t Valid: 74.14%\tTest: 64.90%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2979\t Train: 76.78%\t Valid: 74.05%\tTest: 64.52%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2964\t Train: 77.10%\t Valid: 73.96%\tTest: 65.08%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2965\t Train: 77.16%\t Valid: 74.10%\tTest: 65.28%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2939\t Train: 78.17%\t Valid: 75.06%\tTest: 66.38%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2927\t Train: 78.29%\t Valid: 75.14%\tTest: 66.65%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2911\t Train: 78.35%\t Valid: 75.30%\tTest: 66.70%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2940\t Train: 78.25%\t Valid: 75.32%\tTest: 67.36%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2905\t Train: 78.41%\t Valid: 75.22%\tTest: 66.50%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2942\t Train: 77.91%\t Valid: 75.12%\tTest: 66.58%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2934\t Train: 78.82%\t Valid: 75.82%\tTest: 68.36%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2904\t Train: 78.99%\t Valid: 75.95%\tTest: 68.17%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2965\t Train: 79.39%\t Valid: 76.34%\tTest: 69.12%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2883\t Train: 79.74%\t Valid: 76.67%\tTest: 68.93%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2858\t Train: 79.88%\t Valid: 76.93%\tTest: 69.08%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2848\t Train: 79.92%\t Valid: 76.64%\tTest: 68.83%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2830\t Train: 80.27%\t Valid: 77.06%\tTest: 69.63%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2896\t Train: 79.34%\t Valid: 76.49%\tTest: 68.38%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2836\t Train: 80.63%\t Valid: 77.58%\tTest: 69.78%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2826\t Train: 80.44%\t Valid: 77.29%\tTest: 69.10%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2819\t Train: 81.16%\t Valid: 78.09%\tTest: 70.75%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2799\t Train: 81.04%\t Valid: 77.75%\tTest: 70.23%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2844\t Train: 81.38%\t Valid: 78.61%\tTest: 71.70%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2781\t Train: 81.10%\t Valid: 77.87%\tTest: 70.59%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2808\t Train: 80.59%\t Valid: 76.68%\tTest: 68.85%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2830\t Train: 81.15%\t Valid: 77.96%\tTest: 69.91%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2773\t Train: 81.81%\t Valid: 78.51%\tTest: 70.77%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2742\t Train: 81.91%\t Valid: 78.25%\tTest: 70.91%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2875\t Train: 80.97%\t Valid: 78.07%\tTest: 70.73%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2773\t Train: 81.70%\t Valid: 78.24%\tTest: 71.24%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2735\t Train: 82.43%\t Valid: 78.82%\tTest: 70.75%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2713\t Train: 82.67%\t Valid: 78.92%\tTest: 71.70%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2741\t Train: 82.21%\t Valid: 78.88%\tTest: 72.56%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2721\t Train: 82.53%\t Valid: 78.83%\tTest: 71.14%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2704\t Train: 82.72%\t Valid: 78.74%\tTest: 70.96%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2747\t Train: 81.77%\t Valid: 77.57%\tTest: 70.62%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2702\t Train: 82.85%\t Valid: 78.99%\tTest: 73.12%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2687\t Train: 82.76%\t Valid: 78.28%\tTest: 72.07%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2688\t Train: 82.94%\t Valid: 79.07%\tTest: 73.28%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2676\t Train: 83.17%\t Valid: 79.24%\tTest: 72.80%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2659\t Train: 83.58%\t Valid: 79.31%\tTest: 73.10%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2658\t Train: 83.25%\t Valid: 78.74%\tTest: 72.18%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2662\t Train: 83.72%\t Valid: 79.39%\tTest: 72.78%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2645\t Train: 83.80%\t Valid: 79.37%\tTest: 72.39%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2641\t Train: 83.97%\t Valid: 79.71%\tTest: 72.83%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2820\t Train: 82.91%\t Valid: 79.72%\tTest: 71.50%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2666\t Train: 83.53%\t Valid: 79.70%\tTest: 70.45%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2654\t Train: 83.81%\t Valid: 79.60%\tTest: 72.21%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2638\t Train: 84.05%\t Valid: 79.68%\tTest: 72.43%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2615\t Train: 84.20%\t Valid: 79.76%\tTest: 71.92%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2653\t Train: 84.40%\t Valid: 80.18%\tTest: 72.26%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2636\t Train: 84.16%\t Valid: 80.00%\tTest: 70.96%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2609\t Train: 84.25%\t Valid: 79.71%\tTest: 70.09%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2597\t Train: 84.58%\t Valid: 80.14%\tTest: 70.35%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2654\t Train: 84.14%\t Valid: 80.27%\tTest: 70.63%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2614\t Train: 84.41%\t Valid: 80.26%\tTest: 68.72%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2600\t Train: 84.69%\t Valid: 80.39%\tTest: 70.45%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2604\t Train: 84.61%\t Valid: 80.48%\tTest: 70.18%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2628\t Train: 84.72%\t Valid: 80.62%\tTest: 70.03%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2589\t Train: 84.43%\t Valid: 80.32%\tTest: 67.99%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2589\t Train: 84.64%\t Valid: 80.15%\tTest: 67.49%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2576\t Train: 84.93%\t Valid: 80.40%\tTest: 66.81%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2632\t Train: 84.79%\t Valid: 80.54%\tTest: 67.05%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2585\t Train: 85.02%\t Valid: 80.50%\tTest: 64.08%\n",
      "Seed =  5\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4892\t Train: 38.27%\t Valid: 36.14%\tTest: 38.91%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4225\t Train: 37.13%\t Valid: 33.36%\tTest: 36.02%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3992\t Train: 40.60%\t Valid: 34.19%\tTest: 35.46%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3880\t Train: 44.09%\t Valid: 34.98%\tTest: 35.31%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3765\t Train: 53.09%\t Valid: 38.10%\tTest: 36.78%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3687\t Train: 53.52%\t Valid: 40.20%\tTest: 38.04%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3663\t Train: 55.18%\t Valid: 43.11%\tTest: 39.74%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3553\t Train: 57.10%\t Valid: 45.08%\tTest: 40.88%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3475\t Train: 61.35%\t Valid: 51.76%\tTest: 44.45%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3440\t Train: 62.28%\t Valid: 52.68%\tTest: 45.62%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3413\t Train: 63.37%\t Valid: 54.32%\tTest: 46.34%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3359\t Train: 64.13%\t Valid: 57.12%\tTest: 47.49%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3333\t Train: 66.20%\t Valid: 60.73%\tTest: 50.21%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3311\t Train: 66.53%\t Valid: 61.38%\tTest: 50.75%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3290\t Train: 67.82%\t Valid: 63.70%\tTest: 52.46%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3262\t Train: 67.72%\t Valid: 65.17%\tTest: 53.30%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3273\t Train: 66.45%\t Valid: 62.98%\tTest: 52.95%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3244\t Train: 68.96%\t Valid: 65.95%\tTest: 55.39%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3209\t Train: 69.38%\t Valid: 67.21%\tTest: 56.02%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3193\t Train: 71.18%\t Valid: 70.41%\tTest: 59.17%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3166\t Train: 71.40%\t Valid: 70.62%\tTest: 59.47%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3151\t Train: 71.77%\t Valid: 71.26%\tTest: 60.30%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3135\t Train: 72.03%\t Valid: 71.45%\tTest: 60.41%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3152\t Train: 71.81%\t Valid: 70.90%\tTest: 59.78%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3122\t Train: 72.06%\t Valid: 71.33%\tTest: 60.52%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3099\t Train: 73.78%\t Valid: 72.92%\tTest: 62.78%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3080\t Train: 73.50%\t Valid: 72.58%\tTest: 62.34%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3089\t Train: 72.84%\t Valid: 70.85%\tTest: 60.58%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3055\t Train: 74.85%\t Valid: 73.67%\tTest: 64.01%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3043\t Train: 74.70%\t Valid: 73.56%\tTest: 63.83%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3061\t Train: 75.88%\t Valid: 74.59%\tTest: 65.01%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3030\t Train: 75.84%\t Valid: 74.54%\tTest: 64.81%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3010\t Train: 75.77%\t Valid: 74.24%\tTest: 64.50%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2991\t Train: 76.32%\t Valid: 75.00%\tTest: 65.27%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2994\t Train: 76.88%\t Valid: 75.66%\tTest: 66.65%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3008\t Train: 76.46%\t Valid: 75.43%\tTest: 66.75%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2985\t Train: 76.63%\t Valid: 75.21%\tTest: 65.80%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2958\t Train: 77.50%\t Valid: 75.84%\tTest: 66.88%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2938\t Train: 77.97%\t Valid: 76.66%\tTest: 67.99%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2921\t Train: 78.65%\t Valid: 77.30%\tTest: 68.81%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2959\t Train: 77.83%\t Valid: 76.92%\tTest: 69.13%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2915\t Train: 77.89%\t Valid: 76.67%\tTest: 69.15%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2911\t Train: 78.82%\t Valid: 77.75%\tTest: 69.86%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2876\t Train: 79.50%\t Valid: 77.84%\tTest: 69.74%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2882\t Train: 79.90%\t Valid: 78.61%\tTest: 71.18%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2897\t Train: 79.64%\t Valid: 78.05%\tTest: 70.72%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2864\t Train: 79.16%\t Valid: 77.61%\tTest: 70.13%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2841\t Train: 80.22%\t Valid: 78.20%\tTest: 70.57%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2836\t Train: 80.62%\t Valid: 78.46%\tTest: 71.08%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2822\t Train: 80.57%\t Valid: 78.03%\tTest: 70.56%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2833\t Train: 80.55%\t Valid: 77.95%\tTest: 70.35%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2811\t Train: 81.51%\t Valid: 79.07%\tTest: 71.93%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2841\t Train: 81.32%\t Valid: 79.22%\tTest: 72.37%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2805\t Train: 81.10%\t Valid: 78.56%\tTest: 70.93%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2771\t Train: 81.51%\t Valid: 79.01%\tTest: 72.09%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2760\t Train: 81.62%\t Valid: 78.64%\tTest: 71.57%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2778\t Train: 81.82%\t Valid: 79.27%\tTest: 72.30%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2737\t Train: 82.26%\t Valid: 79.34%\tTest: 72.40%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2726\t Train: 82.17%\t Valid: 79.31%\tTest: 72.82%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2758\t Train: 82.07%\t Valid: 79.58%\tTest: 72.70%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2725\t Train: 82.60%\t Valid: 79.31%\tTest: 72.05%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2712\t Train: 82.24%\t Valid: 79.14%\tTest: 72.06%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2710\t Train: 82.24%\t Valid: 78.62%\tTest: 71.50%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2740\t Train: 82.78%\t Valid: 79.31%\tTest: 71.82%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2719\t Train: 82.62%\t Valid: 79.68%\tTest: 73.13%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2702\t Train: 82.76%\t Valid: 78.92%\tTest: 72.60%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2678\t Train: 83.11%\t Valid: 79.02%\tTest: 72.17%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2684\t Train: 83.44%\t Valid: 79.97%\tTest: 73.39%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2689\t Train: 82.90%\t Valid: 79.41%\tTest: 72.29%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2689\t Train: 83.17%\t Valid: 79.51%\tTest: 72.51%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2669\t Train: 83.33%\t Valid: 79.49%\tTest: 72.65%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2689\t Train: 83.44%\t Valid: 79.51%\tTest: 72.34%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2662\t Train: 83.67%\t Valid: 80.15%\tTest: 73.56%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2678\t Train: 83.77%\t Valid: 79.97%\tTest: 73.45%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2754\t Train: 83.20%\t Valid: 79.82%\tTest: 73.16%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2696\t Train: 83.06%\t Valid: 79.78%\tTest: 72.72%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2668\t Train: 83.40%\t Valid: 79.15%\tTest: 71.90%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2716\t Train: 83.08%\t Valid: 79.72%\tTest: 73.23%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2668\t Train: 83.49%\t Valid: 79.71%\tTest: 72.61%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2638\t Train: 83.86%\t Valid: 79.71%\tTest: 72.59%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2634\t Train: 84.18%\t Valid: 80.15%\tTest: 73.25%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2618\t Train: 84.21%\t Valid: 79.97%\tTest: 73.03%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2617\t Train: 84.19%\t Valid: 79.90%\tTest: 72.96%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2604\t Train: 84.22%\t Valid: 79.75%\tTest: 72.53%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2596\t Train: 84.32%\t Valid: 79.72%\tTest: 72.44%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2682\t Train: 84.00%\t Valid: 79.97%\tTest: 72.54%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2630\t Train: 83.92%\t Valid: 79.99%\tTest: 72.65%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2629\t Train: 84.20%\t Valid: 79.84%\tTest: 72.55%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2587\t Train: 84.75%\t Valid: 80.52%\tTest: 73.53%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2600\t Train: 84.60%\t Valid: 80.23%\tTest: 72.65%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2633\t Train: 84.34%\t Valid: 79.74%\tTest: 72.16%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2595\t Train: 84.74%\t Valid: 80.53%\tTest: 73.61%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2600\t Train: 84.84%\t Valid: 80.51%\tTest: 73.07%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2592\t Train: 84.86%\t Valid: 80.09%\tTest: 71.87%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2613\t Train: 84.85%\t Valid: 80.62%\tTest: 72.46%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2591\t Train: 84.75%\t Valid: 80.29%\tTest: 72.02%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2585\t Train: 84.81%\t Valid: 80.00%\tTest: 72.05%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2576\t Train: 85.08%\t Valid: 80.50%\tTest: 72.52%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2561\t Train: 85.27%\t Valid: 80.72%\tTest: 72.08%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2550\t Train: 85.14%\t Valid: 80.50%\tTest: 71.56%\n",
      "Seed =  6\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5253\t Train: 38.09%\t Valid: 35.88%\tTest: 38.08%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4186\t Train: 36.48%\t Valid: 33.46%\tTest: 35.79%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4018\t Train: 38.31%\t Valid: 33.31%\tTest: 34.93%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3910\t Train: 42.67%\t Valid: 34.71%\tTest: 35.05%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3785\t Train: 48.30%\t Valid: 35.99%\tTest: 35.38%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3708\t Train: 53.24%\t Valid: 39.29%\tTest: 37.44%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3647\t Train: 54.86%\t Valid: 41.90%\tTest: 38.89%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3589\t Train: 58.11%\t Valid: 46.04%\tTest: 41.46%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3481\t Train: 59.41%\t Valid: 47.82%\tTest: 42.04%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3521\t Train: 62.10%\t Valid: 52.19%\tTest: 45.12%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3435\t Train: 62.87%\t Valid: 52.45%\tTest: 46.60%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3350\t Train: 65.66%\t Valid: 58.60%\tTest: 49.36%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3340\t Train: 66.28%\t Valid: 62.44%\tTest: 50.64%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3281\t Train: 67.08%\t Valid: 64.17%\tTest: 52.14%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3254\t Train: 69.01%\t Valid: 67.47%\tTest: 55.05%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3260\t Train: 70.11%\t Valid: 68.99%\tTest: 56.92%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3220\t Train: 70.06%\t Valid: 68.42%\tTest: 56.71%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3196\t Train: 71.10%\t Valid: 70.67%\tTest: 58.99%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3213\t Train: 70.43%\t Valid: 68.39%\tTest: 57.62%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3185\t Train: 70.84%\t Valid: 69.73%\tTest: 58.54%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3169\t Train: 72.05%\t Valid: 71.52%\tTest: 60.74%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3148\t Train: 71.58%\t Valid: 70.18%\tTest: 59.94%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3132\t Train: 72.20%\t Valid: 71.93%\tTest: 61.42%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3133\t Train: 72.38%\t Valid: 71.88%\tTest: 61.76%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3118\t Train: 73.79%\t Valid: 73.71%\tTest: 63.75%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3085\t Train: 73.54%\t Valid: 72.66%\tTest: 62.89%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3109\t Train: 74.61%\t Valid: 74.35%\tTest: 65.28%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3066\t Train: 74.43%\t Valid: 74.14%\tTest: 65.28%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3149\t Train: 73.34%\t Valid: 72.90%\tTest: 64.52%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3095\t Train: 73.75%\t Valid: 72.88%\tTest: 63.80%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3050\t Train: 74.87%\t Valid: 74.00%\tTest: 65.30%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3024\t Train: 75.53%\t Valid: 74.98%\tTest: 66.33%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3009\t Train: 75.72%\t Valid: 75.10%\tTest: 66.98%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2992\t Train: 77.11%\t Valid: 76.35%\tTest: 68.64%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3010\t Train: 75.81%\t Valid: 74.84%\tTest: 67.28%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2992\t Train: 76.26%\t Valid: 75.69%\tTest: 67.91%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2963\t Train: 77.13%\t Valid: 76.29%\tTest: 68.96%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2941\t Train: 77.82%\t Valid: 76.86%\tTest: 69.88%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3126\t Train: 76.67%\t Valid: 76.45%\tTest: 69.47%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2973\t Train: 77.46%\t Valid: 76.16%\tTest: 69.09%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2939\t Train: 77.79%\t Valid: 76.66%\tTest: 69.53%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2914\t Train: 78.41%\t Valid: 77.15%\tTest: 70.28%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2896\t Train: 78.97%\t Valid: 77.79%\tTest: 71.34%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2897\t Train: 79.63%\t Valid: 78.26%\tTest: 72.01%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2905\t Train: 79.06%\t Valid: 77.16%\tTest: 71.53%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2880\t Train: 79.44%\t Valid: 78.00%\tTest: 71.87%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2862\t Train: 79.77%\t Valid: 78.02%\tTest: 71.94%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2870\t Train: 80.23%\t Valid: 78.52%\tTest: 72.25%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2842\t Train: 80.05%\t Valid: 78.10%\tTest: 71.94%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2838\t Train: 79.94%\t Valid: 77.94%\tTest: 71.99%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2858\t Train: 80.60%\t Valid: 79.00%\tTest: 73.13%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2846\t Train: 80.88%\t Valid: 78.62%\tTest: 72.61%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2793\t Train: 80.96%\t Valid: 78.73%\tTest: 72.68%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2798\t Train: 81.32%\t Valid: 79.14%\tTest: 73.48%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2801\t Train: 81.15%\t Valid: 78.13%\tTest: 71.90%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2798\t Train: 81.37%\t Valid: 79.29%\tTest: 73.32%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2770\t Train: 81.54%\t Valid: 79.11%\tTest: 72.96%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2781\t Train: 81.95%\t Valid: 79.36%\tTest: 72.76%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2763\t Train: 81.95%\t Valid: 79.53%\tTest: 73.18%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2744\t Train: 82.24%\t Valid: 79.83%\tTest: 73.39%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2753\t Train: 82.24%\t Valid: 79.95%\tTest: 73.35%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2755\t Train: 81.98%\t Valid: 79.21%\tTest: 72.06%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2731\t Train: 82.07%\t Valid: 78.84%\tTest: 71.36%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2717\t Train: 82.34%\t Valid: 79.44%\tTest: 72.02%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2740\t Train: 82.33%\t Valid: 79.77%\tTest: 72.35%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2751\t Train: 82.48%\t Valid: 79.71%\tTest: 71.75%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2720\t Train: 82.57%\t Valid: 79.60%\tTest: 71.52%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2731\t Train: 82.34%\t Valid: 79.27%\tTest: 70.71%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2707\t Train: 82.38%\t Valid: 78.71%\tTest: 70.03%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2701\t Train: 82.85%\t Valid: 79.84%\tTest: 71.31%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2692\t Train: 83.18%\t Valid: 80.52%\tTest: 71.83%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2682\t Train: 83.34%\t Valid: 80.32%\tTest: 70.77%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2707\t Train: 82.50%\t Valid: 78.66%\tTest: 67.76%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2681\t Train: 83.16%\t Valid: 80.14%\tTest: 71.79%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2657\t Train: 83.47%\t Valid: 79.94%\tTest: 68.76%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2728\t Train: 83.62%\t Valid: 80.73%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2704\t Train: 83.07%\t Valid: 80.24%\tTest: 69.77%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2656\t Train: 83.69%\t Valid: 80.62%\tTest: 70.80%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2643\t Train: 83.62%\t Valid: 80.23%\tTest: 68.67%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2661\t Train: 83.83%\t Valid: 80.04%\tTest: 67.97%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2702\t Train: 83.51%\t Valid: 80.16%\tTest: 68.48%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2639\t Train: 83.87%\t Valid: 80.33%\tTest: 67.27%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2621\t Train: 83.90%\t Valid: 79.96%\tTest: 67.08%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2619\t Train: 83.97%\t Valid: 79.46%\tTest: 66.02%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2608\t Train: 84.17%\t Valid: 79.65%\tTest: 65.87%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2626\t Train: 84.38%\t Valid: 80.42%\tTest: 66.29%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2634\t Train: 83.33%\t Valid: 78.47%\tTest: 68.42%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2686\t Train: 83.74%\t Valid: 80.31%\tTest: 66.04%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2628\t Train: 84.16%\t Valid: 79.62%\tTest: 65.17%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2605\t Train: 84.50%\t Valid: 79.80%\tTest: 64.05%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2592\t Train: 84.59%\t Valid: 79.98%\tTest: 62.79%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2768\t Train: 82.28%\t Valid: 80.07%\tTest: 72.12%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2782\t Train: 82.15%\t Valid: 79.87%\tTest: 72.83%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2674\t Train: 83.26%\t Valid: 80.25%\tTest: 70.06%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2633\t Train: 83.81%\t Valid: 79.40%\tTest: 63.77%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2623\t Train: 84.29%\t Valid: 81.03%\tTest: 63.63%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2607\t Train: 84.56%\t Valid: 80.82%\tTest: 61.38%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2585\t Train: 84.63%\t Valid: 80.28%\tTest: 57.41%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2572\t Train: 84.76%\t Valid: 79.22%\tTest: 56.61%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2575\t Train: 85.02%\t Valid: 80.38%\tTest: 57.07%\n",
      "Seed =  7\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4441\t Train: 37.50%\t Valid: 34.75%\tTest: 37.70%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4140\t Train: 38.16%\t Valid: 33.20%\tTest: 35.67%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3962\t Train: 43.63%\t Valid: 34.47%\tTest: 35.47%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3825\t Train: 48.51%\t Valid: 35.72%\tTest: 35.88%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3745\t Train: 53.19%\t Valid: 38.86%\tTest: 37.51%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3668\t Train: 52.96%\t Valid: 40.51%\tTest: 38.13%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3622\t Train: 57.47%\t Valid: 46.88%\tTest: 42.57%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3542\t Train: 57.05%\t Valid: 45.89%\tTest: 41.98%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3466\t Train: 60.36%\t Valid: 51.81%\tTest: 44.82%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3471\t Train: 63.51%\t Valid: 55.47%\tTest: 47.85%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3390\t Train: 64.64%\t Valid: 56.75%\tTest: 49.32%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3319\t Train: 63.32%\t Valid: 56.24%\tTest: 46.58%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3315\t Train: 67.24%\t Valid: 62.23%\tTest: 53.00%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3248\t Train: 68.02%\t Valid: 64.51%\tTest: 53.93%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3223\t Train: 68.69%\t Valid: 66.28%\tTest: 55.71%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3201\t Train: 70.27%\t Valid: 69.34%\tTest: 58.46%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3243\t Train: 70.15%\t Valid: 69.23%\tTest: 58.62%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3182\t Train: 71.34%\t Valid: 70.13%\tTest: 60.10%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3150\t Train: 72.05%\t Valid: 71.07%\tTest: 60.68%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3127\t Train: 72.36%\t Valid: 71.95%\tTest: 61.31%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3119\t Train: 72.32%\t Valid: 71.96%\tTest: 61.66%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3093\t Train: 73.29%\t Valid: 73.26%\tTest: 63.35%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3070\t Train: 73.90%\t Valid: 73.63%\tTest: 63.87%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3065\t Train: 74.31%\t Valid: 74.19%\tTest: 64.80%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3058\t Train: 75.47%\t Valid: 75.05%\tTest: 66.10%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3026\t Train: 74.49%\t Valid: 73.74%\tTest: 65.11%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3019\t Train: 76.30%\t Valid: 75.44%\tTest: 67.01%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3017\t Train: 76.64%\t Valid: 75.66%\tTest: 67.29%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2997\t Train: 75.90%\t Valid: 74.98%\tTest: 66.86%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2960\t Train: 77.28%\t Valid: 76.14%\tTest: 68.24%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2957\t Train: 78.16%\t Valid: 77.26%\tTest: 69.74%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2937\t Train: 77.76%\t Valid: 76.36%\tTest: 68.95%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2932\t Train: 77.80%\t Valid: 76.43%\tTest: 69.15%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2923\t Train: 79.11%\t Valid: 77.39%\tTest: 70.13%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2896\t Train: 78.92%\t Valid: 77.03%\tTest: 69.54%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2882\t Train: 78.57%\t Valid: 77.19%\tTest: 70.38%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2874\t Train: 79.10%\t Valid: 76.88%\tTest: 69.75%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2995\t Train: 79.40%\t Valid: 77.78%\tTest: 71.05%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2873\t Train: 78.91%\t Valid: 76.85%\tTest: 69.91%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2850\t Train: 79.80%\t Valid: 77.54%\tTest: 70.38%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2863\t Train: 79.25%\t Valid: 77.39%\tTest: 70.85%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2844\t Train: 80.37%\t Valid: 78.05%\tTest: 71.09%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2834\t Train: 80.17%\t Valid: 78.33%\tTest: 72.13%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2883\t Train: 80.23%\t Valid: 77.65%\tTest: 70.81%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2812\t Train: 80.57%\t Valid: 78.00%\tTest: 70.90%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2806\t Train: 80.22%\t Valid: 77.62%\tTest: 70.98%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2788\t Train: 81.11%\t Valid: 78.55%\tTest: 71.89%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2777\t Train: 81.39%\t Valid: 78.82%\tTest: 72.29%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2869\t Train: 81.07%\t Valid: 78.21%\tTest: 71.50%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2780\t Train: 81.34%\t Valid: 78.68%\tTest: 71.80%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2762\t Train: 81.51%\t Valid: 78.78%\tTest: 72.16%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2751\t Train: 81.77%\t Valid: 78.81%\tTest: 72.06%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2763\t Train: 81.45%\t Valid: 78.55%\tTest: 72.40%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2751\t Train: 82.02%\t Valid: 79.47%\tTest: 73.32%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2741\t Train: 81.92%\t Valid: 79.09%\tTest: 72.63%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2738\t Train: 82.41%\t Valid: 79.47%\tTest: 72.81%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2727\t Train: 81.87%\t Valid: 78.94%\tTest: 72.68%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2784\t Train: 82.30%\t Valid: 79.76%\tTest: 73.58%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2719\t Train: 82.66%\t Valid: 79.45%\tTest: 72.98%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2713\t Train: 82.72%\t Valid: 79.84%\tTest: 73.55%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2702\t Train: 82.74%\t Valid: 79.79%\tTest: 73.62%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2684\t Train: 82.82%\t Valid: 79.86%\tTest: 73.85%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2774\t Train: 81.98%\t Valid: 79.51%\tTest: 74.01%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2710\t Train: 82.76%\t Valid: 79.67%\tTest: 73.69%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2732\t Train: 83.10%\t Valid: 79.89%\tTest: 73.58%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2683\t Train: 83.19%\t Valid: 79.79%\tTest: 73.29%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2673\t Train: 82.67%\t Valid: 79.49%\tTest: 73.36%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2685\t Train: 83.37%\t Valid: 80.09%\tTest: 74.05%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2663\t Train: 83.51%\t Valid: 80.02%\tTest: 73.99%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2658\t Train: 83.57%\t Valid: 80.14%\tTest: 74.38%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2659\t Train: 83.63%\t Valid: 80.77%\tTest: 74.96%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2638\t Train: 83.75%\t Valid: 80.54%\tTest: 74.31%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2649\t Train: 83.48%\t Valid: 80.47%\tTest: 74.57%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2646\t Train: 83.54%\t Valid: 80.22%\tTest: 74.12%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2649\t Train: 83.87%\t Valid: 80.29%\tTest: 73.92%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2639\t Train: 84.16%\t Valid: 80.69%\tTest: 74.72%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2628\t Train: 84.01%\t Valid: 80.26%\tTest: 74.14%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2647\t Train: 83.70%\t Valid: 80.09%\tTest: 74.24%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2617\t Train: 84.11%\t Valid: 80.78%\tTest: 74.89%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2632\t Train: 84.26%\t Valid: 81.26%\tTest: 75.29%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2632\t Train: 83.88%\t Valid: 80.38%\tTest: 74.13%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2609\t Train: 84.44%\t Valid: 80.83%\tTest: 74.48%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2645\t Train: 83.89%\t Valid: 80.66%\tTest: 74.78%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2604\t Train: 84.45%\t Valid: 81.06%\tTest: 74.79%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2596\t Train: 84.42%\t Valid: 80.82%\tTest: 74.52%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2589\t Train: 84.65%\t Valid: 80.90%\tTest: 74.72%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2579\t Train: 84.55%\t Valid: 80.97%\tTest: 74.67%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2599\t Train: 84.70%\t Valid: 81.33%\tTest: 75.28%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2594\t Train: 84.77%\t Valid: 81.02%\tTest: 74.31%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2596\t Train: 84.88%\t Valid: 81.46%\tTest: 75.32%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2585\t Train: 85.06%\t Valid: 81.39%\tTest: 75.12%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2571\t Train: 84.77%\t Valid: 81.09%\tTest: 74.68%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2564\t Train: 84.98%\t Valid: 80.86%\tTest: 73.84%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2653\t Train: 84.65%\t Valid: 81.29%\tTest: 75.35%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2572\t Train: 84.93%\t Valid: 81.26%\tTest: 75.09%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2578\t Train: 84.82%\t Valid: 80.96%\tTest: 74.61%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2553\t Train: 85.05%\t Valid: 81.28%\tTest: 74.59%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2561\t Train: 85.16%\t Valid: 81.27%\tTest: 74.55%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2557\t Train: 85.10%\t Valid: 81.43%\tTest: 75.12%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2585\t Train: 84.75%\t Valid: 81.16%\tTest: 73.57%\n",
      "Seed =  8\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5134\t Train: 37.39%\t Valid: 35.64%\tTest: 38.50%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4257\t Train: 36.92%\t Valid: 33.75%\tTest: 36.26%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4016\t Train: 38.93%\t Valid: 33.75%\tTest: 35.38%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3931\t Train: 41.23%\t Valid: 34.45%\tTest: 35.11%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3853\t Train: 43.77%\t Valid: 34.96%\tTest: 34.74%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3776\t Train: 47.75%\t Valid: 36.48%\tTest: 35.26%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3700\t Train: 52.89%\t Valid: 39.41%\tTest: 37.15%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3634\t Train: 54.44%\t Valid: 41.43%\tTest: 38.30%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3557\t Train: 57.17%\t Valid: 45.09%\tTest: 40.24%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3487\t Train: 59.24%\t Valid: 49.24%\tTest: 42.69%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3430\t Train: 59.82%\t Valid: 50.80%\tTest: 44.30%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3392\t Train: 63.72%\t Valid: 53.35%\tTest: 46.73%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3420\t Train: 60.66%\t Valid: 48.44%\tTest: 44.08%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3360\t Train: 65.05%\t Valid: 56.23%\tTest: 49.37%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3334\t Train: 68.09%\t Valid: 59.82%\tTest: 51.98%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3297\t Train: 66.95%\t Valid: 58.18%\tTest: 50.28%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3253\t Train: 68.94%\t Valid: 64.71%\tTest: 53.55%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3253\t Train: 68.91%\t Valid: 64.87%\tTest: 53.09%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3208\t Train: 69.80%\t Valid: 66.33%\tTest: 55.01%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3181\t Train: 71.03%\t Valid: 68.83%\tTest: 57.31%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3176\t Train: 72.00%\t Valid: 70.06%\tTest: 59.57%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3180\t Train: 70.22%\t Valid: 68.24%\tTest: 57.29%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3132\t Train: 71.40%\t Valid: 70.29%\tTest: 58.99%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3115\t Train: 72.74%\t Valid: 71.45%\tTest: 60.41%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3094\t Train: 73.22%\t Valid: 71.96%\tTest: 61.35%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3195\t Train: 70.60%\t Valid: 68.98%\tTest: 60.22%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3133\t Train: 72.26%\t Valid: 69.84%\tTest: 60.25%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3088\t Train: 73.81%\t Valid: 72.29%\tTest: 62.10%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3066\t Train: 74.22%\t Valid: 73.05%\tTest: 62.98%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3047\t Train: 74.72%\t Valid: 73.43%\tTest: 63.63%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3039\t Train: 75.67%\t Valid: 74.29%\tTest: 65.13%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3035\t Train: 75.93%\t Valid: 74.29%\tTest: 65.17%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3007\t Train: 75.93%\t Valid: 74.36%\tTest: 65.19%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3002\t Train: 76.32%\t Valid: 74.63%\tTest: 65.62%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2987\t Train: 77.10%\t Valid: 75.90%\tTest: 67.09%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2981\t Train: 76.83%\t Valid: 74.90%\tTest: 66.41%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2967\t Train: 76.61%\t Valid: 74.99%\tTest: 65.79%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2976\t Train: 77.93%\t Valid: 76.70%\tTest: 68.06%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2968\t Train: 77.28%\t Valid: 76.01%\tTest: 67.04%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2966\t Train: 75.99%\t Valid: 74.25%\tTest: 64.97%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2955\t Train: 78.00%\t Valid: 75.98%\tTest: 67.72%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2934\t Train: 78.34%\t Valid: 76.54%\tTest: 68.54%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2896\t Train: 78.90%\t Valid: 77.03%\tTest: 69.07%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2871\t Train: 79.63%\t Valid: 77.66%\tTest: 70.06%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2884\t Train: 79.71%\t Valid: 78.11%\tTest: 70.54%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2869\t Train: 79.58%\t Valid: 78.02%\tTest: 70.69%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2836\t Train: 80.33%\t Valid: 78.31%\tTest: 70.85%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2824\t Train: 80.76%\t Valid: 78.83%\tTest: 71.83%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2825\t Train: 80.23%\t Valid: 78.32%\tTest: 71.24%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2834\t Train: 80.69%\t Valid: 78.56%\tTest: 71.17%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2839\t Train: 80.03%\t Valid: 77.92%\tTest: 70.08%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2809\t Train: 80.80%\t Valid: 78.49%\tTest: 70.81%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2770\t Train: 80.94%\t Valid: 78.58%\tTest: 70.89%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2755\t Train: 81.87%\t Valid: 79.28%\tTest: 71.76%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2777\t Train: 81.50%\t Valid: 78.97%\tTest: 71.60%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2747\t Train: 81.62%\t Valid: 79.48%\tTest: 72.40%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2746\t Train: 82.02%\t Valid: 79.65%\tTest: 72.32%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2739\t Train: 82.20%\t Valid: 79.58%\tTest: 72.17%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2740\t Train: 82.40%\t Valid: 79.65%\tTest: 71.09%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2713\t Train: 82.59%\t Valid: 79.59%\tTest: 70.33%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2699\t Train: 82.34%\t Valid: 79.34%\tTest: 69.18%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2762\t Train: 82.42%\t Valid: 79.72%\tTest: 72.58%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2719\t Train: 82.30%\t Valid: 79.24%\tTest: 71.61%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2695\t Train: 82.82%\t Valid: 79.43%\tTest: 71.89%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2677\t Train: 83.02%\t Valid: 79.89%\tTest: 72.42%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2701\t Train: 82.81%\t Valid: 79.50%\tTest: 71.82%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2723\t Train: 82.95%\t Valid: 79.89%\tTest: 72.24%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2672\t Train: 83.37%\t Valid: 80.05%\tTest: 72.33%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2689\t Train: 83.68%\t Valid: 80.06%\tTest: 72.62%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2659\t Train: 83.18%\t Valid: 79.58%\tTest: 71.92%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2686\t Train: 83.67%\t Valid: 80.26%\tTest: 72.81%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2659\t Train: 83.64%\t Valid: 79.83%\tTest: 72.05%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2644\t Train: 83.82%\t Valid: 80.01%\tTest: 72.56%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2638\t Train: 83.77%\t Valid: 79.78%\tTest: 70.72%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2893\t Train: 81.52%\t Valid: 78.82%\tTest: 66.11%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.3092\t Train: 77.78%\t Valid: 76.44%\tTest: 70.02%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2870\t Train: 80.48%\t Valid: 78.23%\tTest: 71.30%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2773\t Train: 81.82%\t Valid: 79.19%\tTest: 71.44%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2731\t Train: 82.72%\t Valid: 79.89%\tTest: 73.12%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2712\t Train: 83.13%\t Valid: 80.20%\tTest: 73.75%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2685\t Train: 83.18%\t Valid: 79.67%\tTest: 72.34%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2663\t Train: 83.55%\t Valid: 79.83%\tTest: 72.09%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2649\t Train: 83.69%\t Valid: 80.02%\tTest: 72.48%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2662\t Train: 83.61%\t Valid: 79.65%\tTest: 71.92%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2667\t Train: 83.96%\t Valid: 80.30%\tTest: 72.82%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2638\t Train: 84.10%\t Valid: 80.34%\tTest: 72.68%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2655\t Train: 83.75%\t Valid: 80.33%\tTest: 73.51%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2622\t Train: 84.18%\t Valid: 80.18%\tTest: 72.89%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2612\t Train: 84.18%\t Valid: 80.37%\tTest: 73.05%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2617\t Train: 84.22%\t Valid: 80.27%\tTest: 72.80%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2609\t Train: 84.45%\t Valid: 80.31%\tTest: 71.83%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2617\t Train: 84.21%\t Valid: 80.22%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2595\t Train: 84.55%\t Valid: 80.41%\tTest: 71.57%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2589\t Train: 84.50%\t Valid: 80.51%\tTest: 71.12%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2593\t Train: 84.62%\t Valid: 80.69%\tTest: 71.66%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2589\t Train: 84.50%\t Valid: 80.09%\tTest: 70.28%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2577\t Train: 84.91%\t Valid: 80.88%\tTest: 71.85%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2601\t Train: 84.80%\t Valid: 80.53%\tTest: 70.37%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2599\t Train: 84.84%\t Valid: 80.64%\tTest: 69.63%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2574\t Train: 84.79%\t Valid: 80.63%\tTest: 69.95%\n",
      "Seed =  9\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5076\t Train: 38.54%\t Valid: 36.01%\tTest: 38.58%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4235\t Train: 38.25%\t Valid: 33.17%\tTest: 35.73%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3962\t Train: 45.43%\t Valid: 34.78%\tTest: 35.98%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3849\t Train: 49.21%\t Valid: 35.80%\tTest: 36.49%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3774\t Train: 51.24%\t Valid: 37.47%\tTest: 37.11%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3702\t Train: 52.57%\t Valid: 39.52%\tTest: 37.75%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3656\t Train: 54.25%\t Valid: 42.23%\tTest: 39.42%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3566\t Train: 58.47%\t Valid: 49.01%\tTest: 43.10%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3569\t Train: 51.82%\t Valid: 40.79%\tTest: 38.09%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3544\t Train: 57.74%\t Valid: 46.38%\tTest: 42.84%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3454\t Train: 62.11%\t Valid: 52.16%\tTest: 46.27%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3393\t Train: 62.36%\t Valid: 53.23%\tTest: 46.60%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3345\t Train: 64.77%\t Valid: 58.19%\tTest: 49.39%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3323\t Train: 67.61%\t Valid: 63.00%\tTest: 53.31%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3311\t Train: 65.52%\t Valid: 58.91%\tTest: 50.43%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3269\t Train: 66.69%\t Valid: 61.48%\tTest: 51.48%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3249\t Train: 69.09%\t Valid: 65.84%\tTest: 55.41%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3240\t Train: 69.15%\t Valid: 66.11%\tTest: 55.44%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3226\t Train: 69.21%\t Valid: 66.78%\tTest: 55.66%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3194\t Train: 70.33%\t Valid: 68.37%\tTest: 57.23%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3201\t Train: 68.60%\t Valid: 66.85%\tTest: 55.22%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3205\t Train: 69.39%\t Valid: 66.47%\tTest: 55.70%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3170\t Train: 71.20%\t Valid: 69.62%\tTest: 58.09%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3148\t Train: 71.66%\t Valid: 70.89%\tTest: 59.08%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3161\t Train: 71.58%\t Valid: 68.98%\tTest: 58.74%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3143\t Train: 72.04%\t Valid: 69.41%\tTest: 59.48%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3116\t Train: 73.37%\t Valid: 71.41%\tTest: 61.16%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3103\t Train: 72.63%\t Valid: 71.31%\tTest: 60.17%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3082\t Train: 73.84%\t Valid: 72.35%\tTest: 61.64%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3074\t Train: 73.60%\t Valid: 72.20%\tTest: 61.48%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3061\t Train: 74.20%\t Valid: 72.52%\tTest: 61.95%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3071\t Train: 74.00%\t Valid: 72.53%\tTest: 62.32%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3035\t Train: 75.03%\t Valid: 73.45%\tTest: 63.19%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3020\t Train: 75.56%\t Valid: 73.72%\tTest: 63.56%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3027\t Train: 75.14%\t Valid: 72.91%\tTest: 63.19%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2995\t Train: 76.17%\t Valid: 74.26%\tTest: 64.29%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3012\t Train: 76.41%\t Valid: 74.25%\tTest: 65.30%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2988\t Train: 76.96%\t Valid: 74.99%\tTest: 65.64%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2974\t Train: 76.89%\t Valid: 74.73%\tTest: 65.37%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2953\t Train: 77.39%\t Valid: 75.20%\tTest: 65.88%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2952\t Train: 77.82%\t Valid: 75.38%\tTest: 66.33%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2941\t Train: 77.78%\t Valid: 75.09%\tTest: 65.96%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2922\t Train: 78.39%\t Valid: 75.89%\tTest: 67.23%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2929\t Train: 78.27%\t Valid: 75.48%\tTest: 66.65%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2920\t Train: 78.83%\t Valid: 76.06%\tTest: 67.30%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2891\t Train: 78.73%\t Valid: 76.19%\tTest: 66.96%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2883\t Train: 79.51%\t Valid: 76.56%\tTest: 67.63%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2875\t Train: 80.15%\t Valid: 77.07%\tTest: 68.82%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2882\t Train: 79.53%\t Valid: 76.34%\tTest: 68.25%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2859\t Train: 79.74%\t Valid: 76.73%\tTest: 67.83%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2938\t Train: 78.81%\t Valid: 76.43%\tTest: 69.01%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2873\t Train: 79.83%\t Valid: 76.88%\tTest: 68.66%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2844\t Train: 80.29%\t Valid: 77.39%\tTest: 69.18%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2810\t Train: 80.42%\t Valid: 77.25%\tTest: 69.53%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2972\t Train: 77.68%\t Valid: 76.69%\tTest: 70.00%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2889\t Train: 79.78%\t Valid: 77.71%\tTest: 69.84%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2855\t Train: 80.01%\t Valid: 77.51%\tTest: 69.44%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2826\t Train: 80.62%\t Valid: 77.93%\tTest: 69.87%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2823\t Train: 81.14%\t Valid: 78.48%\tTest: 70.96%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2800\t Train: 81.35%\t Valid: 78.42%\tTest: 70.96%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2781\t Train: 81.82%\t Valid: 78.93%\tTest: 71.68%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2864\t Train: 81.97%\t Valid: 78.93%\tTest: 71.94%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2754\t Train: 82.01%\t Valid: 78.97%\tTest: 72.02%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2734\t Train: 81.89%\t Valid: 78.81%\tTest: 71.47%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2854\t Train: 82.26%\t Valid: 79.31%\tTest: 72.17%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2736\t Train: 82.06%\t Valid: 78.74%\tTest: 71.44%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2816\t Train: 82.27%\t Valid: 79.47%\tTest: 72.31%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2713\t Train: 82.39%\t Valid: 78.88%\tTest: 71.04%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2700\t Train: 82.80%\t Valid: 79.44%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2755\t Train: 82.83%\t Valid: 79.38%\tTest: 71.36%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2685\t Train: 82.81%\t Valid: 79.57%\tTest: 72.54%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2680\t Train: 83.26%\t Valid: 79.60%\tTest: 71.53%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2703\t Train: 83.51%\t Valid: 79.74%\tTest: 71.11%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2722\t Train: 83.53%\t Valid: 80.11%\tTest: 72.50%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2650\t Train: 83.46%\t Valid: 79.36%\tTest: 70.43%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2665\t Train: 83.78%\t Valid: 79.94%\tTest: 71.60%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2678\t Train: 83.13%\t Valid: 79.58%\tTest: 68.81%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2648\t Train: 83.67%\t Valid: 79.82%\tTest: 69.41%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2626\t Train: 84.05%\t Valid: 79.91%\tTest: 67.86%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2641\t Train: 83.41%\t Valid: 79.33%\tTest: 64.35%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2643\t Train: 84.21%\t Valid: 79.89%\tTest: 68.51%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2608\t Train: 84.44%\t Valid: 79.79%\tTest: 65.79%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2759\t Train: 83.52%\t Valid: 80.06%\tTest: 63.53%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2689\t Train: 83.38%\t Valid: 79.53%\tTest: 66.82%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2625\t Train: 84.26%\t Valid: 79.68%\tTest: 64.83%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2610\t Train: 84.11%\t Valid: 79.96%\tTest: 62.27%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2578\t Train: 84.69%\t Valid: 80.35%\tTest: 64.61%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2572\t Train: 84.62%\t Valid: 77.81%\tTest: 59.29%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2610\t Train: 84.33%\t Valid: 79.06%\tTest: 59.51%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2585\t Train: 84.56%\t Valid: 79.67%\tTest: 63.78%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2584\t Train: 84.87%\t Valid: 80.17%\tTest: 62.19%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2563\t Train: 85.00%\t Valid: 79.04%\tTest: 58.89%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2585\t Train: 84.58%\t Valid: 78.00%\tTest: 56.39%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2611\t Train: 84.70%\t Valid: 80.28%\tTest: 61.56%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2597\t Train: 84.85%\t Valid: 80.34%\tTest: 59.71%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2581\t Train: 84.49%\t Valid: 80.23%\tTest: 57.50%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2578\t Train: 85.08%\t Valid: 80.33%\tTest: 58.37%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2557\t Train: 85.25%\t Valid: 78.12%\tTest: 56.68%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2542\t Train: 85.04%\t Valid: 78.52%\tTest: 55.32%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2530\t Train: 85.32%\t Valid: 75.74%\tTest: 54.54%\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute GCN normalization.\n",
    "adj_t = data.adj_t.set_diag()\n",
    "deg = adj_t.sum(dim=1).to(torch.float)\n",
    "deg_inv_sqrt = deg.pow(-0.5)\n",
    "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
    "data.adj_t = adj_t\n",
    "    \n",
    "evaluator = Evaluator(name='ogbn-proteins')\n",
    "logger = Logger(args.runs, args)\n",
    "best_test_score = 0\n",
    "\n",
    "\n",
    "for seed in range(10):\n",
    "    print(\"Seed = \",seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    model = GCN(data.num_features, args.hidden_channels, 112, args.num_layers, args.dropout).to(device)\n",
    "    model.reset_parameters()\n",
    "    for run in range(args.runs):        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "\n",
    "            if epoch % args.eval_steps == 0:\n",
    "                result = test(model, data, split_idx, evaluator)\n",
    "                logger.add_result(run, result)\n",
    "\n",
    "                if epoch % args.log_steps == 0:                \n",
    "                    train_rocauc, valid_rocauc, test_rocauc = result\n",
    "                    print(f'Run: {run + 1:02d}\\t '\n",
    "                          f'Epoch: {epoch:02d}\\t '\n",
    "                          f'Loss: {loss:.4f}\\t '\n",
    "                          f'Train: {100 * train_rocauc:.2f}%\\t '\n",
    "                          f'Valid: {100 * valid_rocauc:.2f}%\\t'\n",
    "                          f'Test: {100 * test_rocauc:.2f}%')\n",
    "                    if(test_rocauc > best_test_score):\n",
    "                        best_test_score = test_rocauc\n",
    "                        save_path = \"gcn.pth\"\n",
    "                        torch.save(model, save_path)\n",
    "                        print(\"Model saved.\")\n",
    "        # logger.print_statistics(run)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.38901496723601\n"
     ]
    }
   ],
   "source": [
    "print(best_test_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 hours\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, optimizer\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
