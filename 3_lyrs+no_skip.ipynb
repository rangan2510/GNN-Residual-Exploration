{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "class args:\n",
    "    num_layers = 3\n",
    "    device = 'cuda:0'\n",
    "    log_steps = 1\n",
    "    hidden_channels = 64*4\n",
    "    dropout = 0.2\n",
    "    lr = 0.01\n",
    "    epochs = 1000\n",
    "    eval_steps = 10\n",
    "    runs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import humanize\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\r\n",
      "  Downloading ogb-1.2.1-py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 925 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.18.5)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.1)\r\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.0.3)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Installing collected packages: ogb\r\n",
      "Successfully installed ogb-1.2.1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-scatter==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 8.4 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\r\n",
      "Successfully installed torch-scatter-2.0.5\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-sparse==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.6 MB 4.1 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==latest+cu101) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==latest+cu101) (1.18.5)\r\n",
      "Installing collected packages: torch-sparse\r\n",
      "Successfully installed torch-sparse-0.6.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-cluster==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.3 MB 4.2 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-cluster\r\n",
      "Successfully installed torch-cluster-1.5.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-spline-conv==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 4.6 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\r\n",
      "Successfully installed torch-spline-conv-1.2.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-1.6.0.tar.gz (172 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 172 kB 2.9 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.5.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.18.5)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (4.45.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.4.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.23.1)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.48.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.23.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.0.3)\r\n",
      "Collecting rdflib\r\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 231 kB 9.2 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.10.0)\r\n",
      "Collecting googledrivedownloader\r\n",
      "  Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB)\r\n",
      "Collecting ase\r\n",
      "  Downloading ase-3.19.2-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 9.1 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.11.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torch-geometric) (0.18.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->torch-geometric) (4.4.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (2.1.0)\r\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (0.31.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (1.24.3)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (3.0.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2019.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (1.14.0)\r\n",
      "Collecting isodate\r\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (2.4.7)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ase->torch-geometric) (3.2.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric) (1.1.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (0.10.0)\r\n",
      "Building wheels for collected packages: torch-geometric\r\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-1.6.0-py3-none-any.whl size=296336 sha256=b270115d82b119b5f4e0355d099a6170b2089183d06807a18f2d2e56227c02ca\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/69/d6/b8ed45222466048e1efc27af604aded6825217f0caa6dff569\r\n",
      "Successfully built torch-geometric\r\n",
      "Installing collected packages: isodate, rdflib, googledrivedownloader, ase, torch-geometric\r\n",
      "Successfully installed ase-3.19.2 googledrivedownloader-0.4 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# install Open Graph Benchmark\n",
    "! pip install ogb\n",
    "\n",
    "# install PyTorch Geometric\n",
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR,CosineAnnealingLR\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://snap.stanford.edu/ogb/data/nodeproppred/proteinfunc.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:07<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/proteinfunc.zip\n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 385.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n",
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = f'{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-proteins',\n",
    "                                 transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "\n",
    "# Move edge features to node features.\n",
    "data.x = data.adj_t.mean(dim=1)\n",
    "data.adj_t.set_value_(None)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train'].to(device)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 minutes\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            GCNConv(in_channels, hidden_channels, normalize=False))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, normalize=False))\n",
    "        self.convs.append(\n",
    "            GCNConv(hidden_channels, out_channels, normalize=False))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideResGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
    "        super(WideResGCN, self).__init__()\n",
    "        self.init = GCNConv(in_channels, in_channels, normalize=True)\n",
    "        self.left = GCN(in_channels, hidden_channels, out_channels, num_layers, dropout)\n",
    "        self.right = SAGE(in_channels, hidden_channels, out_channels, num_layers, dropout)\n",
    "        self.final = GCNConv(out_channels*2, out_channels, normalize=True)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.init.reset_parameters()\n",
    "        self.left.reset_parameters()\n",
    "        self.right.reset_parameters()\n",
    "        self.final.reset_parameters()\n",
    "        \n",
    "    def forward(self, x, adj_t):\n",
    "        x = self.init(x, adj_t)        \n",
    "        x_l = self.left(x, adj_t)\n",
    "        x_r = self.right(x, adj_t)\n",
    "        x = torch.cat([x_l, x_r],1)\n",
    "        x = self.final(x, adj_t)\n",
    "        return x                "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = criterion(out, data.y[train_idx].to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(data.x, data.adj_t)\n",
    "\n",
    "    train_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['rocauc']\n",
    "    valid_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['rocauc']\n",
    "    test_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['rocauc']\n",
    "\n",
    "    return train_rocauc, valid_rocauc, test_rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed =  0\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4246\t Train: 36.42%\t Valid: 34.85%\tTest: 37.67%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4167\t Train: 37.87%\t Valid: 33.87%\tTest: 36.12%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3978\t Train: 41.60%\t Valid: 34.30%\tTest: 35.39%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3856\t Train: 46.94%\t Valid: 35.77%\tTest: 35.12%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3763\t Train: 51.18%\t Valid: 38.60%\tTest: 36.26%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3688\t Train: 54.25%\t Valid: 42.65%\tTest: 38.14%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3625\t Train: 55.55%\t Valid: 46.16%\tTest: 39.56%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3574\t Train: 57.16%\t Valid: 48.86%\tTest: 40.90%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3534\t Train: 58.35%\t Valid: 50.87%\tTest: 42.15%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3500\t Train: 59.22%\t Valid: 52.39%\tTest: 43.26%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3471\t Train: 60.19%\t Valid: 53.92%\tTest: 44.69%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3438\t Train: 61.58%\t Valid: 55.83%\tTest: 46.23%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3408\t Train: 63.96%\t Valid: 58.96%\tTest: 48.98%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3373\t Train: 65.29%\t Valid: 60.69%\tTest: 50.71%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3337\t Train: 66.63%\t Valid: 62.63%\tTest: 52.45%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3319\t Train: 67.18%\t Valid: 63.36%\tTest: 53.07%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3283\t Train: 68.75%\t Valid: 65.98%\tTest: 55.16%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3248\t Train: 68.82%\t Valid: 66.20%\tTest: 55.52%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3228\t Train: 69.88%\t Valid: 68.10%\tTest: 57.18%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3210\t Train: 69.49%\t Valid: 67.44%\tTest: 56.65%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3203\t Train: 69.71%\t Valid: 67.74%\tTest: 57.19%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3183\t Train: 71.01%\t Valid: 69.59%\tTest: 58.96%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3169\t Train: 71.36%\t Valid: 69.98%\tTest: 59.26%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3156\t Train: 72.08%\t Valid: 70.92%\tTest: 60.36%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3144\t Train: 72.41%\t Valid: 71.28%\tTest: 60.82%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3133\t Train: 72.63%\t Valid: 71.48%\tTest: 61.13%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3120\t Train: 72.60%\t Valid: 71.36%\tTest: 60.99%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3114\t Train: 72.88%\t Valid: 71.63%\tTest: 61.27%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3111\t Train: 73.48%\t Valid: 72.34%\tTest: 62.09%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3091\t Train: 73.59%\t Valid: 72.37%\tTest: 62.25%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3082\t Train: 73.59%\t Valid: 72.23%\tTest: 62.22%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3072\t Train: 74.29%\t Valid: 73.02%\tTest: 63.21%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3063\t Train: 74.73%\t Valid: 73.41%\tTest: 63.76%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3065\t Train: 75.10%\t Valid: 73.78%\tTest: 64.12%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3056\t Train: 75.05%\t Valid: 73.61%\tTest: 64.03%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3034\t Train: 74.85%\t Valid: 73.35%\tTest: 63.75%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3025\t Train: 75.21%\t Valid: 73.61%\tTest: 64.31%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3018\t Train: 75.47%\t Valid: 73.85%\tTest: 64.72%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3020\t Train: 76.34%\t Valid: 74.51%\tTest: 65.25%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3004\t Train: 76.65%\t Valid: 74.78%\tTest: 65.53%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3005\t Train: 76.86%\t Valid: 74.94%\tTest: 65.79%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2989\t Train: 76.39%\t Valid: 74.40%\tTest: 65.19%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2977\t Train: 76.88%\t Valid: 74.97%\tTest: 66.36%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3010\t Train: 77.49%\t Valid: 75.44%\tTest: 66.57%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2980\t Train: 77.08%\t Valid: 75.13%\tTest: 66.52%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2961\t Train: 77.53%\t Valid: 75.48%\tTest: 67.03%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2950\t Train: 77.51%\t Valid: 75.49%\tTest: 67.05%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2955\t Train: 78.01%\t Valid: 75.67%\tTest: 67.16%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2939\t Train: 78.11%\t Valid: 75.80%\tTest: 67.28%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2934\t Train: 78.27%\t Valid: 75.91%\tTest: 67.30%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2928\t Train: 78.33%\t Valid: 75.99%\tTest: 67.47%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2922\t Train: 78.54%\t Valid: 76.10%\tTest: 67.75%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2916\t Train: 78.26%\t Valid: 75.87%\tTest: 67.79%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2921\t Train: 78.65%\t Valid: 76.08%\tTest: 67.73%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2910\t Train: 78.98%\t Valid: 76.33%\tTest: 68.06%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2902\t Train: 78.50%\t Valid: 76.05%\tTest: 67.97%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2906\t Train: 79.20%\t Valid: 76.44%\tTest: 68.15%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2906\t Train: 78.72%\t Valid: 76.25%\tTest: 68.33%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2883\t Train: 79.29%\t Valid: 76.66%\tTest: 68.77%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2901\t Train: 79.68%\t Valid: 76.83%\tTest: 68.93%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2875\t Train: 79.46%\t Valid: 76.69%\tTest: 68.73%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2878\t Train: 79.59%\t Valid: 76.77%\tTest: 68.97%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2861\t Train: 80.12%\t Valid: 77.16%\tTest: 69.46%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2866\t Train: 79.94%\t Valid: 77.06%\tTest: 69.05%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2858\t Train: 80.17%\t Valid: 77.22%\tTest: 69.35%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2846\t Train: 80.32%\t Valid: 77.29%\tTest: 69.65%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2854\t Train: 80.40%\t Valid: 77.35%\tTest: 69.53%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2840\t Train: 80.51%\t Valid: 77.54%\tTest: 69.74%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2831\t Train: 80.56%\t Valid: 77.61%\tTest: 70.09%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2890\t Train: 80.84%\t Valid: 77.62%\tTest: 69.77%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2837\t Train: 80.81%\t Valid: 77.78%\tTest: 70.31%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2834\t Train: 80.79%\t Valid: 77.79%\tTest: 70.45%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2817\t Train: 80.70%\t Valid: 77.63%\tTest: 69.90%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2836\t Train: 80.92%\t Valid: 77.82%\tTest: 70.26%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2825\t Train: 81.11%\t Valid: 77.88%\tTest: 70.48%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2804\t Train: 80.99%\t Valid: 77.94%\tTest: 70.61%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2804\t Train: 81.12%\t Valid: 78.05%\tTest: 70.76%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2825\t Train: 81.52%\t Valid: 78.18%\tTest: 70.58%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2799\t Train: 81.33%\t Valid: 78.17%\tTest: 70.89%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2788\t Train: 81.40%\t Valid: 78.21%\tTest: 70.97%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2783\t Train: 81.47%\t Valid: 78.28%\tTest: 70.86%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2952\t Train: 81.37%\t Valid: 78.36%\tTest: 70.69%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2795\t Train: 81.02%\t Valid: 78.02%\tTest: 71.15%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2787\t Train: 81.74%\t Valid: 78.21%\tTest: 70.62%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2786\t Train: 81.82%\t Valid: 78.48%\tTest: 71.02%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2770\t Train: 81.87%\t Valid: 78.52%\tTest: 71.11%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2765\t Train: 82.01%\t Valid: 78.62%\tTest: 71.30%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2762\t Train: 81.96%\t Valid: 78.65%\tTest: 71.35%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2773\t Train: 82.01%\t Valid: 78.81%\tTest: 71.81%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2753\t Train: 82.06%\t Valid: 78.78%\tTest: 71.64%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2764\t Train: 82.08%\t Valid: 78.80%\tTest: 71.73%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2750\t Train: 82.41%\t Valid: 78.88%\tTest: 71.54%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2740\t Train: 82.28%\t Valid: 78.84%\tTest: 71.56%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2742\t Train: 82.55%\t Valid: 78.90%\tTest: 71.34%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2758\t Train: 82.45%\t Valid: 78.68%\tTest: 71.12%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2787\t Train: 82.28%\t Valid: 79.01%\tTest: 71.97%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2749\t Train: 82.42%\t Valid: 78.93%\tTest: 71.70%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2740\t Train: 82.66%\t Valid: 79.07%\tTest: 71.69%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2731\t Train: 82.70%\t Valid: 79.12%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2732\t Train: 82.75%\t Valid: 79.24%\tTest: 72.11%\n",
      "Model saved.\n",
      "Seed =  1\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4233\t Train: 37.12%\t Valid: 35.20%\tTest: 37.75%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4174\t Train: 37.97%\t Valid: 33.64%\tTest: 35.83%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3983\t Train: 41.72%\t Valid: 33.97%\tTest: 35.03%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3863\t Train: 46.63%\t Valid: 35.60%\tTest: 35.19%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3781\t Train: 50.82%\t Valid: 38.12%\tTest: 36.38%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3712\t Train: 53.50%\t Valid: 42.20%\tTest: 38.22%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3646\t Train: 54.81%\t Valid: 45.10%\tTest: 39.41%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3587\t Train: 56.24%\t Valid: 47.74%\tTest: 40.56%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3533\t Train: 58.30%\t Valid: 50.50%\tTest: 42.24%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3478\t Train: 61.14%\t Valid: 53.80%\tTest: 45.01%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3426\t Train: 63.60%\t Valid: 56.96%\tTest: 47.84%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3385\t Train: 65.09%\t Valid: 59.20%\tTest: 49.57%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3376\t Train: 65.69%\t Valid: 60.37%\tTest: 50.38%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3339\t Train: 66.62%\t Valid: 62.16%\tTest: 52.05%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3308\t Train: 66.82%\t Valid: 62.80%\tTest: 52.85%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3287\t Train: 68.73%\t Valid: 66.20%\tTest: 55.91%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3261\t Train: 68.69%\t Valid: 66.39%\tTest: 56.31%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3251\t Train: 70.03%\t Valid: 68.61%\tTest: 58.59%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3228\t Train: 69.60%\t Valid: 67.99%\tTest: 58.10%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3210\t Train: 69.97%\t Valid: 68.50%\tTest: 58.74%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3225\t Train: 70.59%\t Valid: 69.33%\tTest: 59.59%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3192\t Train: 71.35%\t Valid: 70.30%\tTest: 60.67%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3174\t Train: 71.53%\t Valid: 70.48%\tTest: 60.98%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3163\t Train: 71.35%\t Valid: 70.24%\tTest: 60.82%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3151\t Train: 71.78%\t Valid: 70.76%\tTest: 61.50%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3138\t Train: 71.94%\t Valid: 70.89%\tTest: 61.73%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3135\t Train: 72.81%\t Valid: 71.95%\tTest: 63.06%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3116\t Train: 72.53%\t Valid: 71.52%\tTest: 62.72%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3107\t Train: 73.02%\t Valid: 72.08%\tTest: 63.26%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3097\t Train: 73.57%\t Valid: 72.67%\tTest: 63.82%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3087\t Train: 72.94%\t Valid: 71.81%\tTest: 62.92%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3077\t Train: 74.32%\t Valid: 73.34%\tTest: 64.67%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3067\t Train: 74.10%\t Valid: 73.01%\tTest: 64.29%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3057\t Train: 74.26%\t Valid: 73.05%\tTest: 64.23%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3061\t Train: 75.21%\t Valid: 74.02%\tTest: 65.15%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3040\t Train: 75.20%\t Valid: 73.88%\tTest: 64.90%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3051\t Train: 75.96%\t Valid: 74.51%\tTest: 65.70%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3036\t Train: 76.07%\t Valid: 74.53%\tTest: 65.66%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3010\t Train: 75.87%\t Valid: 74.27%\tTest: 65.28%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3003\t Train: 76.53%\t Valid: 74.77%\tTest: 65.86%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2994\t Train: 76.58%\t Valid: 74.85%\tTest: 65.88%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2982\t Train: 76.71%\t Valid: 74.81%\tTest: 66.05%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2975\t Train: 76.85%\t Valid: 74.86%\tTest: 66.29%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2967\t Train: 77.42%\t Valid: 75.37%\tTest: 66.64%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2962\t Train: 77.49%\t Valid: 75.39%\tTest: 66.80%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2961\t Train: 78.11%\t Valid: 75.83%\tTest: 66.97%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2945\t Train: 78.11%\t Valid: 75.89%\tTest: 66.93%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2936\t Train: 77.78%\t Valid: 75.52%\tTest: 66.71%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2928\t Train: 78.17%\t Valid: 75.80%\tTest: 67.11%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2921\t Train: 78.23%\t Valid: 75.89%\tTest: 67.25%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2913\t Train: 78.55%\t Valid: 76.17%\tTest: 67.68%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2965\t Train: 78.33%\t Valid: 76.06%\tTest: 67.34%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2915\t Train: 78.39%\t Valid: 76.04%\tTest: 67.44%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2902\t Train: 78.93%\t Valid: 76.33%\tTest: 67.59%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2895\t Train: 78.78%\t Valid: 76.17%\tTest: 67.67%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2888\t Train: 79.06%\t Valid: 76.45%\tTest: 67.98%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2879\t Train: 79.27%\t Valid: 76.66%\tTest: 68.26%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2883\t Train: 79.32%\t Valid: 76.73%\tTest: 68.41%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2875\t Train: 79.38%\t Valid: 76.65%\tTest: 68.39%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2869\t Train: 79.51%\t Valid: 76.74%\tTest: 68.42%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2857\t Train: 79.79%\t Valid: 76.98%\tTest: 68.59%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2910\t Train: 78.88%\t Valid: 76.23%\tTest: 68.38%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2860\t Train: 79.33%\t Valid: 76.68%\tTest: 68.49%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2857\t Train: 80.07%\t Valid: 77.19%\tTest: 68.87%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2841\t Train: 80.32%\t Valid: 77.44%\tTest: 69.45%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2839\t Train: 80.45%\t Valid: 77.47%\tTest: 69.46%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2832\t Train: 80.44%\t Valid: 77.44%\tTest: 69.32%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2833\t Train: 80.64%\t Valid: 77.52%\tTest: 69.54%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2856\t Train: 80.90%\t Valid: 77.84%\tTest: 69.95%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2829\t Train: 80.52%\t Valid: 77.63%\tTest: 69.69%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2818\t Train: 80.79%\t Valid: 77.75%\tTest: 69.95%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2810\t Train: 80.98%\t Valid: 77.88%\tTest: 69.95%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2806\t Train: 80.88%\t Valid: 77.82%\tTest: 70.17%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2831\t Train: 81.19%\t Valid: 78.04%\tTest: 70.17%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2794\t Train: 81.18%\t Valid: 78.05%\tTest: 70.21%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2803\t Train: 80.98%\t Valid: 77.94%\tTest: 70.43%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2799\t Train: 81.37%\t Valid: 78.26%\tTest: 70.49%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2789\t Train: 81.30%\t Valid: 78.17%\tTest: 70.39%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2785\t Train: 81.48%\t Valid: 78.30%\tTest: 70.74%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2824\t Train: 81.71%\t Valid: 78.36%\tTest: 70.38%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2782\t Train: 81.81%\t Valid: 78.51%\tTest: 70.89%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2774\t Train: 81.72%\t Valid: 78.50%\tTest: 70.65%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2779\t Train: 81.62%\t Valid: 78.45%\tTest: 70.84%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2778\t Train: 81.66%\t Valid: 78.49%\tTest: 71.01%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2789\t Train: 81.62%\t Valid: 78.39%\tTest: 71.16%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2765\t Train: 81.89%\t Valid: 78.54%\tTest: 71.27%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2756\t Train: 82.14%\t Valid: 78.76%\tTest: 71.14%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2757\t Train: 82.16%\t Valid: 78.78%\tTest: 71.10%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2754\t Train: 82.16%\t Valid: 78.83%\tTest: 71.23%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2752\t Train: 82.03%\t Valid: 78.69%\tTest: 71.24%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2757\t Train: 82.31%\t Valid: 78.94%\tTest: 71.38%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2740\t Train: 82.43%\t Valid: 78.96%\tTest: 71.63%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2748\t Train: 82.33%\t Valid: 78.94%\tTest: 71.65%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2741\t Train: 82.57%\t Valid: 79.07%\tTest: 71.41%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2729\t Train: 82.64%\t Valid: 79.09%\tTest: 71.75%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2732\t Train: 82.58%\t Valid: 79.15%\tTest: 71.70%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2764\t Train: 82.72%\t Valid: 79.20%\tTest: 71.51%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2743\t Train: 82.80%\t Valid: 79.23%\tTest: 71.77%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2719\t Train: 82.83%\t Valid: 79.27%\tTest: 71.56%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2739\t Train: 82.93%\t Valid: 79.24%\tTest: 71.45%\n",
      "Seed =  2\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4263\t Train: 36.75%\t Valid: 34.32%\tTest: 37.17%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4157\t Train: 38.10%\t Valid: 33.69%\tTest: 35.72%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3977\t Train: 42.10%\t Valid: 34.15%\tTest: 35.14%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3858\t Train: 47.16%\t Valid: 36.10%\tTest: 35.48%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3770\t Train: 51.96%\t Valid: 39.72%\tTest: 36.77%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3698\t Train: 54.54%\t Valid: 44.28%\tTest: 38.77%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3631\t Train: 55.49%\t Valid: 46.65%\tTest: 39.87%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3581\t Train: 56.95%\t Valid: 49.21%\tTest: 41.34%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3540\t Train: 57.94%\t Valid: 50.88%\tTest: 42.22%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3500\t Train: 59.37%\t Valid: 52.64%\tTest: 43.30%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3460\t Train: 61.42%\t Valid: 55.33%\tTest: 45.58%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3418\t Train: 63.37%\t Valid: 58.12%\tTest: 48.13%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3381\t Train: 64.85%\t Valid: 60.20%\tTest: 49.61%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3344\t Train: 66.93%\t Valid: 63.51%\tTest: 51.83%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3317\t Train: 67.58%\t Valid: 64.57%\tTest: 52.58%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3280\t Train: 67.54%\t Valid: 64.56%\tTest: 52.71%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3252\t Train: 68.79%\t Valid: 66.76%\tTest: 55.00%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3244\t Train: 69.04%\t Valid: 67.23%\tTest: 56.09%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3211\t Train: 70.34%\t Valid: 69.20%\tTest: 58.14%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3195\t Train: 70.80%\t Valid: 69.73%\tTest: 58.97%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3177\t Train: 70.95%\t Valid: 69.73%\tTest: 59.21%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3177\t Train: 70.51%\t Valid: 68.75%\tTest: 58.56%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3156\t Train: 71.27%\t Valid: 69.79%\tTest: 59.55%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3141\t Train: 72.84%\t Valid: 72.04%\tTest: 61.80%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3140\t Train: 71.77%\t Valid: 70.26%\tTest: 60.18%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3132\t Train: 72.86%\t Valid: 71.75%\tTest: 61.62%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3113\t Train: 73.72%\t Valid: 72.76%\tTest: 62.77%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3095\t Train: 73.54%\t Valid: 72.40%\tTest: 62.27%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3083\t Train: 73.94%\t Valid: 72.68%\tTest: 62.68%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3075\t Train: 74.22%\t Valid: 72.96%\tTest: 62.79%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3075\t Train: 74.89%\t Valid: 73.59%\tTest: 63.50%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3055\t Train: 74.71%\t Valid: 73.22%\tTest: 63.15%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3048\t Train: 75.06%\t Valid: 73.45%\tTest: 63.67%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3038\t Train: 75.58%\t Valid: 73.87%\tTest: 64.13%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3037\t Train: 75.65%\t Valid: 73.86%\tTest: 64.13%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3018\t Train: 75.93%\t Valid: 74.13%\tTest: 64.77%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3017\t Train: 75.69%\t Valid: 73.73%\tTest: 64.36%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3020\t Train: 75.63%\t Valid: 73.64%\tTest: 64.32%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3004\t Train: 76.41%\t Valid: 74.36%\tTest: 64.91%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2988\t Train: 76.27%\t Valid: 74.12%\tTest: 64.87%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2986\t Train: 76.79%\t Valid: 74.53%\tTest: 65.58%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3008\t Train: 77.09%\t Valid: 74.85%\tTest: 65.64%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3003\t Train: 76.72%\t Valid: 74.49%\tTest: 65.33%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2968\t Train: 77.01%\t Valid: 74.78%\tTest: 65.87%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2962\t Train: 77.07%\t Valid: 74.72%\tTest: 66.08%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2959\t Train: 77.08%\t Valid: 74.69%\tTest: 66.37%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2948\t Train: 77.51%\t Valid: 75.09%\tTest: 66.76%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2942\t Train: 77.90%\t Valid: 75.42%\tTest: 67.10%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2945\t Train: 78.19%\t Valid: 75.59%\tTest: 67.29%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2940\t Train: 78.06%\t Valid: 75.51%\tTest: 67.12%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2930\t Train: 78.27%\t Valid: 75.67%\tTest: 67.47%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2979\t Train: 77.63%\t Valid: 75.12%\tTest: 67.49%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2929\t Train: 78.84%\t Valid: 76.13%\tTest: 68.32%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2922\t Train: 78.48%\t Valid: 75.84%\tTest: 68.12%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2905\t Train: 78.84%\t Valid: 76.15%\tTest: 68.56%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2907\t Train: 78.69%\t Valid: 76.03%\tTest: 68.50%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2899\t Train: 79.22%\t Valid: 76.46%\tTest: 68.76%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2888\t Train: 79.07%\t Valid: 76.22%\tTest: 68.75%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2898\t Train: 78.49%\t Valid: 75.54%\tTest: 68.41%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2882\t Train: 79.52%\t Valid: 76.64%\tTest: 68.93%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2875\t Train: 79.49%\t Valid: 76.68%\tTest: 69.14%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2871\t Train: 79.72%\t Valid: 76.85%\tTest: 69.32%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2876\t Train: 79.55%\t Valid: 76.66%\tTest: 69.30%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2859\t Train: 79.96%\t Valid: 77.08%\tTest: 69.61%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2917\t Train: 80.16%\t Valid: 77.33%\tTest: 69.56%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2860\t Train: 80.05%\t Valid: 77.10%\tTest: 69.64%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2851\t Train: 80.23%\t Valid: 77.29%\tTest: 69.84%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2867\t Train: 80.08%\t Valid: 77.23%\tTest: 69.94%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2839\t Train: 80.52%\t Valid: 77.50%\tTest: 70.10%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2833\t Train: 80.56%\t Valid: 77.54%\tTest: 70.22%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2901\t Train: 79.83%\t Valid: 76.94%\tTest: 70.10%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2831\t Train: 80.76%\t Valid: 77.80%\tTest: 70.30%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2823\t Train: 80.69%\t Valid: 77.73%\tTest: 70.42%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2825\t Train: 81.00%\t Valid: 77.95%\tTest: 70.62%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2816\t Train: 80.74%\t Valid: 77.76%\tTest: 70.78%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2810\t Train: 81.21%\t Valid: 78.10%\tTest: 70.83%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2806\t Train: 81.03%\t Valid: 78.02%\tTest: 70.95%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2876\t Train: 81.34%\t Valid: 78.28%\tTest: 71.04%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2805\t Train: 81.43%\t Valid: 78.33%\tTest: 70.89%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2812\t Train: 81.31%\t Valid: 78.24%\tTest: 71.14%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2797\t Train: 81.51%\t Valid: 78.39%\tTest: 71.07%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2795\t Train: 81.54%\t Valid: 78.41%\tTest: 71.16%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2794\t Train: 81.65%\t Valid: 78.48%\tTest: 71.30%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2780\t Train: 81.64%\t Valid: 78.52%\tTest: 71.30%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2832\t Train: 81.90%\t Valid: 78.69%\tTest: 71.30%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2773\t Train: 81.70%\t Valid: 78.52%\tTest: 71.61%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2778\t Train: 81.79%\t Valid: 78.58%\tTest: 71.75%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2762\t Train: 82.01%\t Valid: 78.82%\tTest: 71.74%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2777\t Train: 82.11%\t Valid: 78.90%\tTest: 71.79%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2773\t Train: 81.78%\t Valid: 78.63%\tTest: 72.10%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2776\t Train: 81.97%\t Valid: 78.81%\tTest: 72.06%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2754\t Train: 82.14%\t Valid: 78.86%\tTest: 72.06%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2763\t Train: 82.30%\t Valid: 79.01%\tTest: 72.00%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2819\t Train: 82.48%\t Valid: 79.20%\tTest: 72.05%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2747\t Train: 82.25%\t Valid: 79.02%\tTest: 72.33%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2743\t Train: 82.47%\t Valid: 79.10%\tTest: 72.44%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2752\t Train: 82.38%\t Valid: 79.12%\tTest: 72.56%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2741\t Train: 82.46%\t Valid: 79.17%\tTest: 72.37%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2729\t Train: 82.53%\t Valid: 79.18%\tTest: 72.61%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2817\t Train: 82.71%\t Valid: 79.35%\tTest: 72.22%\n",
      "Seed =  3\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4209\t Train: 38.36%\t Valid: 36.16%\tTest: 38.34%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4141\t Train: 38.62%\t Valid: 33.95%\tTest: 36.01%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3973\t Train: 42.10%\t Valid: 34.19%\tTest: 35.21%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3853\t Train: 47.37%\t Valid: 35.82%\tTest: 35.31%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3767\t Train: 51.63%\t Valid: 38.52%\tTest: 36.29%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3690\t Train: 54.30%\t Valid: 42.74%\tTest: 38.53%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3628\t Train: 55.42%\t Valid: 46.36%\tTest: 40.14%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3576\t Train: 56.87%\t Valid: 49.05%\tTest: 41.15%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3539\t Train: 58.21%\t Valid: 51.09%\tTest: 42.31%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3510\t Train: 59.07%\t Valid: 52.28%\tTest: 42.94%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3481\t Train: 59.84%\t Valid: 53.39%\tTest: 43.79%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3448\t Train: 61.09%\t Valid: 55.08%\tTest: 45.00%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3415\t Train: 62.46%\t Valid: 57.08%\tTest: 46.62%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3381\t Train: 64.63%\t Valid: 60.12%\tTest: 49.19%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3357\t Train: 66.32%\t Valid: 62.53%\tTest: 51.11%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3317\t Train: 66.71%\t Valid: 63.17%\tTest: 51.48%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3287\t Train: 68.02%\t Valid: 65.53%\tTest: 53.50%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3266\t Train: 68.44%\t Valid: 66.45%\tTest: 54.47%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3246\t Train: 68.78%\t Valid: 67.19%\tTest: 55.61%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3224\t Train: 70.16%\t Valid: 69.39%\tTest: 57.80%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3208\t Train: 69.97%\t Valid: 69.06%\tTest: 57.88%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3200\t Train: 70.47%\t Valid: 69.68%\tTest: 58.50%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3181\t Train: 70.62%\t Valid: 69.80%\tTest: 58.78%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3166\t Train: 71.50%\t Valid: 70.85%\tTest: 59.86%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3153\t Train: 71.14%\t Valid: 70.23%\tTest: 59.40%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3145\t Train: 71.73%\t Valid: 70.97%\tTest: 60.16%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3131\t Train: 72.35%\t Valid: 71.65%\tTest: 61.01%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3129\t Train: 72.96%\t Valid: 72.35%\tTest: 61.96%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3112\t Train: 73.08%\t Valid: 72.41%\tTest: 62.18%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3104\t Train: 72.83%\t Valid: 72.05%\tTest: 61.87%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3091\t Train: 73.24%\t Valid: 72.47%\tTest: 62.59%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3089\t Train: 72.45%\t Valid: 71.42%\tTest: 61.49%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3072\t Train: 74.09%\t Valid: 73.30%\tTest: 63.50%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3063\t Train: 74.20%\t Valid: 73.41%\tTest: 63.59%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3054\t Train: 74.05%\t Valid: 73.08%\tTest: 63.43%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3040\t Train: 74.79%\t Valid: 73.76%\tTest: 64.31%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3032\t Train: 75.41%\t Valid: 74.32%\tTest: 64.90%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3030\t Train: 75.11%\t Valid: 73.91%\tTest: 64.58%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3039\t Train: 75.06%\t Valid: 73.72%\tTest: 64.30%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3010\t Train: 75.89%\t Valid: 74.57%\tTest: 65.15%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3002\t Train: 76.70%\t Valid: 75.15%\tTest: 65.69%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2989\t Train: 76.38%\t Valid: 74.74%\tTest: 65.31%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2979\t Train: 76.67%\t Valid: 74.99%\tTest: 65.86%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2973\t Train: 77.19%\t Valid: 75.36%\tTest: 65.91%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2978\t Train: 77.65%\t Valid: 75.58%\tTest: 66.30%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2966\t Train: 77.18%\t Valid: 75.18%\tTest: 66.13%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2948\t Train: 77.58%\t Valid: 75.29%\tTest: 66.09%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2947\t Train: 77.77%\t Valid: 75.44%\tTest: 66.41%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2941\t Train: 77.80%\t Valid: 75.63%\tTest: 66.67%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2946\t Train: 78.58%\t Valid: 76.22%\tTest: 67.43%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2922\t Train: 78.60%\t Valid: 76.26%\tTest: 67.77%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2915\t Train: 78.62%\t Valid: 76.30%\tTest: 67.81%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2907\t Train: 78.62%\t Valid: 76.28%\tTest: 67.65%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2909\t Train: 79.39%\t Valid: 76.72%\tTest: 68.15%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2913\t Train: 79.10%\t Valid: 76.56%\tTest: 67.96%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2913\t Train: 78.65%\t Valid: 76.22%\tTest: 67.83%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2891\t Train: 79.00%\t Valid: 76.68%\tTest: 68.53%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2886\t Train: 79.21%\t Valid: 76.77%\tTest: 68.51%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2886\t Train: 79.11%\t Valid: 76.68%\tTest: 68.55%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2876\t Train: 79.56%\t Valid: 76.92%\tTest: 68.60%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2870\t Train: 79.77%\t Valid: 77.08%\tTest: 68.99%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2869\t Train: 79.95%\t Valid: 77.23%\tTest: 69.37%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2860\t Train: 79.69%\t Valid: 77.27%\tTest: 69.23%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2862\t Train: 79.83%\t Valid: 77.36%\tTest: 69.32%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2851\t Train: 79.89%\t Valid: 77.23%\tTest: 69.46%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2849\t Train: 80.00%\t Valid: 77.28%\tTest: 69.83%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2849\t Train: 80.28%\t Valid: 77.64%\tTest: 70.00%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2861\t Train: 80.24%\t Valid: 77.73%\tTest: 70.02%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2839\t Train: 80.69%\t Valid: 77.79%\tTest: 69.94%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2850\t Train: 80.75%\t Valid: 77.77%\tTest: 69.78%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2823\t Train: 80.55%\t Valid: 77.72%\tTest: 70.05%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2816\t Train: 80.64%\t Valid: 77.69%\tTest: 70.26%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2820\t Train: 80.56%\t Valid: 77.74%\tTest: 70.32%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2815\t Train: 81.02%\t Valid: 77.98%\tTest: 70.02%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2807\t Train: 81.13%\t Valid: 78.01%\tTest: 70.09%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2804\t Train: 81.27%\t Valid: 78.09%\tTest: 70.38%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2803\t Train: 81.09%\t Valid: 78.10%\tTest: 70.58%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2822\t Train: 80.85%\t Valid: 77.99%\tTest: 70.92%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2792\t Train: 81.32%\t Valid: 78.23%\tTest: 70.72%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2792\t Train: 81.50%\t Valid: 78.27%\tTest: 70.69%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2780\t Train: 81.62%\t Valid: 78.38%\tTest: 70.79%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2789\t Train: 81.80%\t Valid: 78.38%\tTest: 70.48%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2791\t Train: 81.73%\t Valid: 78.45%\tTest: 71.16%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2785\t Train: 81.69%\t Valid: 78.41%\tTest: 71.28%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2769\t Train: 81.89%\t Valid: 78.57%\tTest: 71.20%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2767\t Train: 81.94%\t Valid: 78.62%\tTest: 71.38%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2756\t Train: 82.04%\t Valid: 78.71%\tTest: 71.63%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2814\t Train: 82.01%\t Valid: 78.38%\tTest: 70.35%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2781\t Train: 81.96%\t Valid: 78.41%\tTest: 71.39%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2777\t Train: 82.15%\t Valid: 78.89%\tTest: 71.86%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2753\t Train: 82.18%\t Valid: 78.78%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2747\t Train: 82.33%\t Valid: 78.89%\tTest: 71.77%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2744\t Train: 82.55%\t Valid: 79.04%\tTest: 71.70%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2733\t Train: 82.58%\t Valid: 79.06%\tTest: 71.98%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2737\t Train: 82.52%\t Valid: 79.05%\tTest: 72.20%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2733\t Train: 82.62%\t Valid: 79.08%\tTest: 72.15%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.3008\t Train: 82.75%\t Valid: 78.92%\tTest: 71.18%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2752\t Train: 82.75%\t Valid: 79.18%\tTest: 71.96%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2726\t Train: 82.68%\t Valid: 79.12%\tTest: 72.12%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2717\t Train: 82.76%\t Valid: 79.19%\tTest: 72.45%\n",
      "Seed =  4\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4254\t Train: 37.10%\t Valid: 35.17%\tTest: 37.63%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4170\t Train: 37.90%\t Valid: 33.69%\tTest: 35.88%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3977\t Train: 41.59%\t Valid: 34.14%\tTest: 35.16%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3866\t Train: 46.32%\t Valid: 35.52%\tTest: 35.12%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3775\t Train: 50.71%\t Valid: 38.48%\tTest: 36.42%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3702\t Train: 53.78%\t Valid: 42.59%\tTest: 38.28%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3633\t Train: 55.24%\t Valid: 45.96%\tTest: 39.76%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3571\t Train: 56.98%\t Valid: 48.72%\tTest: 41.21%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3516\t Train: 59.39%\t Valid: 51.64%\tTest: 43.14%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3467\t Train: 61.90%\t Valid: 54.39%\tTest: 45.28%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3427\t Train: 63.35%\t Valid: 56.03%\tTest: 46.54%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3394\t Train: 64.75%\t Valid: 57.94%\tTest: 48.15%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3366\t Train: 65.86%\t Valid: 59.91%\tTest: 49.63%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3344\t Train: 66.11%\t Valid: 60.87%\tTest: 50.53%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3320\t Train: 66.55%\t Valid: 62.02%\tTest: 51.79%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3294\t Train: 67.38%\t Valid: 63.63%\tTest: 53.48%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3272\t Train: 68.48%\t Valid: 65.59%\tTest: 55.40%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3249\t Train: 70.02%\t Valid: 68.17%\tTest: 58.04%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3221\t Train: 70.15%\t Valid: 68.48%\tTest: 58.83%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3215\t Train: 69.36%\t Valid: 67.34%\tTest: 58.14%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3204\t Train: 71.30%\t Valid: 70.06%\tTest: 60.93%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3184\t Train: 71.31%\t Valid: 70.04%\tTest: 60.85%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3163\t Train: 71.28%\t Valid: 69.91%\tTest: 60.81%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3155\t Train: 71.57%\t Valid: 70.21%\tTest: 61.20%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3145\t Train: 71.85%\t Valid: 70.53%\tTest: 61.53%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3135\t Train: 72.91%\t Valid: 71.79%\tTest: 62.94%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3122\t Train: 72.98%\t Valid: 71.79%\tTest: 62.85%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3115\t Train: 74.00%\t Valid: 72.97%\tTest: 64.05%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3127\t Train: 73.58%\t Valid: 72.40%\tTest: 63.75%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3096\t Train: 73.65%\t Valid: 72.49%\tTest: 63.78%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3082\t Train: 74.11%\t Valid: 72.90%\tTest: 64.20%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3067\t Train: 74.42%\t Valid: 73.08%\tTest: 64.37%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3056\t Train: 74.65%\t Valid: 73.23%\tTest: 64.45%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3050\t Train: 74.91%\t Valid: 73.34%\tTest: 64.55%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3040\t Train: 74.97%\t Valid: 73.35%\tTest: 64.46%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3037\t Train: 75.87%\t Valid: 74.24%\tTest: 65.45%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3021\t Train: 75.59%\t Valid: 73.78%\tTest: 65.01%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3030\t Train: 76.41%\t Valid: 74.72%\tTest: 65.79%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3010\t Train: 76.56%\t Valid: 74.69%\tTest: 65.99%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2996\t Train: 76.41%\t Valid: 74.48%\tTest: 65.67%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3003\t Train: 76.01%\t Valid: 74.02%\tTest: 65.17%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2980\t Train: 76.80%\t Valid: 74.74%\tTest: 66.14%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2972\t Train: 77.05%\t Valid: 75.00%\tTest: 66.36%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2997\t Train: 77.64%\t Valid: 75.50%\tTest: 66.87%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2960\t Train: 77.17%\t Valid: 75.06%\tTest: 66.54%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2955\t Train: 77.37%\t Valid: 75.07%\tTest: 66.56%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2948\t Train: 77.73%\t Valid: 75.51%\tTest: 67.06%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2942\t Train: 78.07%\t Valid: 75.65%\tTest: 67.20%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2936\t Train: 77.99%\t Valid: 75.63%\tTest: 67.26%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2934\t Train: 78.47%\t Valid: 75.91%\tTest: 67.56%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2929\t Train: 78.18%\t Valid: 75.74%\tTest: 67.42%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2917\t Train: 78.53%\t Valid: 76.22%\tTest: 68.11%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2912\t Train: 78.46%\t Valid: 76.03%\tTest: 68.01%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2907\t Train: 78.85%\t Valid: 76.27%\tTest: 68.26%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2904\t Train: 78.73%\t Valid: 76.26%\tTest: 68.43%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2912\t Train: 78.73%\t Valid: 76.16%\tTest: 68.32%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2895\t Train: 78.89%\t Valid: 76.25%\tTest: 68.59%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2905\t Train: 78.67%\t Valid: 76.26%\tTest: 68.89%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2898\t Train: 79.13%\t Valid: 76.58%\tTest: 69.03%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2877\t Train: 79.59%\t Valid: 76.81%\tTest: 68.93%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2873\t Train: 79.54%\t Valid: 76.82%\tTest: 69.37%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2867\t Train: 79.67%\t Valid: 76.91%\tTest: 69.34%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2873\t Train: 79.61%\t Valid: 76.87%\tTest: 69.51%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2881\t Train: 79.57%\t Valid: 76.98%\tTest: 70.16%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2855\t Train: 80.07%\t Valid: 77.20%\tTest: 69.66%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2855\t Train: 80.13%\t Valid: 77.27%\tTest: 69.85%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2845\t Train: 80.36%\t Valid: 77.44%\tTest: 70.12%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2858\t Train: 80.29%\t Valid: 77.27%\tTest: 69.72%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2840\t Train: 80.37%\t Valid: 77.44%\tTest: 70.00%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2838\t Train: 80.60%\t Valid: 77.61%\tTest: 70.41%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2828\t Train: 80.65%\t Valid: 77.61%\tTest: 70.42%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2835\t Train: 80.43%\t Valid: 77.57%\tTest: 70.31%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2822\t Train: 80.82%\t Valid: 77.78%\tTest: 70.67%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2824\t Train: 80.42%\t Valid: 77.64%\tTest: 71.34%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2826\t Train: 80.72%\t Valid: 77.88%\tTest: 70.92%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2811\t Train: 81.02%\t Valid: 77.94%\tTest: 70.99%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2806\t Train: 81.08%\t Valid: 78.04%\tTest: 71.21%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2804\t Train: 80.98%\t Valid: 78.02%\tTest: 71.34%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2811\t Train: 81.22%\t Valid: 78.14%\tTest: 71.23%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2806\t Train: 81.16%\t Valid: 78.17%\tTest: 71.56%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2793\t Train: 81.33%\t Valid: 78.27%\tTest: 71.59%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2783\t Train: 81.47%\t Valid: 78.36%\tTest: 71.63%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2783\t Train: 81.61%\t Valid: 78.37%\tTest: 71.43%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2776\t Train: 81.57%\t Valid: 78.42%\tTest: 71.65%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2816\t Train: 81.38%\t Valid: 78.35%\tTest: 71.59%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2773\t Train: 81.74%\t Valid: 78.42%\tTest: 71.56%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2766\t Train: 81.90%\t Valid: 78.61%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2764\t Train: 82.02%\t Valid: 78.74%\tTest: 72.00%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2766\t Train: 81.90%\t Valid: 78.79%\tTest: 72.34%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2754\t Train: 82.19%\t Valid: 78.81%\tTest: 72.04%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2758\t Train: 82.02%\t Valid: 78.92%\tTest: 72.50%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2775\t Train: 82.26%\t Valid: 78.86%\tTest: 72.07%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2750\t Train: 82.20%\t Valid: 78.93%\tTest: 72.34%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2738\t Train: 82.33%\t Valid: 78.94%\tTest: 72.39%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2842\t Train: 82.41%\t Valid: 78.78%\tTest: 71.71%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2758\t Train: 82.10%\t Valid: 78.87%\tTest: 72.95%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2736\t Train: 82.50%\t Valid: 78.89%\tTest: 71.79%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2733\t Train: 82.54%\t Valid: 79.09%\tTest: 72.36%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2738\t Train: 82.72%\t Valid: 79.13%\tTest: 72.33%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2743\t Train: 82.76%\t Valid: 79.09%\tTest: 72.09%\n",
      "Seed =  5\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4235\t Train: 37.23%\t Valid: 34.61%\tTest: 37.30%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4158\t Train: 38.31%\t Valid: 33.65%\tTest: 35.80%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3978\t Train: 41.85%\t Valid: 34.07%\tTest: 35.19%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3873\t Train: 46.24%\t Valid: 35.54%\tTest: 35.29%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3791\t Train: 49.98%\t Valid: 37.52%\tTest: 36.09%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3727\t Train: 52.52%\t Valid: 40.31%\tTest: 37.18%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3658\t Train: 55.02%\t Valid: 43.85%\tTest: 38.58%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3598\t Train: 56.67%\t Valid: 47.25%\tTest: 39.83%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3549\t Train: 58.36%\t Valid: 50.12%\tTest: 41.20%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3508\t Train: 58.74%\t Valid: 51.11%\tTest: 41.57%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3470\t Train: 60.53%\t Valid: 53.40%\tTest: 42.96%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3428\t Train: 62.51%\t Valid: 55.91%\tTest: 44.49%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3389\t Train: 63.97%\t Valid: 57.84%\tTest: 45.79%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3358\t Train: 65.53%\t Valid: 60.27%\tTest: 47.24%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3317\t Train: 67.04%\t Valid: 63.06%\tTest: 49.27%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3309\t Train: 67.19%\t Valid: 63.69%\tTest: 50.45%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3272\t Train: 67.77%\t Valid: 64.91%\tTest: 52.32%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3251\t Train: 69.17%\t Valid: 67.29%\tTest: 54.64%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3228\t Train: 69.40%\t Valid: 67.70%\tTest: 55.01%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3215\t Train: 70.17%\t Valid: 68.86%\tTest: 56.06%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3199\t Train: 70.10%\t Valid: 68.67%\tTest: 55.94%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3192\t Train: 71.29%\t Valid: 70.32%\tTest: 58.01%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3184\t Train: 70.56%\t Valid: 69.19%\tTest: 56.97%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3168\t Train: 71.11%\t Valid: 69.91%\tTest: 57.82%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3155\t Train: 71.52%\t Valid: 70.39%\tTest: 58.39%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3152\t Train: 70.86%\t Valid: 69.33%\tTest: 57.30%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3144\t Train: 71.94%\t Valid: 70.84%\tTest: 58.97%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3126\t Train: 71.98%\t Valid: 70.79%\tTest: 59.27%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3119\t Train: 73.18%\t Valid: 72.26%\tTest: 60.97%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3109\t Train: 72.65%\t Valid: 71.54%\tTest: 60.05%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3103\t Train: 73.16%\t Valid: 72.11%\tTest: 60.84%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3092\t Train: 73.41%\t Valid: 72.33%\tTest: 61.18%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3083\t Train: 73.47%\t Valid: 72.34%\tTest: 61.44%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3099\t Train: 74.25%\t Valid: 73.13%\tTest: 62.45%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3067\t Train: 73.73%\t Valid: 72.50%\tTest: 61.87%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3064\t Train: 74.11%\t Valid: 72.84%\tTest: 62.35%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3053\t Train: 74.83%\t Valid: 73.55%\tTest: 63.48%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3045\t Train: 75.29%\t Valid: 73.94%\tTest: 63.99%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3045\t Train: 75.88%\t Valid: 74.44%\tTest: 64.81%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3035\t Train: 75.28%\t Valid: 73.80%\tTest: 64.05%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3017\t Train: 75.86%\t Valid: 74.23%\tTest: 64.55%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3010\t Train: 75.27%\t Valid: 73.54%\tTest: 63.88%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3002\t Train: 76.49%\t Valid: 74.71%\tTest: 65.40%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2999\t Train: 76.27%\t Valid: 74.43%\tTest: 65.13%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2984\t Train: 76.54%\t Valid: 74.63%\tTest: 65.56%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2976\t Train: 76.83%\t Valid: 74.80%\tTest: 65.68%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2967\t Train: 77.07%\t Valid: 74.93%\tTest: 65.96%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2965\t Train: 76.94%\t Valid: 74.79%\tTest: 65.85%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2955\t Train: 77.35%\t Valid: 75.16%\tTest: 66.53%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2954\t Train: 77.32%\t Valid: 75.09%\tTest: 66.54%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2949\t Train: 77.86%\t Valid: 75.42%\tTest: 66.59%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2944\t Train: 77.70%\t Valid: 75.31%\tTest: 66.49%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2939\t Train: 78.38%\t Valid: 75.87%\tTest: 67.22%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2925\t Train: 78.21%\t Valid: 75.78%\tTest: 67.20%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2927\t Train: 78.09%\t Valid: 75.67%\tTest: 67.30%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2913\t Train: 78.35%\t Valid: 75.73%\tTest: 67.22%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2911\t Train: 78.68%\t Valid: 76.14%\tTest: 67.71%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2917\t Train: 78.13%\t Valid: 75.71%\tTest: 67.23%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2896\t Train: 78.78%\t Valid: 76.21%\tTest: 67.90%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2893\t Train: 79.27%\t Valid: 76.53%\tTest: 68.41%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2889\t Train: 79.02%\t Valid: 76.44%\tTest: 68.38%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2889\t Train: 79.59%\t Valid: 76.80%\tTest: 68.51%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2889\t Train: 79.60%\t Valid: 76.74%\tTest: 68.49%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2875\t Train: 79.77%\t Valid: 77.01%\tTest: 68.87%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2863\t Train: 79.83%\t Valid: 77.07%\tTest: 69.11%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2873\t Train: 79.39%\t Valid: 76.85%\tTest: 69.22%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2860\t Train: 79.73%\t Valid: 77.24%\tTest: 69.49%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2853\t Train: 80.20%\t Valid: 77.35%\tTest: 69.06%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2844\t Train: 80.15%\t Valid: 77.31%\tTest: 69.26%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2841\t Train: 80.37%\t Valid: 77.39%\tTest: 69.66%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2882\t Train: 80.78%\t Valid: 77.58%\tTest: 69.39%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2893\t Train: 80.23%\t Valid: 77.49%\tTest: 69.25%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2843\t Train: 80.41%\t Valid: 77.54%\tTest: 69.45%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2827\t Train: 80.73%\t Valid: 77.88%\tTest: 70.36%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2815\t Train: 80.93%\t Valid: 78.08%\tTest: 70.61%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2811\t Train: 80.77%\t Valid: 77.84%\tTest: 70.40%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2806\t Train: 81.02%\t Valid: 78.07%\tTest: 70.61%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2803\t Train: 81.10%\t Valid: 78.09%\tTest: 70.81%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2793\t Train: 81.41%\t Valid: 78.24%\tTest: 70.79%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2797\t Train: 81.54%\t Valid: 78.37%\tTest: 70.80%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2825\t Train: 81.10%\t Valid: 78.20%\tTest: 70.81%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2797\t Train: 81.50%\t Valid: 78.38%\tTest: 71.04%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2785\t Train: 81.54%\t Valid: 78.38%\tTest: 70.83%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2791\t Train: 81.59%\t Valid: 78.52%\tTest: 71.31%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2772\t Train: 81.69%\t Valid: 78.49%\tTest: 71.35%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2782\t Train: 82.05%\t Valid: 78.68%\tTest: 71.13%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2761\t Train: 82.13%\t Valid: 78.73%\tTest: 71.26%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2765\t Train: 81.91%\t Valid: 78.69%\tTest: 71.46%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2756\t Train: 82.24%\t Valid: 78.73%\tTest: 71.13%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2772\t Train: 82.39%\t Valid: 78.85%\tTest: 71.46%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2767\t Train: 82.21%\t Valid: 78.94%\tTest: 71.56%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2751\t Train: 82.39%\t Valid: 78.86%\tTest: 71.38%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2766\t Train: 82.27%\t Valid: 78.99%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2745\t Train: 82.34%\t Valid: 78.91%\tTest: 71.55%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2734\t Train: 82.49%\t Valid: 79.08%\tTest: 71.79%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2738\t Train: 82.21%\t Valid: 78.85%\tTest: 72.00%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2749\t Train: 82.74%\t Valid: 79.08%\tTest: 71.58%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2725\t Train: 82.66%\t Valid: 79.21%\tTest: 72.09%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2727\t Train: 82.66%\t Valid: 79.08%\tTest: 71.84%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2762\t Train: 82.97%\t Valid: 79.16%\tTest: 71.56%\n",
      "Seed =  6\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4255\t Train: 37.08%\t Valid: 35.11%\tTest: 37.76%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4166\t Train: 38.14%\t Valid: 33.85%\tTest: 36.09%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3977\t Train: 41.73%\t Valid: 34.31%\tTest: 35.45%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3859\t Train: 47.05%\t Valid: 36.26%\tTest: 35.64%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3775\t Train: 51.30%\t Valid: 39.36%\tTest: 36.99%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3704\t Train: 53.65%\t Valid: 42.85%\tTest: 38.61%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3639\t Train: 55.43%\t Valid: 46.47%\tTest: 40.30%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3585\t Train: 56.66%\t Valid: 48.80%\tTest: 41.48%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3545\t Train: 58.12%\t Valid: 50.97%\tTest: 42.81%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3512\t Train: 59.21%\t Valid: 52.50%\tTest: 43.75%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3483\t Train: 60.03%\t Valid: 53.59%\tTest: 44.49%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3452\t Train: 61.05%\t Valid: 54.98%\tTest: 45.41%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3418\t Train: 62.78%\t Valid: 57.45%\tTest: 47.28%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3410\t Train: 64.02%\t Valid: 59.17%\tTest: 49.24%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3355\t Train: 64.66%\t Valid: 60.00%\tTest: 50.16%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3329\t Train: 66.59%\t Valid: 62.89%\tTest: 52.64%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3296\t Train: 67.32%\t Valid: 63.98%\tTest: 53.88%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3268\t Train: 68.82%\t Valid: 66.30%\tTest: 56.17%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3244\t Train: 69.34%\t Valid: 67.13%\tTest: 56.99%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3219\t Train: 69.98%\t Valid: 68.25%\tTest: 58.34%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3212\t Train: 69.34%\t Valid: 67.27%\tTest: 57.27%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3190\t Train: 69.96%\t Valid: 68.20%\tTest: 58.33%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3179\t Train: 70.61%\t Valid: 69.05%\tTest: 59.25%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3164\t Train: 71.27%\t Valid: 69.87%\tTest: 60.23%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3167\t Train: 71.23%\t Valid: 69.68%\tTest: 60.16%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3144\t Train: 72.05%\t Valid: 70.76%\tTest: 61.41%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3134\t Train: 72.45%\t Valid: 71.23%\tTest: 61.88%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3123\t Train: 72.73%\t Valid: 71.52%\tTest: 62.40%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3119\t Train: 73.18%\t Valid: 72.02%\tTest: 63.11%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3110\t Train: 73.31%\t Valid: 72.18%\tTest: 63.32%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3101\t Train: 73.40%\t Valid: 72.27%\tTest: 63.48%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3088\t Train: 73.42%\t Valid: 72.28%\tTest: 63.53%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3079\t Train: 73.81%\t Valid: 72.70%\tTest: 64.01%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3072\t Train: 73.82%\t Valid: 72.70%\tTest: 64.08%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3068\t Train: 73.58%\t Valid: 72.32%\tTest: 63.70%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3075\t Train: 74.98%\t Valid: 73.78%\tTest: 65.63%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3058\t Train: 75.02%\t Valid: 73.81%\tTest: 65.59%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3049\t Train: 75.18%\t Valid: 73.90%\tTest: 65.68%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3028\t Train: 75.02%\t Valid: 73.67%\tTest: 65.46%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3023\t Train: 75.70%\t Valid: 74.22%\tTest: 66.17%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3008\t Train: 75.90%\t Valid: 74.41%\tTest: 66.29%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3034\t Train: 76.65%\t Valid: 75.00%\tTest: 66.85%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2998\t Train: 76.39%\t Valid: 74.79%\tTest: 66.70%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2983\t Train: 76.51%\t Valid: 74.76%\tTest: 66.69%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2973\t Train: 76.55%\t Valid: 74.80%\tTest: 66.80%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2973\t Train: 76.86%\t Valid: 75.01%\tTest: 67.04%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2960\t Train: 77.36%\t Valid: 75.36%\tTest: 67.26%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2952\t Train: 77.64%\t Valid: 75.54%\tTest: 67.46%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2945\t Train: 77.63%\t Valid: 75.48%\tTest: 67.28%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2945\t Train: 77.61%\t Valid: 75.55%\tTest: 67.76%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2933\t Train: 77.64%\t Valid: 75.57%\tTest: 67.43%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2931\t Train: 78.31%\t Valid: 75.94%\tTest: 67.52%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2926\t Train: 78.02%\t Valid: 75.75%\tTest: 67.57%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2916\t Train: 78.57%\t Valid: 76.16%\tTest: 68.03%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2903\t Train: 78.65%\t Valid: 76.14%\tTest: 68.16%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2902\t Train: 78.84%\t Valid: 76.37%\tTest: 68.48%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2906\t Train: 79.31%\t Valid: 76.61%\tTest: 68.47%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2912\t Train: 78.67%\t Valid: 76.26%\tTest: 68.58%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2906\t Train: 78.87%\t Valid: 76.49%\tTest: 68.94%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2880\t Train: 79.63%\t Valid: 76.82%\tTest: 69.11%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2873\t Train: 79.68%\t Valid: 76.95%\tTest: 69.35%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2872\t Train: 79.76%\t Valid: 76.95%\tTest: 68.97%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2879\t Train: 80.01%\t Valid: 77.13%\tTest: 69.25%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2879\t Train: 79.45%\t Valid: 76.85%\tTest: 69.31%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2858\t Train: 80.03%\t Valid: 77.28%\tTest: 69.48%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2853\t Train: 80.02%\t Valid: 77.23%\tTest: 69.51%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2874\t Train: 79.76%\t Valid: 77.13%\tTest: 69.74%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2843\t Train: 80.30%\t Valid: 77.37%\tTest: 69.70%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2838\t Train: 80.52%\t Valid: 77.59%\tTest: 69.94%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2836\t Train: 80.24%\t Valid: 77.47%\tTest: 69.99%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2841\t Train: 80.50%\t Valid: 77.62%\tTest: 70.26%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2827\t Train: 80.73%\t Valid: 77.73%\tTest: 70.09%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2830\t Train: 80.76%\t Valid: 77.80%\tTest: 70.31%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2815\t Train: 80.79%\t Valid: 77.88%\tTest: 70.41%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2824\t Train: 81.00%\t Valid: 77.98%\tTest: 70.45%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2834\t Train: 81.26%\t Valid: 78.10%\tTest: 70.57%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2818\t Train: 81.26%\t Valid: 78.12%\tTest: 70.65%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2808\t Train: 81.36%\t Valid: 78.19%\tTest: 70.75%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2798\t Train: 81.26%\t Valid: 78.18%\tTest: 70.82%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2808\t Train: 81.02%\t Valid: 78.04%\tTest: 71.13%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2800\t Train: 81.46%\t Valid: 78.37%\tTest: 71.16%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2788\t Train: 81.18%\t Valid: 78.17%\tTest: 71.17%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2792\t Train: 81.64%\t Valid: 78.49%\tTest: 71.28%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2777\t Train: 81.71%\t Valid: 78.48%\tTest: 71.34%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2780\t Train: 81.69%\t Valid: 78.56%\tTest: 71.40%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2817\t Train: 81.07%\t Valid: 78.07%\tTest: 71.76%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2786\t Train: 81.93%\t Valid: 78.67%\tTest: 71.39%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2801\t Train: 81.62%\t Valid: 78.54%\tTest: 71.75%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2797\t Train: 81.98%\t Valid: 78.78%\tTest: 72.00%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2780\t Train: 82.10%\t Valid: 78.81%\tTest: 71.58%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2759\t Train: 82.01%\t Valid: 78.76%\tTest: 71.87%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2762\t Train: 82.11%\t Valid: 78.78%\tTest: 71.63%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2751\t Train: 82.23%\t Valid: 78.88%\tTest: 71.89%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2748\t Train: 82.34%\t Valid: 78.91%\tTest: 71.75%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2796\t Train: 82.11%\t Valid: 78.83%\tTest: 72.17%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2751\t Train: 82.38%\t Valid: 79.00%\tTest: 72.12%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2742\t Train: 82.47%\t Valid: 79.03%\tTest: 72.16%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2740\t Train: 82.49%\t Valid: 79.06%\tTest: 72.05%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2735\t Train: 82.58%\t Valid: 79.11%\tTest: 72.42%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2732\t Train: 82.61%\t Valid: 79.13%\tTest: 72.02%\n",
      "Seed =  7\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4245\t Train: 36.88%\t Valid: 34.87%\tTest: 37.55%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4170\t Train: 37.83%\t Valid: 33.65%\tTest: 35.92%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3981\t Train: 41.32%\t Valid: 34.02%\tTest: 35.29%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3872\t Train: 45.67%\t Valid: 35.11%\tTest: 34.98%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3785\t Train: 50.11%\t Valid: 37.86%\tTest: 36.28%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3716\t Train: 52.92%\t Valid: 40.54%\tTest: 37.34%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3648\t Train: 55.28%\t Valid: 44.71%\tTest: 39.18%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3592\t Train: 56.27%\t Valid: 47.23%\tTest: 40.11%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3548\t Train: 57.47%\t Valid: 49.52%\tTest: 41.26%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3512\t Train: 58.71%\t Valid: 51.50%\tTest: 42.61%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3476\t Train: 60.22%\t Valid: 53.37%\tTest: 43.86%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3433\t Train: 62.35%\t Valid: 55.86%\tTest: 46.13%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3393\t Train: 64.64%\t Valid: 58.64%\tTest: 48.57%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3360\t Train: 66.42%\t Valid: 61.14%\tTest: 50.60%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3328\t Train: 66.78%\t Valid: 62.26%\tTest: 52.34%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3298\t Train: 67.50%\t Valid: 63.87%\tTest: 54.22%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3271\t Train: 68.45%\t Valid: 65.64%\tTest: 55.78%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3256\t Train: 68.64%\t Valid: 66.13%\tTest: 56.32%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3229\t Train: 69.88%\t Valid: 68.26%\tTest: 58.15%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3218\t Train: 70.50%\t Valid: 69.29%\tTest: 59.22%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3207\t Train: 70.01%\t Valid: 68.35%\tTest: 58.64%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3181\t Train: 70.99%\t Valid: 69.79%\tTest: 60.00%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3168\t Train: 71.71%\t Valid: 70.79%\tTest: 61.05%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3168\t Train: 71.25%\t Valid: 69.99%\tTest: 60.49%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3146\t Train: 72.15%\t Valid: 71.25%\tTest: 61.67%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3141\t Train: 72.70%\t Valid: 71.90%\tTest: 62.34%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3124\t Train: 72.62%\t Valid: 71.77%\tTest: 62.22%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3118\t Train: 72.99%\t Valid: 72.21%\tTest: 62.70%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3106\t Train: 73.41%\t Valid: 72.63%\tTest: 63.35%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3103\t Train: 73.38%\t Valid: 72.63%\tTest: 63.20%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3092\t Train: 73.46%\t Valid: 72.55%\tTest: 63.31%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3080\t Train: 73.88%\t Valid: 73.05%\tTest: 63.90%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3070\t Train: 73.95%\t Valid: 73.05%\tTest: 64.11%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3065\t Train: 73.99%\t Valid: 73.09%\tTest: 64.03%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3053\t Train: 74.42%\t Valid: 73.46%\tTest: 64.65%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3048\t Train: 74.40%\t Valid: 73.31%\tTest: 64.57%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3045\t Train: 74.59%\t Valid: 73.51%\tTest: 64.74%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3030\t Train: 75.12%\t Valid: 73.96%\tTest: 65.36%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3022\t Train: 75.35%\t Valid: 74.13%\tTest: 65.64%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3035\t Train: 76.18%\t Valid: 74.89%\tTest: 66.20%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3007\t Train: 76.00%\t Valid: 74.65%\tTest: 66.19%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2997\t Train: 76.07%\t Valid: 74.61%\tTest: 66.16%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3009\t Train: 76.39%\t Valid: 74.71%\tTest: 66.09%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2986\t Train: 76.75%\t Valid: 75.13%\tTest: 66.88%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2973\t Train: 76.77%\t Valid: 75.00%\tTest: 66.80%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2966\t Train: 76.70%\t Valid: 74.87%\tTest: 66.77%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2979\t Train: 77.42%\t Valid: 75.45%\tTest: 67.20%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2953\t Train: 76.81%\t Valid: 74.84%\tTest: 66.64%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2954\t Train: 77.74%\t Valid: 75.65%\tTest: 67.51%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2939\t Train: 77.72%\t Valid: 75.45%\tTest: 67.26%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2934\t Train: 78.11%\t Valid: 75.83%\tTest: 67.82%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2926\t Train: 78.02%\t Valid: 75.62%\tTest: 67.61%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2924\t Train: 78.91%\t Valid: 76.27%\tTest: 67.97%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2928\t Train: 78.72%\t Valid: 76.32%\tTest: 68.32%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2913\t Train: 78.85%\t Valid: 76.36%\tTest: 68.42%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2913\t Train: 78.97%\t Valid: 76.33%\tTest: 68.29%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2902\t Train: 78.91%\t Valid: 76.25%\tTest: 68.26%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2893\t Train: 78.96%\t Valid: 76.33%\tTest: 68.39%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2884\t Train: 79.18%\t Valid: 76.46%\tTest: 68.52%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2891\t Train: 78.88%\t Valid: 76.41%\tTest: 68.89%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2891\t Train: 79.51%\t Valid: 76.59%\tTest: 68.13%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2898\t Train: 79.61%\t Valid: 76.96%\tTest: 68.98%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2876\t Train: 79.61%\t Valid: 76.96%\tTest: 68.87%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2864\t Train: 79.73%\t Valid: 76.88%\tTest: 68.86%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2857\t Train: 79.90%\t Valid: 77.05%\tTest: 69.35%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2854\t Train: 80.09%\t Valid: 77.27%\tTest: 69.30%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2850\t Train: 80.24%\t Valid: 77.40%\tTest: 69.51%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2841\t Train: 80.25%\t Valid: 77.34%\tTest: 69.66%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2857\t Train: 80.63%\t Valid: 77.59%\tTest: 69.66%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2835\t Train: 80.53%\t Valid: 77.56%\tTest: 69.51%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2847\t Train: 80.76%\t Valid: 77.83%\tTest: 70.14%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2833\t Train: 80.75%\t Valid: 77.74%\tTest: 69.96%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2847\t Train: 80.57%\t Valid: 77.77%\tTest: 69.98%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2819\t Train: 80.79%\t Valid: 77.79%\tTest: 70.34%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2818\t Train: 80.97%\t Valid: 77.91%\tTest: 70.15%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2806\t Train: 81.02%\t Valid: 78.03%\tTest: 70.40%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2807\t Train: 81.15%\t Valid: 78.12%\tTest: 70.67%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2818\t Train: 81.22%\t Valid: 78.24%\tTest: 70.51%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2822\t Train: 81.37%\t Valid: 78.22%\tTest: 70.61%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2803\t Train: 81.42%\t Valid: 78.41%\tTest: 70.84%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2797\t Train: 81.45%\t Valid: 78.28%\tTest: 70.73%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2787\t Train: 81.31%\t Valid: 78.37%\tTest: 71.07%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2803\t Train: 81.75%\t Valid: 78.57%\tTest: 71.04%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2778\t Train: 81.78%\t Valid: 78.61%\tTest: 71.16%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2776\t Train: 81.66%\t Valid: 78.66%\tTest: 71.51%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2770\t Train: 81.80%\t Valid: 78.66%\tTest: 71.44%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2771\t Train: 81.54%\t Valid: 78.54%\tTest: 71.62%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2788\t Train: 81.79%\t Valid: 78.84%\tTest: 71.86%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2760\t Train: 82.17%\t Valid: 78.91%\tTest: 71.44%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2804\t Train: 82.21%\t Valid: 78.90%\tTest: 71.53%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2755\t Train: 82.10%\t Valid: 78.89%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2754\t Train: 82.18%\t Valid: 78.88%\tTest: 71.64%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2746\t Train: 82.29%\t Valid: 78.98%\tTest: 71.74%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2859\t Train: 82.40%\t Valid: 79.04%\tTest: 71.39%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2767\t Train: 82.17%\t Valid: 79.04%\tTest: 72.32%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2741\t Train: 82.49%\t Valid: 79.05%\tTest: 71.70%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2740\t Train: 82.52%\t Valid: 79.10%\tTest: 71.80%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2738\t Train: 82.65%\t Valid: 79.26%\tTest: 72.06%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2731\t Train: 82.70%\t Valid: 79.21%\tTest: 71.94%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2766\t Train: 82.65%\t Valid: 79.44%\tTest: 72.19%\n",
      "Seed =  8\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4231\t Train: 37.11%\t Valid: 35.11%\tTest: 37.73%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4172\t Train: 37.89%\t Valid: 33.58%\tTest: 35.80%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3987\t Train: 41.50%\t Valid: 33.94%\tTest: 35.10%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3867\t Train: 46.28%\t Valid: 35.58%\tTest: 35.30%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3780\t Train: 50.45%\t Valid: 38.61%\tTest: 36.83%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3715\t Train: 52.81%\t Valid: 41.94%\tTest: 38.22%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3647\t Train: 54.89%\t Valid: 45.68%\tTest: 39.61%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3584\t Train: 56.69%\t Valid: 48.55%\tTest: 40.90%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3534\t Train: 58.10%\t Valid: 50.75%\tTest: 42.37%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3486\t Train: 59.50%\t Valid: 52.52%\tTest: 43.92%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3443\t Train: 61.79%\t Valid: 55.27%\tTest: 46.42%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3408\t Train: 64.61%\t Valid: 58.85%\tTest: 49.12%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3372\t Train: 65.39%\t Valid: 59.98%\tTest: 50.55%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3341\t Train: 67.07%\t Valid: 62.69%\tTest: 52.71%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3314\t Train: 67.76%\t Valid: 64.03%\tTest: 54.26%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3287\t Train: 68.16%\t Valid: 64.94%\tTest: 55.37%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3264\t Train: 68.47%\t Valid: 65.67%\tTest: 56.25%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3261\t Train: 68.50%\t Valid: 65.93%\tTest: 56.53%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3225\t Train: 69.46%\t Valid: 67.60%\tTest: 58.19%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3206\t Train: 69.75%\t Valid: 68.02%\tTest: 58.40%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3200\t Train: 71.04%\t Valid: 69.92%\tTest: 60.25%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3176\t Train: 71.25%\t Valid: 70.17%\tTest: 60.44%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3180\t Train: 70.84%\t Valid: 69.42%\tTest: 59.75%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3154\t Train: 71.59%\t Valid: 70.44%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3142\t Train: 71.72%\t Valid: 70.58%\tTest: 61.08%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3131\t Train: 72.45%\t Valid: 71.54%\tTest: 62.17%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3150\t Train: 71.30%\t Valid: 69.79%\tTest: 60.57%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3124\t Train: 72.61%\t Valid: 71.61%\tTest: 62.35%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3111\t Train: 73.47%\t Valid: 72.71%\tTest: 63.52%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3095\t Train: 73.24%\t Valid: 72.36%\tTest: 63.35%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3082\t Train: 73.78%\t Valid: 72.97%\tTest: 64.04%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3077\t Train: 74.26%\t Valid: 73.48%\tTest: 64.76%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3063\t Train: 74.18%\t Valid: 73.25%\tTest: 64.64%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3055\t Train: 74.75%\t Valid: 73.84%\tTest: 65.19%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3052\t Train: 74.18%\t Valid: 73.12%\tTest: 64.52%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3043\t Train: 74.91%\t Valid: 73.84%\tTest: 65.21%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3027\t Train: 75.53%\t Valid: 74.42%\tTest: 66.04%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3020\t Train: 75.75%\t Valid: 74.56%\tTest: 66.22%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3009\t Train: 75.80%\t Valid: 74.50%\tTest: 66.08%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3054\t Train: 75.10%\t Valid: 73.66%\tTest: 65.32%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3001\t Train: 76.77%\t Valid: 75.30%\tTest: 66.76%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2989\t Train: 76.20%\t Valid: 74.59%\tTest: 66.34%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2976\t Train: 76.91%\t Valid: 75.25%\tTest: 67.05%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2980\t Train: 76.74%\t Valid: 75.10%\tTest: 67.00%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2967\t Train: 77.42%\t Valid: 75.62%\tTest: 67.61%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2957\t Train: 77.41%\t Valid: 75.43%\tTest: 67.26%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2950\t Train: 77.66%\t Valid: 75.66%\tTest: 67.70%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2990\t Train: 77.64%\t Valid: 75.60%\tTest: 67.01%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2948\t Train: 77.36%\t Valid: 75.28%\tTest: 67.01%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2937\t Train: 77.98%\t Valid: 75.82%\tTest: 67.89%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2927\t Train: 78.25%\t Valid: 76.07%\tTest: 68.47%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2919\t Train: 78.59%\t Valid: 76.31%\tTest: 68.69%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2916\t Train: 78.75%\t Valid: 76.35%\tTest: 68.58%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2910\t Train: 78.89%\t Valid: 76.47%\tTest: 68.74%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2915\t Train: 78.43%\t Valid: 76.27%\tTest: 69.02%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2897\t Train: 78.76%\t Valid: 76.46%\tTest: 68.95%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2898\t Train: 79.10%\t Valid: 76.68%\tTest: 69.08%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2894\t Train: 79.39%\t Valid: 76.81%\tTest: 69.00%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2892\t Train: 79.55%\t Valid: 76.92%\tTest: 69.39%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2880\t Train: 79.71%\t Valid: 77.02%\tTest: 69.60%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2873\t Train: 79.48%\t Valid: 77.00%\tTest: 69.73%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2865\t Train: 79.77%\t Valid: 77.10%\tTest: 69.64%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2865\t Train: 79.67%\t Valid: 77.12%\tTest: 69.97%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2857\t Train: 80.09%\t Valid: 77.31%\tTest: 69.97%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2854\t Train: 79.69%\t Valid: 77.11%\tTest: 70.17%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2864\t Train: 80.05%\t Valid: 77.42%\tTest: 70.27%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2842\t Train: 80.44%\t Valid: 77.65%\tTest: 70.21%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2840\t Train: 80.18%\t Valid: 77.40%\tTest: 70.37%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2842\t Train: 80.57%\t Valid: 77.74%\tTest: 70.50%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2829\t Train: 80.65%\t Valid: 77.76%\tTest: 70.59%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2824\t Train: 80.50%\t Valid: 77.78%\tTest: 70.83%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2868\t Train: 80.88%\t Valid: 77.99%\tTest: 70.63%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2823\t Train: 80.84%\t Valid: 77.97%\tTest: 70.78%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2818\t Train: 81.06%\t Valid: 77.97%\tTest: 70.63%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2816\t Train: 81.14%\t Valid: 77.99%\tTest: 70.79%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2801\t Train: 81.19%\t Valid: 78.16%\tTest: 71.07%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2798\t Train: 81.17%\t Valid: 78.12%\tTest: 71.20%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2814\t Train: 81.43%\t Valid: 78.25%\tTest: 70.81%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2794\t Train: 81.48%\t Valid: 78.26%\tTest: 71.00%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2786\t Train: 81.49%\t Valid: 78.35%\tTest: 71.35%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2848\t Train: 80.94%\t Valid: 78.17%\tTest: 71.50%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2788\t Train: 81.75%\t Valid: 78.46%\tTest: 71.07%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2791\t Train: 81.24%\t Valid: 78.15%\tTest: 71.40%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2782\t Train: 81.60%\t Valid: 78.60%\tTest: 71.73%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2775\t Train: 81.72%\t Valid: 78.52%\tTest: 71.62%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2763\t Train: 81.91%\t Valid: 78.62%\tTest: 71.50%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2787\t Train: 81.50%\t Valid: 78.59%\tTest: 71.99%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2781\t Train: 81.93%\t Valid: 78.60%\tTest: 71.51%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2761\t Train: 81.80%\t Valid: 78.58%\tTest: 71.56%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2756\t Train: 82.06%\t Valid: 78.66%\tTest: 71.66%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2746\t Train: 82.22%\t Valid: 78.74%\tTest: 71.69%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2755\t Train: 82.08%\t Valid: 78.82%\tTest: 71.98%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2769\t Train: 82.44%\t Valid: 78.86%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2744\t Train: 82.34%\t Valid: 79.01%\tTest: 72.11%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2737\t Train: 82.35%\t Valid: 78.96%\tTest: 72.01%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2736\t Train: 82.52%\t Valid: 78.91%\tTest: 71.86%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2775\t Train: 82.35%\t Valid: 78.93%\tTest: 71.94%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2731\t Train: 82.67%\t Valid: 79.06%\tTest: 72.05%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2730\t Train: 82.75%\t Valid: 79.06%\tTest: 71.90%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2734\t Train: 82.77%\t Valid: 78.98%\tTest: 71.71%\n",
      "Seed =  9\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4245\t Train: 37.06%\t Valid: 35.32%\tTest: 38.01%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4167\t Train: 37.89%\t Valid: 34.00%\tTest: 36.28%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3993\t Train: 41.01%\t Valid: 33.95%\tTest: 35.22%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3889\t Train: 45.05%\t Valid: 34.96%\tTest: 35.31%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3802\t Train: 49.03%\t Valid: 36.90%\tTest: 35.78%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3735\t Train: 52.34%\t Valid: 39.77%\tTest: 36.94%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3672\t Train: 54.50%\t Valid: 42.93%\tTest: 38.25%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3614\t Train: 55.41%\t Valid: 45.38%\tTest: 38.89%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3566\t Train: 56.78%\t Valid: 47.97%\tTest: 40.11%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3527\t Train: 58.08%\t Valid: 50.53%\tTest: 41.70%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3495\t Train: 59.05%\t Valid: 52.29%\tTest: 43.23%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3468\t Train: 60.35%\t Valid: 54.08%\tTest: 44.73%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3441\t Train: 61.41%\t Valid: 55.60%\tTest: 46.21%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3409\t Train: 62.56%\t Valid: 57.18%\tTest: 47.58%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3375\t Train: 64.14%\t Valid: 59.24%\tTest: 49.60%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3343\t Train: 65.52%\t Valid: 61.26%\tTest: 51.58%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3316\t Train: 67.72%\t Valid: 64.84%\tTest: 54.61%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3286\t Train: 68.32%\t Valid: 65.84%\tTest: 55.51%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3266\t Train: 68.96%\t Valid: 66.82%\tTest: 56.36%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3243\t Train: 69.65%\t Valid: 67.86%\tTest: 57.42%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3231\t Train: 69.64%\t Valid: 67.86%\tTest: 57.87%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3201\t Train: 70.35%\t Valid: 69.02%\tTest: 59.18%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3196\t Train: 71.39%\t Valid: 70.47%\tTest: 60.64%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3173\t Train: 71.49%\t Valid: 70.55%\tTest: 61.00%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3161\t Train: 71.30%\t Valid: 70.18%\tTest: 60.62%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3148\t Train: 71.52%\t Valid: 70.39%\tTest: 60.76%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3138\t Train: 71.54%\t Valid: 70.33%\tTest: 60.93%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3127\t Train: 72.73%\t Valid: 71.83%\tTest: 62.37%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3116\t Train: 72.61%\t Valid: 71.58%\tTest: 62.02%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3106\t Train: 73.20%\t Valid: 72.24%\tTest: 62.74%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3106\t Train: 73.06%\t Valid: 71.97%\tTest: 62.40%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3087\t Train: 73.92%\t Valid: 72.92%\tTest: 63.58%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3081\t Train: 73.28%\t Valid: 72.05%\tTest: 62.55%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3070\t Train: 74.03%\t Valid: 72.88%\tTest: 63.39%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3056\t Train: 74.50%\t Valid: 73.35%\tTest: 63.83%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3046\t Train: 74.70%\t Valid: 73.46%\tTest: 63.88%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3044\t Train: 74.69%\t Valid: 73.37%\tTest: 63.64%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3036\t Train: 75.28%\t Valid: 73.93%\tTest: 64.44%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3017\t Train: 75.85%\t Valid: 74.36%\tTest: 65.04%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3012\t Train: 76.27%\t Valid: 74.66%\tTest: 65.35%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3002\t Train: 76.44%\t Valid: 74.75%\tTest: 65.32%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2996\t Train: 76.19%\t Valid: 74.49%\tTest: 65.07%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2984\t Train: 76.28%\t Valid: 74.38%\tTest: 64.92%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2976\t Train: 76.99%\t Valid: 74.91%\tTest: 65.58%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2971\t Train: 76.80%\t Valid: 74.74%\tTest: 65.67%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2975\t Train: 77.31%\t Valid: 75.11%\tTest: 65.79%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2967\t Train: 77.19%\t Valid: 74.99%\tTest: 65.64%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2949\t Train: 77.78%\t Valid: 75.32%\tTest: 65.87%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2942\t Train: 78.00%\t Valid: 75.49%\tTest: 66.17%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2939\t Train: 78.08%\t Valid: 75.68%\tTest: 66.62%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2931\t Train: 77.95%\t Valid: 75.50%\tTest: 66.53%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2921\t Train: 78.29%\t Valid: 75.71%\tTest: 66.54%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2952\t Train: 78.75%\t Valid: 75.96%\tTest: 66.70%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2914\t Train: 78.41%\t Valid: 75.76%\tTest: 66.54%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2906\t Train: 78.75%\t Valid: 75.96%\tTest: 66.99%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2902\t Train: 78.80%\t Valid: 76.10%\tTest: 67.38%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2899\t Train: 79.11%\t Valid: 76.22%\tTest: 67.16%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2893\t Train: 79.32%\t Valid: 76.48%\tTest: 67.62%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2903\t Train: 78.66%\t Valid: 76.23%\tTest: 67.91%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2909\t Train: 79.39%\t Valid: 76.65%\tTest: 68.03%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2876\t Train: 79.49%\t Valid: 76.58%\tTest: 68.07%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2868\t Train: 79.78%\t Valid: 76.78%\tTest: 68.20%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2870\t Train: 79.53%\t Valid: 76.70%\tTest: 68.17%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2861\t Train: 79.77%\t Valid: 76.87%\tTest: 68.53%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2854\t Train: 80.08%\t Valid: 77.06%\tTest: 68.62%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2851\t Train: 80.18%\t Valid: 76.96%\tTest: 68.29%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2847\t Train: 80.03%\t Valid: 77.06%\tTest: 68.82%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2881\t Train: 80.45%\t Valid: 77.32%\tTest: 69.17%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2848\t Train: 80.41%\t Valid: 77.43%\tTest: 69.12%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2834\t Train: 80.56%\t Valid: 77.37%\tTest: 69.08%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2832\t Train: 80.66%\t Valid: 77.48%\tTest: 69.25%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2824\t Train: 80.62%\t Valid: 77.49%\tTest: 69.38%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2831\t Train: 81.02%\t Valid: 77.60%\tTest: 69.17%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2888\t Train: 80.77%\t Valid: 77.62%\tTest: 69.41%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2842\t Train: 80.90%\t Valid: 77.88%\tTest: 69.82%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2820\t Train: 80.96%\t Valid: 77.86%\tTest: 69.88%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2808\t Train: 81.04%\t Valid: 77.76%\tTest: 69.60%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2801\t Train: 81.19%\t Valid: 77.97%\tTest: 69.97%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2797\t Train: 81.28%\t Valid: 78.04%\tTest: 70.02%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2798\t Train: 81.15%\t Valid: 78.08%\tTest: 70.39%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2791\t Train: 81.30%\t Valid: 78.15%\tTest: 70.42%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2789\t Train: 81.42%\t Valid: 78.20%\tTest: 70.57%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2786\t Train: 81.55%\t Valid: 78.36%\tTest: 70.74%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2778\t Train: 81.69%\t Valid: 78.34%\tTest: 70.53%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2779\t Train: 81.72%\t Valid: 78.41%\tTest: 70.67%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2779\t Train: 81.54%\t Valid: 78.40%\tTest: 70.99%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2773\t Train: 81.68%\t Valid: 78.51%\tTest: 70.95%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2776\t Train: 81.84%\t Valid: 78.61%\tTest: 71.37%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2762\t Train: 82.01%\t Valid: 78.61%\tTest: 70.86%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2816\t Train: 82.29%\t Valid: 78.63%\tTest: 70.84%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2758\t Train: 81.94%\t Valid: 78.63%\tTest: 71.16%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2761\t Train: 82.05%\t Valid: 78.71%\tTest: 71.33%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2749\t Train: 82.31%\t Valid: 78.81%\tTest: 71.17%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2749\t Train: 82.39%\t Valid: 78.81%\tTest: 71.29%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2757\t Train: 82.35%\t Valid: 78.88%\tTest: 71.39%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2753\t Train: 82.23%\t Valid: 78.90%\tTest: 71.67%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2749\t Train: 82.57%\t Valid: 78.95%\tTest: 71.39%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2746\t Train: 82.67%\t Valid: 78.87%\tTest: 71.07%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2738\t Train: 82.67%\t Valid: 78.98%\tTest: 71.43%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2732\t Train: 82.71%\t Valid: 79.09%\tTest: 71.64%\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute GCN normalization.\n",
    "adj_t = data.adj_t.set_diag()\n",
    "deg = adj_t.sum(dim=1).to(torch.float)\n",
    "deg_inv_sqrt = deg.pow(-0.5)\n",
    "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
    "data.adj_t = adj_t\n",
    "    \n",
    "evaluator = Evaluator(name='ogbn-proteins')\n",
    "logger = Logger(args.runs, args)\n",
    "best_test_score = 0\n",
    "\n",
    "\n",
    "for seed in range(10):\n",
    "    print(\"Seed = \",seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    model = GCN(data.num_features, args.hidden_channels, 112, args.num_layers, args.dropout).to(device)\n",
    "    model.reset_parameters()\n",
    "    for run in range(args.runs):        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "\n",
    "            if epoch % args.eval_steps == 0:\n",
    "                result = test(model, data, split_idx, evaluator)\n",
    "                logger.add_result(run, result)\n",
    "\n",
    "                if epoch % args.log_steps == 0:                \n",
    "                    train_rocauc, valid_rocauc, test_rocauc = result\n",
    "                    print(f'Run: {run + 1:02d}\\t '\n",
    "                          f'Epoch: {epoch:02d}\\t '\n",
    "                          f'Loss: {loss:.4f}\\t '\n",
    "                          f'Train: {100 * train_rocauc:.2f}%\\t '\n",
    "                          f'Valid: {100 * valid_rocauc:.2f}%\\t'\n",
    "                          f'Test: {100 * test_rocauc:.2f}%')\n",
    "                    if(test_rocauc > best_test_score):\n",
    "                        best_test_score = test_rocauc\n",
    "                        save_path = \"gcn.pth\"\n",
    "                        torch.save(model, save_path)\n",
    "                        print(\"Model saved.\")\n",
    "        # logger.print_statistics(run)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.94542262983494\n"
     ]
    }
   ],
   "source": [
    "print(best_test_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 hours\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, optimizer\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
