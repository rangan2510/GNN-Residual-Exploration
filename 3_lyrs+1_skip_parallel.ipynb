{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    num_layers = 3\n",
    "    device = 'cuda:0'\n",
    "    log_steps = 1\n",
    "    hidden_channels = 64*4\n",
    "    dropout = 0.2\n",
    "    lr = 0.01\n",
    "    epochs = 1000\n",
    "    eval_steps = 10\n",
    "    runs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import humanize\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\r\n",
      "  Downloading ogb-1.2.1-py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 984 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.1)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.0.3)\r\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.18.5)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Installing collected packages: ogb\r\n",
      "Successfully installed ogb-1.2.1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-scatter==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 4.0 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\r\n",
      "Successfully installed torch-scatter-2.0.5\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-sparse==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.6 MB 3.7 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==latest+cu101) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==latest+cu101) (1.18.5)\r\n",
      "Installing collected packages: torch-sparse\r\n",
      "Successfully installed torch-sparse-0.6.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-cluster==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.3 MB 4.6 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-cluster\r\n",
      "Successfully installed torch-cluster-1.5.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-spline-conv==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 4.0 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\r\n",
      "Successfully installed torch-spline-conv-1.2.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-1.6.0.tar.gz (172 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 172 kB 2.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.5.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.18.5)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (4.45.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.4.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.23.1)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.48.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.23.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.0.3)\r\n",
      "Collecting rdflib\r\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 231 kB 8.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.10.0)\r\n",
      "Collecting googledrivedownloader\r\n",
      "  Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB)\r\n",
      "Collecting ase\r\n",
      "  Downloading ase-3.19.2-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 11.6 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.11.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torch-geometric) (0.18.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->torch-geometric) (4.4.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (2.1.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (0.31.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (1.24.3)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2.8.1)\r\n",
      "Collecting isodate\r\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 3.5 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (1.14.0)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (2.4.7)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ase->torch-geometric) (3.2.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric) (1.1.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (0.10.0)\r\n",
      "Building wheels for collected packages: torch-geometric\r\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-1.6.0-py3-none-any.whl size=296336 sha256=21845d299da81f8c95d07f26f2184370b4e562dc0c535adfbec3adf3475d156c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/69/d6/b8ed45222466048e1efc27af604aded6825217f0caa6dff569\r\n",
      "Successfully built torch-geometric\r\n",
      "Installing collected packages: isodate, rdflib, googledrivedownloader, ase, torch-geometric\r\n",
      "Successfully installed ase-3.19.2 googledrivedownloader-0.4 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# install Open Graph Benchmark\n",
    "!pip install ogb\n",
    "\n",
    "# install PyTorch Geometric\n",
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR,CosineAnnealingLR\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://snap.stanford.edu/ogb/data/nodeproppred/proteinfunc.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:07<00:00, 29.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/proteinfunc.zip\n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 427.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n",
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = f'{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-proteins',\n",
    "                                 transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "\n",
    "# Move edge features to node features.\n",
    "data.x = data.adj_t.mean(dim=1)\n",
    "data.adj_t.set_value_(None)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train'].to(device)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 minutes\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            GCNConv(in_channels, hidden_channels, normalize=False))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, normalize=False))\n",
    "        self.convs.append(\n",
    "            GCNConv(hidden_channels*2, out_channels, normalize=False))\n",
    "        self.parallel =  GCNConv(hidden_channels, hidden_channels, normalize=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        x = self.convs[0](x, adj_t)\n",
    "        x_ = x\n",
    "        for conv in self.convs[1:-1]:\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x_ = self.parallel(x_, adj_t)\n",
    "        x = torch.cat([x_,x],1)    \n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = criterion(out, data.y[train_idx].to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(data.x, data.adj_t)\n",
    "\n",
    "    train_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['rocauc']\n",
    "    valid_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['rocauc']\n",
    "    test_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['rocauc']\n",
    "\n",
    "    return train_rocauc, valid_rocauc, test_rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed =  0\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4395\t Train: 37.18%\t Valid: 33.23%\tTest: 35.77%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4006\t Train: 42.51%\t Valid: 33.81%\tTest: 34.66%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3853\t Train: 49.07%\t Valid: 37.95%\tTest: 36.61%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3702\t Train: 54.16%\t Valid: 45.22%\tTest: 41.37%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3622\t Train: 56.93%\t Valid: 47.76%\tTest: 42.41%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3530\t Train: 60.45%\t Valid: 51.85%\tTest: 44.80%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3437\t Train: 63.68%\t Valid: 55.57%\tTest: 46.48%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3390\t Train: 62.54%\t Valid: 54.88%\tTest: 44.88%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3333\t Train: 67.27%\t Valid: 62.44%\tTest: 50.15%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3293\t Train: 69.11%\t Valid: 65.72%\tTest: 53.08%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3341\t Train: 64.94%\t Valid: 58.87%\tTest: 46.82%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3256\t Train: 68.82%\t Valid: 65.05%\tTest: 53.24%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3234\t Train: 70.27%\t Valid: 67.26%\tTest: 54.71%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3211\t Train: 70.27%\t Valid: 67.39%\tTest: 53.94%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3192\t Train: 71.36%\t Valid: 68.97%\tTest: 55.47%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3174\t Train: 71.71%\t Valid: 69.30%\tTest: 55.38%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3180\t Train: 71.95%\t Valid: 69.45%\tTest: 55.80%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3147\t Train: 72.76%\t Valid: 70.49%\tTest: 57.13%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3129\t Train: 72.18%\t Valid: 69.86%\tTest: 56.38%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3112\t Train: 73.31%\t Valid: 71.23%\tTest: 57.58%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3097\t Train: 73.34%\t Valid: 71.28%\tTest: 57.41%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3141\t Train: 71.67%\t Valid: 69.06%\tTest: 55.67%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3083\t Train: 72.73%\t Valid: 70.12%\tTest: 57.53%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3064\t Train: 74.57%\t Valid: 72.81%\tTest: 60.24%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3040\t Train: 74.97%\t Valid: 73.05%\tTest: 59.99%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3026\t Train: 75.46%\t Valid: 73.47%\tTest: 60.20%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3016\t Train: 75.45%\t Valid: 73.09%\tTest: 60.03%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3000\t Train: 76.38%\t Valid: 74.17%\tTest: 61.30%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2985\t Train: 76.89%\t Valid: 74.52%\tTest: 61.48%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2986\t Train: 76.67%\t Valid: 73.70%\tTest: 59.91%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2972\t Train: 77.43%\t Valid: 74.60%\tTest: 61.00%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3008\t Train: 76.49%\t Valid: 73.29%\tTest: 59.84%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2963\t Train: 76.92%\t Valid: 73.73%\tTest: 58.72%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2933\t Train: 77.79%\t Valid: 74.71%\tTest: 60.41%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2920\t Train: 78.43%\t Valid: 75.29%\tTest: 61.38%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2906\t Train: 78.81%\t Valid: 75.37%\tTest: 61.09%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2895\t Train: 78.86%\t Valid: 75.53%\tTest: 61.14%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2950\t Train: 77.72%\t Valid: 74.77%\tTest: 60.05%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2908\t Train: 79.07%\t Valid: 75.52%\tTest: 60.06%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2880\t Train: 79.59%\t Valid: 75.41%\tTest: 59.00%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2866\t Train: 79.66%\t Valid: 76.22%\tTest: 62.58%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2855\t Train: 79.95%\t Valid: 75.85%\tTest: 60.28%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2849\t Train: 79.78%\t Valid: 76.04%\tTest: 60.42%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2843\t Train: 80.27%\t Valid: 76.20%\tTest: 60.47%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2840\t Train: 80.73%\t Valid: 76.05%\tTest: 58.93%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2839\t Train: 80.16%\t Valid: 75.56%\tTest: 59.18%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2822\t Train: 80.87%\t Valid: 76.44%\tTest: 60.59%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2847\t Train: 81.04%\t Valid: 76.85%\tTest: 62.02%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2811\t Train: 81.07%\t Valid: 76.73%\tTest: 60.91%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2789\t Train: 81.30%\t Valid: 77.01%\tTest: 61.07%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2781\t Train: 81.32%\t Valid: 76.94%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2929\t Train: 81.16%\t Valid: 76.54%\tTest: 58.96%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2826\t Train: 81.06%\t Valid: 75.98%\tTest: 57.53%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2782\t Train: 81.58%\t Valid: 77.01%\tTest: 60.72%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2763\t Train: 81.86%\t Valid: 77.07%\tTest: 60.41%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2758\t Train: 81.67%\t Valid: 77.07%\tTest: 60.05%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2753\t Train: 81.90%\t Valid: 76.41%\tTest: 59.44%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2756\t Train: 82.14%\t Valid: 77.53%\tTest: 61.21%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2738\t Train: 82.21%\t Valid: 77.07%\tTest: 60.04%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2733\t Train: 82.27%\t Valid: 76.80%\tTest: 59.41%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2730\t Train: 82.53%\t Valid: 77.06%\tTest: 59.84%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2723\t Train: 82.51%\t Valid: 77.27%\tTest: 60.85%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2717\t Train: 82.65%\t Valid: 77.67%\tTest: 61.85%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2714\t Train: 82.58%\t Valid: 77.19%\tTest: 60.23%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2713\t Train: 82.80%\t Valid: 77.32%\tTest: 60.20%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2699\t Train: 82.75%\t Valid: 76.99%\tTest: 59.66%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2755\t Train: 83.07%\t Valid: 77.80%\tTest: 60.84%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2701\t Train: 82.81%\t Valid: 76.97%\tTest: 59.79%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2698\t Train: 83.06%\t Valid: 77.65%\tTest: 61.20%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2686\t Train: 83.11%\t Valid: 77.63%\tTest: 61.19%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2689\t Train: 83.32%\t Valid: 77.70%\tTest: 61.21%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2678\t Train: 83.27%\t Valid: 77.44%\tTest: 60.42%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2682\t Train: 83.41%\t Valid: 77.64%\tTest: 60.96%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2727\t Train: 83.37%\t Valid: 77.89%\tTest: 60.97%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2678\t Train: 83.40%\t Valid: 77.09%\tTest: 59.48%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2674\t Train: 83.37%\t Valid: 78.20%\tTest: 61.65%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2667\t Train: 83.52%\t Valid: 77.73%\tTest: 60.54%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2663\t Train: 83.57%\t Valid: 78.11%\tTest: 61.69%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2657\t Train: 83.71%\t Valid: 77.89%\tTest: 61.10%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2658\t Train: 83.70%\t Valid: 78.12%\tTest: 61.39%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2734\t Train: 83.84%\t Valid: 78.05%\tTest: 61.16%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2719\t Train: 81.98%\t Valid: 77.42%\tTest: 59.88%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2713\t Train: 83.24%\t Valid: 77.24%\tTest: 60.35%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2667\t Train: 83.79%\t Valid: 77.74%\tTest: 61.39%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2650\t Train: 83.84%\t Valid: 78.10%\tTest: 61.84%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2649\t Train: 83.92%\t Valid: 78.16%\tTest: 61.68%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2643\t Train: 84.02%\t Valid: 78.16%\tTest: 61.78%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2638\t Train: 84.03%\t Valid: 77.84%\tTest: 60.68%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2635\t Train: 84.19%\t Valid: 78.00%\tTest: 60.98%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2637\t Train: 84.24%\t Valid: 78.35%\tTest: 62.19%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2627\t Train: 84.14%\t Valid: 78.02%\tTest: 61.14%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2628\t Train: 84.15%\t Valid: 77.90%\tTest: 60.66%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2629\t Train: 84.35%\t Valid: 78.18%\tTest: 61.14%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2622\t Train: 84.37%\t Valid: 78.17%\tTest: 60.97%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2622\t Train: 84.39%\t Valid: 78.28%\tTest: 60.98%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2631\t Train: 84.47%\t Valid: 78.64%\tTest: 62.41%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2615\t Train: 84.49%\t Valid: 78.04%\tTest: 60.90%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2620\t Train: 84.53%\t Valid: 78.38%\tTest: 61.40%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2618\t Train: 84.60%\t Valid: 78.21%\tTest: 61.01%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2617\t Train: 84.56%\t Valid: 78.58%\tTest: 61.39%\n",
      "Seed =  1\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4352\t Train: 38.24%\t Valid: 33.45%\tTest: 35.71%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.3969\t Train: 43.60%\t Valid: 34.35%\tTest: 34.90%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3833\t Train: 50.82%\t Valid: 39.96%\tTest: 38.01%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3692\t Train: 54.95%\t Valid: 46.25%\tTest: 41.96%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3608\t Train: 57.17%\t Valid: 48.55%\tTest: 43.11%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3515\t Train: 61.08%\t Valid: 53.06%\tTest: 46.15%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3417\t Train: 64.51%\t Valid: 57.91%\tTest: 48.69%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3374\t Train: 64.52%\t Valid: 58.97%\tTest: 48.55%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3302\t Train: 68.80%\t Valid: 65.69%\tTest: 54.09%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3263\t Train: 69.72%\t Valid: 67.32%\tTest: 55.04%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3353\t Train: 68.49%\t Valid: 65.08%\tTest: 54.47%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3267\t Train: 69.04%\t Valid: 65.49%\tTest: 55.28%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3234\t Train: 70.19%\t Valid: 67.48%\tTest: 55.83%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3203\t Train: 70.97%\t Valid: 68.64%\tTest: 56.50%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3182\t Train: 71.35%\t Valid: 69.35%\tTest: 56.96%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3163\t Train: 71.72%\t Valid: 69.76%\tTest: 56.89%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3146\t Train: 72.27%\t Valid: 70.48%\tTest: 57.00%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3128\t Train: 72.83%\t Valid: 71.23%\tTest: 57.49%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3238\t Train: 70.45%\t Valid: 68.20%\tTest: 56.16%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3144\t Train: 72.79%\t Valid: 70.67%\tTest: 58.43%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3099\t Train: 74.07%\t Valid: 72.57%\tTest: 60.32%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3079\t Train: 73.87%\t Valid: 71.91%\tTest: 58.82%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3061\t Train: 74.48%\t Valid: 72.68%\tTest: 59.37%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3044\t Train: 74.89%\t Valid: 72.89%\tTest: 59.42%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3028\t Train: 75.26%\t Valid: 73.16%\tTest: 60.01%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3025\t Train: 76.39%\t Valid: 74.48%\tTest: 61.76%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3002\t Train: 76.71%\t Valid: 74.48%\tTest: 61.64%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2993\t Train: 76.84%\t Valid: 74.27%\tTest: 60.93%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3009\t Train: 76.04%\t Valid: 73.77%\tTest: 62.07%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2971\t Train: 77.37%\t Valid: 75.02%\tTest: 63.08%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2954\t Train: 77.78%\t Valid: 75.07%\tTest: 62.78%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2956\t Train: 76.51%\t Valid: 73.02%\tTest: 60.30%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2940\t Train: 78.59%\t Valid: 75.38%\tTest: 62.13%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2918\t Train: 78.28%\t Valid: 75.00%\tTest: 61.98%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2902\t Train: 78.74%\t Valid: 75.18%\tTest: 61.39%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2896\t Train: 79.25%\t Valid: 75.73%\tTest: 62.04%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2895\t Train: 79.24%\t Valid: 75.35%\tTest: 61.03%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2886\t Train: 78.93%\t Valid: 75.01%\tTest: 61.08%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2874\t Train: 79.28%\t Valid: 74.87%\tTest: 60.64%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2856\t Train: 79.88%\t Valid: 75.41%\tTest: 60.32%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2841\t Train: 79.77%\t Valid: 75.73%\tTest: 60.39%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2876\t Train: 80.43%\t Valid: 76.45%\tTest: 61.56%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2829\t Train: 79.85%\t Valid: 75.38%\tTest: 59.86%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2831\t Train: 80.60%\t Valid: 75.84%\tTest: 60.24%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2813\t Train: 80.41%\t Valid: 75.13%\tTest: 58.43%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2803\t Train: 80.73%\t Valid: 75.77%\tTest: 59.90%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2811\t Train: 81.24%\t Valid: 76.93%\tTest: 62.41%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2810\t Train: 81.37%\t Valid: 77.08%\tTest: 63.02%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2802\t Train: 81.10%\t Valid: 76.94%\tTest: 61.55%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2783\t Train: 81.17%\t Valid: 76.77%\tTest: 60.74%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2768\t Train: 81.53%\t Valid: 77.04%\tTest: 61.52%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2805\t Train: 80.51%\t Valid: 75.71%\tTest: 60.82%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2778\t Train: 81.57%\t Valid: 76.36%\tTest: 60.44%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2757\t Train: 81.54%\t Valid: 76.32%\tTest: 60.03%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2754\t Train: 81.62%\t Valid: 76.27%\tTest: 59.46%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2747\t Train: 81.72%\t Valid: 76.85%\tTest: 60.50%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2740\t Train: 82.03%\t Valid: 76.99%\tTest: 60.98%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2733\t Train: 82.31%\t Valid: 77.05%\tTest: 61.37%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2734\t Train: 82.31%\t Valid: 76.93%\tTest: 61.02%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2737\t Train: 82.08%\t Valid: 77.20%\tTest: 60.77%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2721\t Train: 82.38%\t Valid: 76.90%\tTest: 60.04%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2718\t Train: 82.54%\t Valid: 77.40%\tTest: 60.97%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2721\t Train: 82.25%\t Valid: 76.99%\tTest: 59.96%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2748\t Train: 82.87%\t Valid: 77.70%\tTest: 61.75%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2740\t Train: 82.60%\t Valid: 76.78%\tTest: 59.56%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2720\t Train: 82.36%\t Valid: 75.06%\tTest: 58.62%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2715\t Train: 82.64%\t Valid: 76.99%\tTest: 61.39%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2704\t Train: 82.98%\t Valid: 77.23%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2693\t Train: 82.97%\t Valid: 77.35%\tTest: 61.41%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2695\t Train: 83.20%\t Valid: 77.50%\tTest: 61.83%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2682\t Train: 83.10%\t Valid: 76.99%\tTest: 60.86%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2683\t Train: 83.33%\t Valid: 77.58%\tTest: 61.87%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2677\t Train: 83.38%\t Valid: 77.48%\tTest: 61.09%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2688\t Train: 83.15%\t Valid: 77.35%\tTest: 61.23%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2675\t Train: 83.30%\t Valid: 77.45%\tTest: 61.35%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2683\t Train: 83.11%\t Valid: 76.95%\tTest: 60.72%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2675\t Train: 83.43%\t Valid: 77.70%\tTest: 61.79%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2671\t Train: 83.36%\t Valid: 77.38%\tTest: 61.09%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2661\t Train: 83.56%\t Valid: 77.03%\tTest: 60.42%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2675\t Train: 83.30%\t Valid: 76.88%\tTest: 60.56%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2657\t Train: 83.73%\t Valid: 77.65%\tTest: 62.38%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2661\t Train: 83.86%\t Valid: 77.55%\tTest: 61.01%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2665\t Train: 83.92%\t Valid: 77.48%\tTest: 61.12%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2672\t Train: 83.96%\t Valid: 77.65%\tTest: 62.09%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2651\t Train: 83.94%\t Valid: 77.64%\tTest: 62.03%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2643\t Train: 83.88%\t Valid: 77.36%\tTest: 61.22%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2642\t Train: 83.98%\t Valid: 77.59%\tTest: 60.92%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2631\t Train: 84.02%\t Valid: 77.55%\tTest: 61.33%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2645\t Train: 83.95%\t Valid: 77.18%\tTest: 60.41%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2639\t Train: 83.91%\t Valid: 77.50%\tTest: 61.46%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2644\t Train: 83.78%\t Valid: 77.23%\tTest: 61.76%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2629\t Train: 84.24%\t Valid: 77.47%\tTest: 61.15%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2637\t Train: 84.38%\t Valid: 77.38%\tTest: 61.22%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2641\t Train: 84.13%\t Valid: 77.80%\tTest: 61.86%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2632\t Train: 84.14%\t Valid: 77.13%\tTest: 60.38%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2630\t Train: 84.42%\t Valid: 77.93%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2631\t Train: 84.45%\t Valid: 77.66%\tTest: 61.71%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2618\t Train: 84.34%\t Valid: 78.02%\tTest: 62.33%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2635\t Train: 84.15%\t Valid: 77.60%\tTest: 62.02%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2614\t Train: 84.42%\t Valid: 76.66%\tTest: 60.73%\n",
      "Seed =  2\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4408\t Train: 37.15%\t Valid: 33.02%\tTest: 35.77%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4024\t Train: 42.79%\t Valid: 33.84%\tTest: 34.71%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3850\t Train: 48.36%\t Valid: 37.43%\tTest: 36.48%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3723\t Train: 55.19%\t Valid: 46.50%\tTest: 42.60%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3625\t Train: 57.16%\t Valid: 48.53%\tTest: 43.45%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3528\t Train: 60.00%\t Valid: 51.43%\tTest: 44.97%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3420\t Train: 63.99%\t Valid: 56.66%\tTest: 47.32%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3404\t Train: 68.72%\t Valid: 65.30%\tTest: 54.39%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3319\t Train: 67.98%\t Valid: 64.23%\tTest: 51.56%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3275\t Train: 68.82%\t Valid: 65.35%\tTest: 52.43%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3502\t Train: 65.84%\t Valid: 60.91%\tTest: 50.05%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3267\t Train: 67.37%\t Valid: 61.93%\tTest: 51.39%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3252\t Train: 69.95%\t Valid: 66.85%\tTest: 55.48%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3211\t Train: 71.00%\t Valid: 68.59%\tTest: 56.79%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3189\t Train: 71.29%\t Valid: 68.95%\tTest: 56.50%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3170\t Train: 71.51%\t Valid: 69.27%\tTest: 56.24%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3152\t Train: 71.92%\t Valid: 69.69%\tTest: 56.12%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3134\t Train: 72.66%\t Valid: 70.85%\tTest: 56.80%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3119\t Train: 72.75%\t Valid: 70.90%\tTest: 56.31%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3102\t Train: 73.66%\t Valid: 72.13%\tTest: 58.25%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3090\t Train: 73.54%\t Valid: 71.77%\tTest: 57.27%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3073\t Train: 74.77%\t Valid: 73.40%\tTest: 59.36%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3059\t Train: 75.04%\t Valid: 73.58%\tTest: 60.17%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3045\t Train: 75.48%\t Valid: 74.04%\tTest: 60.33%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3027\t Train: 75.45%\t Valid: 73.50%\tTest: 59.36%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3018\t Train: 76.36%\t Valid: 74.66%\tTest: 61.05%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3012\t Train: 75.97%\t Valid: 73.87%\tTest: 59.43%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2990\t Train: 76.28%\t Valid: 73.41%\tTest: 58.46%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2981\t Train: 76.56%\t Valid: 74.18%\tTest: 59.93%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2962\t Train: 77.70%\t Valid: 75.00%\tTest: 60.43%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2954\t Train: 77.18%\t Valid: 74.53%\tTest: 59.82%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2944\t Train: 78.26%\t Valid: 75.52%\tTest: 61.64%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2934\t Train: 77.56%\t Valid: 74.52%\tTest: 59.65%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2918\t Train: 78.46%\t Valid: 75.34%\tTest: 60.18%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2909\t Train: 78.70%\t Valid: 75.64%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2897\t Train: 78.95%\t Valid: 75.95%\tTest: 61.57%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2888\t Train: 79.26%\t Valid: 75.86%\tTest: 60.90%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2944\t Train: 78.38%\t Valid: 75.22%\tTest: 60.36%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2885\t Train: 79.38%\t Valid: 76.39%\tTest: 61.93%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2864\t Train: 79.69%\t Valid: 76.12%\tTest: 60.64%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2860\t Train: 80.14%\t Valid: 76.43%\tTest: 61.30%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2859\t Train: 79.62%\t Valid: 75.74%\tTest: 60.11%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2845\t Train: 80.34%\t Valid: 76.37%\tTest: 60.69%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2859\t Train: 79.62%\t Valid: 75.72%\tTest: 59.82%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2846\t Train: 80.23%\t Valid: 76.54%\tTest: 61.63%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2820\t Train: 80.35%\t Valid: 75.83%\tTest: 60.09%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2820\t Train: 80.98%\t Valid: 76.96%\tTest: 62.56%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2803\t Train: 81.06%\t Valid: 76.55%\tTest: 61.40%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2843\t Train: 81.29%\t Valid: 76.90%\tTest: 61.85%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2805\t Train: 80.72%\t Valid: 76.67%\tTest: 62.26%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2794\t Train: 81.37%\t Valid: 77.02%\tTest: 61.77%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2779\t Train: 81.50%\t Valid: 76.99%\tTest: 62.63%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2784\t Train: 81.70%\t Valid: 77.53%\tTest: 63.66%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2768\t Train: 81.62%\t Valid: 76.88%\tTest: 62.56%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2759\t Train: 81.65%\t Valid: 76.96%\tTest: 62.33%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2772\t Train: 82.03%\t Valid: 77.39%\tTest: 62.70%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2777\t Train: 82.07%\t Valid: 77.15%\tTest: 62.08%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2748\t Train: 82.12%\t Valid: 77.18%\tTest: 62.41%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2747\t Train: 82.28%\t Valid: 76.96%\tTest: 62.12%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2746\t Train: 81.88%\t Valid: 77.40%\tTest: 63.19%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2727\t Train: 82.24%\t Valid: 77.00%\tTest: 62.24%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2728\t Train: 82.24%\t Valid: 77.36%\tTest: 63.08%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2724\t Train: 82.50%\t Valid: 77.52%\tTest: 62.54%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2731\t Train: 81.98%\t Valid: 76.36%\tTest: 61.63%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2735\t Train: 82.24%\t Valid: 77.64%\tTest: 63.81%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2715\t Train: 82.93%\t Valid: 77.26%\tTest: 63.06%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2714\t Train: 82.64%\t Valid: 77.72%\tTest: 63.15%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2709\t Train: 82.54%\t Valid: 77.31%\tTest: 62.77%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2703\t Train: 82.96%\t Valid: 78.16%\tTest: 63.66%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2706\t Train: 83.11%\t Valid: 77.65%\tTest: 63.03%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2692\t Train: 83.15%\t Valid: 77.69%\tTest: 62.92%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2706\t Train: 83.25%\t Valid: 77.59%\tTest: 62.25%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2688\t Train: 82.87%\t Valid: 77.61%\tTest: 62.38%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2698\t Train: 83.13%\t Valid: 77.75%\tTest: 62.35%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2707\t Train: 82.99%\t Valid: 76.32%\tTest: 60.66%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2678\t Train: 83.24%\t Valid: 77.13%\tTest: 62.39%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2671\t Train: 83.39%\t Valid: 77.70%\tTest: 63.13%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2667\t Train: 83.50%\t Valid: 77.79%\tTest: 63.34%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2659\t Train: 83.50%\t Valid: 78.00%\tTest: 63.57%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2668\t Train: 83.43%\t Valid: 77.61%\tTest: 62.31%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2677\t Train: 83.48%\t Valid: 77.56%\tTest: 61.75%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2657\t Train: 83.69%\t Valid: 78.16%\tTest: 63.24%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2662\t Train: 83.87%\t Valid: 78.07%\tTest: 63.27%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2683\t Train: 83.88%\t Valid: 78.00%\tTest: 63.23%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2660\t Train: 83.77%\t Valid: 77.90%\tTest: 63.32%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2649\t Train: 83.98%\t Valid: 78.04%\tTest: 63.60%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2683\t Train: 83.40%\t Valid: 78.31%\tTest: 64.17%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2661\t Train: 83.79%\t Valid: 78.38%\tTest: 64.54%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2637\t Train: 83.94%\t Valid: 78.25%\tTest: 64.48%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2649\t Train: 83.94%\t Valid: 78.10%\tTest: 64.04%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2636\t Train: 84.09%\t Valid: 77.94%\tTest: 63.47%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2641\t Train: 84.18%\t Valid: 78.15%\tTest: 62.87%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2641\t Train: 83.82%\t Valid: 77.72%\tTest: 62.37%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2629\t Train: 84.28%\t Valid: 77.91%\tTest: 63.04%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2634\t Train: 84.04%\t Valid: 78.21%\tTest: 63.45%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2629\t Train: 84.36%\t Valid: 78.23%\tTest: 63.85%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2622\t Train: 84.18%\t Valid: 78.57%\tTest: 64.13%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2627\t Train: 84.40%\t Valid: 78.20%\tTest: 63.29%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2622\t Train: 84.37%\t Valid: 78.40%\tTest: 63.62%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2618\t Train: 84.46%\t Valid: 78.25%\tTest: 63.77%\n",
      "Seed =  3\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4390\t Train: 37.48%\t Valid: 33.00%\tTest: 35.58%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.3999\t Train: 43.16%\t Valid: 34.16%\tTest: 34.88%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3841\t Train: 49.34%\t Valid: 38.67%\tTest: 37.39%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3702\t Train: 55.88%\t Valid: 47.31%\tTest: 42.98%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3599\t Train: 58.26%\t Valid: 49.84%\tTest: 44.40%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3503\t Train: 61.42%\t Valid: 53.28%\tTest: 46.64%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3420\t Train: 64.31%\t Valid: 57.75%\tTest: 49.02%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3345\t Train: 65.74%\t Valid: 60.78%\tTest: 50.13%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3298\t Train: 66.81%\t Valid: 62.99%\tTest: 51.28%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3271\t Train: 68.38%\t Valid: 65.12%\tTest: 52.63%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3230\t Train: 69.33%\t Valid: 66.64%\tTest: 54.03%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3209\t Train: 71.62%\t Valid: 70.08%\tTest: 57.01%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3184\t Train: 70.71%\t Valid: 68.41%\tTest: 55.63%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3164\t Train: 72.85%\t Valid: 71.62%\tTest: 58.63%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3179\t Train: 71.92%\t Valid: 69.88%\tTest: 56.99%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3139\t Train: 72.25%\t Valid: 70.02%\tTest: 57.57%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3122\t Train: 73.35%\t Valid: 71.33%\tTest: 58.96%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3101\t Train: 73.14%\t Valid: 70.79%\tTest: 58.22%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3098\t Train: 74.67%\t Valid: 72.95%\tTest: 60.64%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3061\t Train: 74.57%\t Valid: 72.36%\tTest: 59.49%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3130\t Train: 73.37%\t Valid: 70.94%\tTest: 58.93%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3052\t Train: 74.44%\t Valid: 71.98%\tTest: 58.80%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3029\t Train: 75.16%\t Valid: 72.58%\tTest: 59.80%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3010\t Train: 75.96%\t Valid: 73.43%\tTest: 60.55%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.2995\t Train: 75.98%\t Valid: 72.95%\tTest: 59.78%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.2986\t Train: 77.08%\t Valid: 74.39%\tTest: 61.02%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.2981\t Train: 76.70%\t Valid: 73.31%\tTest: 58.97%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2957\t Train: 77.23%\t Valid: 74.08%\tTest: 60.40%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3045\t Train: 73.51%\t Valid: 68.77%\tTest: 55.58%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3028\t Train: 77.11%\t Valid: 72.98%\tTest: 57.92%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2948\t Train: 77.47%\t Valid: 73.47%\tTest: 59.56%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2926\t Train: 78.10%\t Valid: 74.24%\tTest: 60.77%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2909\t Train: 78.56%\t Valid: 74.93%\tTest: 61.87%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2895\t Train: 78.80%\t Valid: 74.87%\tTest: 61.54%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2934\t Train: 77.77%\t Valid: 73.43%\tTest: 59.80%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2885\t Train: 79.06%\t Valid: 74.68%\tTest: 59.71%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2873\t Train: 79.44%\t Valid: 74.88%\tTest: 60.31%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2859\t Train: 79.64%\t Valid: 75.09%\tTest: 60.60%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2848\t Train: 79.90%\t Valid: 75.15%\tTest: 60.29%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2837\t Train: 79.83%\t Valid: 74.98%\tTest: 60.08%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2887\t Train: 80.24%\t Valid: 75.03%\tTest: 58.52%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2831\t Train: 80.00%\t Valid: 75.29%\tTest: 59.98%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2820\t Train: 80.42%\t Valid: 75.77%\tTest: 60.49%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2806\t Train: 80.54%\t Valid: 75.75%\tTest: 60.22%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2825\t Train: 81.13%\t Valid: 76.01%\tTest: 59.93%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2807\t Train: 80.92%\t Valid: 75.68%\tTest: 59.09%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2785\t Train: 80.92%\t Valid: 76.13%\tTest: 60.34%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2789\t Train: 81.54%\t Valid: 76.78%\tTest: 61.97%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2789\t Train: 81.69%\t Valid: 77.01%\tTest: 62.35%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2763\t Train: 81.19%\t Valid: 75.65%\tTest: 59.49%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2761\t Train: 81.71%\t Valid: 76.65%\tTest: 61.21%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2761\t Train: 82.03%\t Valid: 77.12%\tTest: 61.82%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2744\t Train: 81.96%\t Valid: 76.53%\tTest: 60.78%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2738\t Train: 81.98%\t Valid: 76.57%\tTest: 61.42%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2738\t Train: 82.11%\t Valid: 76.61%\tTest: 60.67%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2729\t Train: 82.27%\t Valid: 76.65%\tTest: 60.86%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2723\t Train: 82.49%\t Valid: 77.31%\tTest: 62.47%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2729\t Train: 82.10%\t Valid: 75.85%\tTest: 60.20%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2732\t Train: 82.58%\t Valid: 77.17%\tTest: 62.39%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2716\t Train: 82.65%\t Valid: 76.42%\tTest: 60.78%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2725\t Train: 82.48%\t Valid: 77.74%\tTest: 65.10%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2715\t Train: 82.39%\t Valid: 77.08%\tTest: 64.12%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2770\t Train: 82.03%\t Valid: 75.42%\tTest: 60.48%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2727\t Train: 82.68%\t Valid: 77.01%\tTest: 63.01%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2705\t Train: 82.82%\t Valid: 77.08%\tTest: 61.54%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2692\t Train: 82.84%\t Valid: 77.09%\tTest: 62.17%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2687\t Train: 83.10%\t Valid: 77.65%\tTest: 63.54%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2676\t Train: 83.12%\t Valid: 77.11%\tTest: 62.04%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2677\t Train: 83.30%\t Valid: 77.47%\tTest: 62.43%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2673\t Train: 83.37%\t Valid: 77.57%\tTest: 62.90%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2674\t Train: 83.30%\t Valid: 76.86%\tTest: 60.85%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2676\t Train: 83.47%\t Valid: 77.94%\tTest: 63.64%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2679\t Train: 83.61%\t Valid: 77.95%\tTest: 63.92%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2663\t Train: 83.50%\t Valid: 77.44%\tTest: 62.70%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2658\t Train: 83.47%\t Valid: 77.36%\tTest: 62.33%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2654\t Train: 83.64%\t Valid: 77.74%\tTest: 63.30%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2654\t Train: 83.65%\t Valid: 77.52%\tTest: 63.00%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2654\t Train: 83.64%\t Valid: 77.53%\tTest: 62.58%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2656\t Train: 83.66%\t Valid: 77.76%\tTest: 63.44%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2649\t Train: 83.71%\t Valid: 77.76%\tTest: 63.43%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2640\t Train: 83.83%\t Valid: 77.70%\tTest: 63.01%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2646\t Train: 84.06%\t Valid: 77.75%\tTest: 63.08%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2643\t Train: 84.04%\t Valid: 77.55%\tTest: 62.15%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2639\t Train: 84.07%\t Valid: 77.66%\tTest: 62.60%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2629\t Train: 84.12%\t Valid: 77.72%\tTest: 62.67%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2630\t Train: 84.12%\t Valid: 77.51%\tTest: 62.75%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2624\t Train: 84.27%\t Valid: 77.63%\tTest: 62.57%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2633\t Train: 84.13%\t Valid: 77.91%\tTest: 62.82%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2659\t Train: 84.31%\t Valid: 78.18%\tTest: 63.39%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2622\t Train: 84.22%\t Valid: 77.60%\tTest: 61.94%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2636\t Train: 84.25%\t Valid: 78.42%\tTest: 63.94%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2627\t Train: 84.27%\t Valid: 77.43%\tTest: 62.23%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2625\t Train: 84.50%\t Valid: 77.79%\tTest: 62.54%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2616\t Train: 84.50%\t Valid: 78.00%\tTest: 63.38%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2617\t Train: 84.57%\t Valid: 77.71%\tTest: 62.77%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2611\t Train: 84.61%\t Valid: 78.08%\tTest: 63.75%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2616\t Train: 84.67%\t Valid: 78.05%\tTest: 63.66%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2610\t Train: 84.67%\t Valid: 78.00%\tTest: 64.02%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2605\t Train: 84.70%\t Valid: 78.26%\tTest: 64.17%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2604\t Train: 84.69%\t Valid: 78.15%\tTest: 63.33%\n",
      "Seed =  4\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4381\t Train: 37.01%\t Valid: 33.58%\tTest: 36.29%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.3991\t Train: 42.72%\t Valid: 33.95%\tTest: 34.79%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3846\t Train: 50.14%\t Valid: 38.76%\tTest: 37.06%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3699\t Train: 54.30%\t Valid: 45.75%\tTest: 41.80%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3622\t Train: 57.01%\t Valid: 48.40%\tTest: 42.99%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3536\t Train: 59.97%\t Valid: 51.91%\tTest: 45.10%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3441\t Train: 63.19%\t Valid: 55.56%\tTest: 47.02%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3360\t Train: 66.69%\t Valid: 61.93%\tTest: 50.46%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3316\t Train: 68.91%\t Valid: 65.74%\tTest: 53.94%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3293\t Train: 69.20%\t Valid: 66.12%\tTest: 54.20%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3258\t Train: 69.63%\t Valid: 66.86%\tTest: 54.37%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3231\t Train: 69.62%\t Valid: 66.72%\tTest: 53.98%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3210\t Train: 70.18%\t Valid: 67.37%\tTest: 54.12%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3217\t Train: 70.19%\t Valid: 67.25%\tTest: 54.23%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3174\t Train: 71.93%\t Valid: 69.48%\tTest: 56.54%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3157\t Train: 72.32%\t Valid: 69.95%\tTest: 56.60%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3216\t Train: 74.12%\t Valid: 72.32%\tTest: 60.30%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3205\t Train: 71.75%\t Valid: 68.73%\tTest: 56.33%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3144\t Train: 72.82%\t Valid: 70.09%\tTest: 58.90%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3115\t Train: 73.17%\t Valid: 70.38%\tTest: 58.27%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3094\t Train: 73.66%\t Valid: 71.37%\tTest: 58.68%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3077\t Train: 74.04%\t Valid: 71.98%\tTest: 58.21%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3062\t Train: 74.43%\t Valid: 72.29%\tTest: 57.97%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3049\t Train: 74.27%\t Valid: 71.66%\tTest: 56.44%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3048\t Train: 74.78%\t Valid: 71.91%\tTest: 56.80%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3015\t Train: 76.17%\t Valid: 73.79%\tTest: 59.51%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3002\t Train: 76.38%\t Valid: 73.75%\tTest: 59.35%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2995\t Train: 76.41%\t Valid: 73.01%\tTest: 57.09%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2989\t Train: 77.14%\t Valid: 74.39%\tTest: 60.04%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2974\t Train: 77.22%\t Valid: 74.22%\tTest: 59.54%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2958\t Train: 77.52%\t Valid: 73.79%\tTest: 57.86%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2936\t Train: 78.12%\t Valid: 75.00%\tTest: 60.06%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2952\t Train: 78.90%\t Valid: 75.78%\tTest: 61.62%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2952\t Train: 77.84%\t Valid: 73.33%\tTest: 55.98%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2927\t Train: 78.37%\t Valid: 74.80%\tTest: 57.91%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2900\t Train: 78.87%\t Valid: 75.17%\tTest: 59.27%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2882\t Train: 79.34%\t Valid: 75.52%\tTest: 59.92%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2868\t Train: 79.37%\t Valid: 75.13%\tTest: 59.10%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2893\t Train: 79.58%\t Valid: 75.34%\tTest: 59.96%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2848\t Train: 79.80%\t Valid: 75.09%\tTest: 59.24%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2849\t Train: 79.96%\t Valid: 75.88%\tTest: 59.89%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2832\t Train: 80.02%\t Valid: 75.52%\tTest: 59.26%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2842\t Train: 79.71%\t Valid: 75.00%\tTest: 57.83%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2828\t Train: 80.86%\t Valid: 76.75%\tTest: 60.97%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2827\t Train: 80.71%\t Valid: 75.57%\tTest: 58.78%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2800\t Train: 80.61%\t Valid: 75.47%\tTest: 58.71%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2797\t Train: 80.86%\t Valid: 76.40%\tTest: 60.53%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2792\t Train: 81.18%\t Valid: 76.95%\tTest: 61.76%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2793\t Train: 81.28%\t Valid: 76.29%\tTest: 59.37%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2801\t Train: 80.43%\t Valid: 74.26%\tTest: 56.42%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2776\t Train: 81.44%\t Valid: 76.10%\tTest: 59.46%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2764\t Train: 81.66%\t Valid: 76.71%\tTest: 61.16%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2759\t Train: 81.85%\t Valid: 77.02%\tTest: 61.33%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2768\t Train: 82.07%\t Valid: 77.33%\tTest: 62.00%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2746\t Train: 82.14%\t Valid: 77.26%\tTest: 61.69%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2743\t Train: 82.32%\t Valid: 77.19%\tTest: 60.84%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2755\t Train: 82.28%\t Valid: 77.09%\tTest: 60.63%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2739\t Train: 82.10%\t Valid: 76.87%\tTest: 60.83%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2739\t Train: 82.44%\t Valid: 77.67%\tTest: 62.97%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2726\t Train: 82.63%\t Valid: 77.60%\tTest: 61.92%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2724\t Train: 82.72%\t Valid: 77.98%\tTest: 62.73%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2726\t Train: 82.64%\t Valid: 77.65%\tTest: 62.11%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2709\t Train: 82.78%\t Valid: 77.41%\tTest: 62.08%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2705\t Train: 82.71%\t Valid: 77.19%\tTest: 61.08%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2698\t Train: 82.82%\t Valid: 77.36%\tTest: 60.88%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2699\t Train: 82.83%\t Valid: 77.16%\tTest: 60.54%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2689\t Train: 83.02%\t Valid: 77.30%\tTest: 61.48%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2697\t Train: 83.09%\t Valid: 77.73%\tTest: 62.63%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2705\t Train: 83.28%\t Valid: 77.63%\tTest: 62.39%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2676\t Train: 83.19%\t Valid: 76.91%\tTest: 61.37%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2682\t Train: 83.31%\t Valid: 77.35%\tTest: 62.60%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2683\t Train: 83.22%\t Valid: 77.77%\tTest: 63.30%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2674\t Train: 83.50%\t Valid: 77.59%\tTest: 62.28%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2667\t Train: 83.51%\t Valid: 77.21%\tTest: 61.57%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2664\t Train: 83.52%\t Valid: 77.61%\tTest: 61.63%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2702\t Train: 82.94%\t Valid: 76.46%\tTest: 60.56%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2677\t Train: 83.55%\t Valid: 77.75%\tTest: 62.50%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2683\t Train: 83.74%\t Valid: 78.31%\tTest: 63.63%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2670\t Train: 83.61%\t Valid: 78.10%\tTest: 63.45%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2655\t Train: 83.86%\t Valid: 77.61%\tTest: 62.23%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2647\t Train: 83.82%\t Valid: 77.39%\tTest: 62.15%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2644\t Train: 83.91%\t Valid: 77.69%\tTest: 62.37%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2643\t Train: 83.85%\t Valid: 77.49%\tTest: 61.92%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2649\t Train: 83.74%\t Valid: 77.14%\tTest: 61.03%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2643\t Train: 83.90%\t Valid: 77.06%\tTest: 61.10%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2652\t Train: 83.95%\t Valid: 77.25%\tTest: 61.37%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2646\t Train: 83.99%\t Valid: 76.81%\tTest: 59.98%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2638\t Train: 84.16%\t Valid: 77.86%\tTest: 62.27%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2644\t Train: 84.22%\t Valid: 77.63%\tTest: 61.76%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2636\t Train: 84.28%\t Valid: 77.94%\tTest: 62.10%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2621\t Train: 84.28%\t Valid: 77.40%\tTest: 61.06%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2638\t Train: 84.25%\t Valid: 78.38%\tTest: 63.25%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2629\t Train: 84.49%\t Valid: 77.87%\tTest: 61.54%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2640\t Train: 84.46%\t Valid: 78.18%\tTest: 62.63%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2615\t Train: 84.39%\t Valid: 77.55%\tTest: 61.65%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2616\t Train: 84.47%\t Valid: 77.31%\tTest: 61.22%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2620\t Train: 84.42%\t Valid: 77.95%\tTest: 62.73%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2608\t Train: 84.59%\t Valid: 77.88%\tTest: 62.26%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2632\t Train: 84.35%\t Valid: 78.08%\tTest: 63.46%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2612\t Train: 84.64%\t Valid: 77.73%\tTest: 62.48%\n",
      "Seed =  5\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4360\t Train: 37.73%\t Valid: 33.46%\tTest: 36.10%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4001\t Train: 43.02%\t Valid: 34.01%\tTest: 34.82%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3855\t Train: 49.13%\t Valid: 37.49%\tTest: 36.29%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3708\t Train: 54.54%\t Valid: 45.05%\tTest: 41.30%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3612\t Train: 57.10%\t Valid: 47.97%\tTest: 42.73%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3509\t Train: 61.08%\t Valid: 52.51%\tTest: 45.29%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3407\t Train: 65.49%\t Valid: 58.56%\tTest: 48.21%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3339\t Train: 65.43%\t Valid: 59.03%\tTest: 47.24%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3310\t Train: 66.94%\t Valid: 61.40%\tTest: 48.12%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3275\t Train: 69.01%\t Valid: 65.20%\tTest: 51.56%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3305\t Train: 70.54%\t Valid: 67.61%\tTest: 54.29%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3239\t Train: 71.00%\t Valid: 68.22%\tTest: 54.86%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3211\t Train: 71.26%\t Valid: 68.70%\tTest: 54.81%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3197\t Train: 70.43%\t Valid: 67.53%\tTest: 53.33%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3175\t Train: 71.53%\t Valid: 69.23%\tTest: 54.72%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3156\t Train: 71.57%\t Valid: 69.28%\tTest: 54.42%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3136\t Train: 72.88%\t Valid: 71.22%\tTest: 56.60%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3120\t Train: 72.85%\t Valid: 71.14%\tTest: 56.31%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3100\t Train: 73.19%\t Valid: 71.47%\tTest: 56.46%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3088\t Train: 74.48%\t Valid: 72.89%\tTest: 58.54%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3070\t Train: 73.65%\t Valid: 71.63%\tTest: 56.63%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3052\t Train: 74.50%\t Valid: 72.60%\tTest: 58.47%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3109\t Train: 74.77%\t Valid: 72.47%\tTest: 58.27%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3039\t Train: 75.48%\t Valid: 73.60%\tTest: 59.81%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3018\t Train: 75.75%\t Valid: 73.86%\tTest: 60.35%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3007\t Train: 76.09%\t Valid: 73.93%\tTest: 59.46%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.2985\t Train: 76.66%\t Valid: 73.95%\tTest: 58.72%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2979\t Train: 76.15%\t Valid: 73.12%\tTest: 57.05%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2976\t Train: 77.59%\t Valid: 74.73%\tTest: 58.69%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2972\t Train: 77.46%\t Valid: 74.62%\tTest: 59.78%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2945\t Train: 77.95%\t Valid: 75.25%\tTest: 59.73%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2932\t Train: 78.13%\t Valid: 74.67%\tTest: 57.94%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2929\t Train: 77.80%\t Valid: 73.89%\tTest: 55.76%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2911\t Train: 78.85%\t Valid: 75.45%\tTest: 57.89%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2922\t Train: 78.24%\t Valid: 74.41%\tTest: 56.49%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2896\t Train: 78.74%\t Valid: 74.84%\tTest: 57.18%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2888\t Train: 79.42%\t Valid: 75.99%\tTest: 59.16%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2875\t Train: 79.33%\t Valid: 75.26%\tTest: 57.11%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2875\t Train: 79.42%\t Valid: 74.97%\tTest: 56.51%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2864\t Train: 80.23%\t Valid: 76.62%\tTest: 59.86%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2854\t Train: 80.20%\t Valid: 76.56%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2856\t Train: 79.95%\t Valid: 74.79%\tTest: 56.11%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2829\t Train: 80.02%\t Valid: 74.92%\tTest: 56.65%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2820\t Train: 80.35%\t Valid: 75.13%\tTest: 56.95%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2818\t Train: 80.79%\t Valid: 76.10%\tTest: 58.73%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2807\t Train: 80.94%\t Valid: 76.29%\tTest: 58.04%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2840\t Train: 80.97%\t Valid: 76.65%\tTest: 58.24%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2825\t Train: 80.66%\t Valid: 75.16%\tTest: 58.16%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2795\t Train: 80.96%\t Valid: 76.02%\tTest: 59.41%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2787\t Train: 81.22%\t Valid: 75.78%\tTest: 57.83%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2778\t Train: 81.48%\t Valid: 76.55%\tTest: 59.68%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2784\t Train: 81.28%\t Valid: 76.34%\tTest: 58.77%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2765\t Train: 81.43%\t Valid: 76.33%\tTest: 58.88%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2772\t Train: 81.92%\t Valid: 76.38%\tTest: 58.79%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2759\t Train: 81.83%\t Valid: 76.39%\tTest: 60.02%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2799\t Train: 81.94%\t Valid: 76.98%\tTest: 60.78%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2751\t Train: 81.84%\t Valid: 76.75%\tTest: 60.44%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2740\t Train: 81.99%\t Valid: 76.61%\tTest: 60.14%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2735\t Train: 82.08%\t Valid: 76.69%\tTest: 59.70%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2732\t Train: 82.27%\t Valid: 76.59%\tTest: 59.88%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2731\t Train: 82.35%\t Valid: 77.13%\tTest: 60.33%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2721\t Train: 82.40%\t Valid: 76.96%\tTest: 60.28%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2720\t Train: 82.60%\t Valid: 77.13%\tTest: 60.60%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2712\t Train: 82.65%\t Valid: 77.10%\tTest: 60.54%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2706\t Train: 82.71%\t Valid: 76.91%\tTest: 60.74%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2705\t Train: 82.78%\t Valid: 77.31%\tTest: 61.56%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2699\t Train: 82.85%\t Valid: 77.09%\tTest: 60.99%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2693\t Train: 82.82%\t Valid: 77.07%\tTest: 60.37%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2710\t Train: 82.43%\t Valid: 76.45%\tTest: 60.00%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2700\t Train: 82.57%\t Valid: 77.05%\tTest: 61.21%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2713\t Train: 82.26%\t Valid: 76.15%\tTest: 61.44%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2712\t Train: 82.77%\t Valid: 75.69%\tTest: 59.84%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2714\t Train: 83.00%\t Valid: 75.44%\tTest: 58.85%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2696\t Train: 83.05%\t Valid: 76.61%\tTest: 61.68%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2675\t Train: 83.26%\t Valid: 76.92%\tTest: 61.16%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2673\t Train: 83.37%\t Valid: 77.16%\tTest: 61.64%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2677\t Train: 83.19%\t Valid: 76.87%\tTest: 61.65%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2682\t Train: 83.21%\t Valid: 76.85%\tTest: 60.90%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2666\t Train: 83.38%\t Valid: 77.86%\tTest: 62.89%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2670\t Train: 83.54%\t Valid: 77.70%\tTest: 62.59%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2661\t Train: 83.73%\t Valid: 77.32%\tTest: 62.18%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2656\t Train: 83.74%\t Valid: 77.19%\tTest: 62.03%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2655\t Train: 83.84%\t Valid: 77.54%\tTest: 62.85%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2678\t Train: 83.75%\t Valid: 77.80%\tTest: 64.03%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2664\t Train: 83.64%\t Valid: 77.67%\tTest: 63.69%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2650\t Train: 83.86%\t Valid: 77.44%\tTest: 62.38%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2645\t Train: 83.94%\t Valid: 77.63%\tTest: 62.48%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2644\t Train: 83.96%\t Valid: 77.31%\tTest: 62.20%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2642\t Train: 84.08%\t Valid: 77.11%\tTest: 61.94%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2635\t Train: 84.08%\t Valid: 77.45%\tTest: 62.52%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2634\t Train: 84.01%\t Valid: 77.53%\tTest: 62.67%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2632\t Train: 84.12%\t Valid: 77.39%\tTest: 62.87%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2628\t Train: 84.25%\t Valid: 77.28%\tTest: 62.35%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2624\t Train: 84.31%\t Valid: 77.62%\tTest: 62.78%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2619\t Train: 84.28%\t Valid: 77.53%\tTest: 62.75%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2629\t Train: 84.32%\t Valid: 77.45%\tTest: 62.77%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2620\t Train: 84.43%\t Valid: 77.65%\tTest: 63.03%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2624\t Train: 84.46%\t Valid: 77.79%\tTest: 63.76%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2618\t Train: 84.39%\t Valid: 77.71%\tTest: 63.08%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2620\t Train: 84.50%\t Valid: 77.53%\tTest: 62.42%\n",
      "Seed =  6\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4396\t Train: 37.23%\t Valid: 33.02%\tTest: 35.72%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4022\t Train: 43.04%\t Valid: 33.82%\tTest: 34.60%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3843\t Train: 48.72%\t Valid: 37.66%\tTest: 36.48%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3718\t Train: 55.60%\t Valid: 46.60%\tTest: 42.21%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3619\t Train: 57.25%\t Valid: 48.40%\tTest: 42.81%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3524\t Train: 60.62%\t Valid: 52.17%\tTest: 44.93%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3418\t Train: 64.36%\t Valid: 57.15%\tTest: 47.40%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3346\t Train: 65.53%\t Valid: 59.72%\tTest: 47.76%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3314\t Train: 66.50%\t Valid: 61.89%\tTest: 49.38%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3263\t Train: 69.59%\t Valid: 66.73%\tTest: 53.53%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3251\t Train: 68.08%\t Valid: 64.19%\tTest: 50.73%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3216\t Train: 71.47%\t Valid: 69.19%\tTest: 56.15%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3203\t Train: 71.04%\t Valid: 68.46%\tTest: 54.92%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3173\t Train: 71.25%\t Valid: 68.62%\tTest: 54.44%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3157\t Train: 72.70%\t Valid: 70.74%\tTest: 56.34%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3143\t Train: 71.61%\t Valid: 68.91%\tTest: 54.84%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3121\t Train: 72.68%\t Valid: 70.44%\tTest: 56.33%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3098\t Train: 73.22%\t Valid: 70.88%\tTest: 56.56%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3153\t Train: 72.23%\t Valid: 69.61%\tTest: 56.84%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3085\t Train: 74.80%\t Valid: 72.52%\tTest: 59.37%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3069\t Train: 74.87%\t Valid: 72.77%\tTest: 59.27%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3047\t Train: 74.89%\t Valid: 72.74%\tTest: 59.29%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3030\t Train: 75.24%\t Valid: 72.80%\tTest: 58.91%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3019\t Train: 76.31%\t Valid: 74.03%\tTest: 60.07%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3016\t Train: 75.60%\t Valid: 72.78%\tTest: 58.38%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.2993\t Train: 76.24%\t Valid: 73.09%\tTest: 57.43%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.2978\t Train: 77.08%\t Valid: 74.25%\tTest: 58.71%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2992\t Train: 75.73%\t Valid: 72.05%\tTest: 55.77%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2966\t Train: 77.82%\t Valid: 75.01%\tTest: 61.13%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2963\t Train: 78.28%\t Valid: 75.32%\tTest: 60.23%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2937\t Train: 77.62%\t Valid: 74.71%\tTest: 59.68%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2936\t Train: 78.75%\t Valid: 75.30%\tTest: 58.66%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2933\t Train: 77.84%\t Valid: 73.82%\tTest: 55.58%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2901\t Train: 78.77%\t Valid: 74.50%\tTest: 56.47%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2884\t Train: 79.34%\t Valid: 75.42%\tTest: 58.27%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2869\t Train: 79.60%\t Valid: 75.75%\tTest: 58.67%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2886\t Train: 78.49%\t Valid: 73.96%\tTest: 55.58%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2887\t Train: 79.59%\t Valid: 75.52%\tTest: 56.98%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2850\t Train: 80.08%\t Valid: 76.13%\tTest: 59.52%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2841\t Train: 80.25%\t Valid: 76.26%\tTest: 59.24%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2840\t Train: 80.52%\t Valid: 76.60%\tTest: 59.99%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2839\t Train: 79.80%\t Valid: 74.37%\tTest: 56.04%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2820\t Train: 80.87%\t Valid: 76.81%\tTest: 60.02%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2836\t Train: 81.00%\t Valid: 75.94%\tTest: 57.52%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2819\t Train: 80.52%\t Valid: 75.95%\tTest: 60.38%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2796\t Train: 80.96%\t Valid: 76.39%\tTest: 58.93%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2794\t Train: 81.03%\t Valid: 76.84%\tTest: 60.37%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2805\t Train: 80.96%\t Valid: 76.38%\tTest: 59.88%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2777\t Train: 81.21%\t Valid: 75.96%\tTest: 58.59%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2769\t Train: 81.65%\t Valid: 76.77%\tTest: 59.27%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2759\t Train: 81.69%\t Valid: 76.75%\tTest: 59.58%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2763\t Train: 81.91%\t Valid: 77.00%\tTest: 60.24%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2747\t Train: 81.99%\t Valid: 77.25%\tTest: 60.87%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2743\t Train: 82.01%\t Valid: 76.84%\tTest: 59.12%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2747\t Train: 82.16%\t Valid: 76.88%\tTest: 59.19%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2745\t Train: 82.00%\t Valid: 76.52%\tTest: 58.85%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2737\t Train: 82.17%\t Valid: 76.98%\tTest: 59.39%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2748\t Train: 82.52%\t Valid: 77.32%\tTest: 60.52%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2744\t Train: 82.15%\t Valid: 77.30%\tTest: 61.41%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2735\t Train: 82.59%\t Valid: 77.14%\tTest: 59.19%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2741\t Train: 82.37%\t Valid: 76.13%\tTest: 58.00%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2762\t Train: 82.23%\t Valid: 77.01%\tTest: 61.48%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2720\t Train: 82.71%\t Valid: 77.37%\tTest: 59.81%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2709\t Train: 82.79%\t Valid: 77.69%\tTest: 61.49%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2705\t Train: 82.89%\t Valid: 77.72%\tTest: 60.97%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2700\t Train: 83.01%\t Valid: 77.52%\tTest: 61.15%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2719\t Train: 83.19%\t Valid: 77.76%\tTest: 61.22%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2723\t Train: 82.95%\t Valid: 76.97%\tTest: 59.22%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2693\t Train: 83.00%\t Valid: 77.11%\tTest: 60.71%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2685\t Train: 83.11%\t Valid: 77.34%\tTest: 60.49%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2679\t Train: 83.27%\t Valid: 77.54%\tTest: 60.95%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2694\t Train: 83.40%\t Valid: 77.83%\tTest: 60.92%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2681\t Train: 83.33%\t Valid: 77.16%\tTest: 59.98%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2684\t Train: 83.47%\t Valid: 78.13%\tTest: 61.43%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2665\t Train: 83.44%\t Valid: 78.05%\tTest: 61.39%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2672\t Train: 83.46%\t Valid: 77.25%\tTest: 60.16%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2670\t Train: 83.61%\t Valid: 77.46%\tTest: 60.40%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2678\t Train: 83.76%\t Valid: 78.24%\tTest: 61.75%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2664\t Train: 83.76%\t Valid: 77.89%\tTest: 61.14%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2676\t Train: 83.85%\t Valid: 78.27%\tTest: 61.77%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2653\t Train: 83.71%\t Valid: 77.64%\tTest: 60.80%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2658\t Train: 83.92%\t Valid: 77.91%\tTest: 61.77%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2665\t Train: 83.64%\t Valid: 77.18%\tTest: 60.64%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2650\t Train: 83.97%\t Valid: 78.35%\tTest: 62.04%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2650\t Train: 84.01%\t Valid: 77.76%\tTest: 61.82%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2643\t Train: 84.09%\t Valid: 78.19%\tTest: 61.93%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2654\t Train: 84.02%\t Valid: 77.25%\tTest: 60.38%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2701\t Train: 83.75%\t Valid: 77.97%\tTest: 63.59%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2659\t Train: 83.91%\t Valid: 78.62%\tTest: 63.96%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2649\t Train: 84.06%\t Valid: 78.33%\tTest: 62.41%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2638\t Train: 84.12%\t Valid: 78.02%\tTest: 61.14%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2630\t Train: 84.25%\t Valid: 78.07%\tTest: 61.58%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2640\t Train: 84.36%\t Valid: 78.23%\tTest: 62.13%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2626\t Train: 84.38%\t Valid: 78.24%\tTest: 62.05%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2626\t Train: 84.42%\t Valid: 78.19%\tTest: 62.24%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2626\t Train: 84.35%\t Valid: 77.80%\tTest: 61.23%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2619\t Train: 84.44%\t Valid: 78.22%\tTest: 62.08%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2618\t Train: 84.39%\t Valid: 78.11%\tTest: 62.11%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2631\t Train: 84.57%\t Valid: 78.48%\tTest: 62.32%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2614\t Train: 84.55%\t Valid: 77.64%\tTest: 61.93%\n",
      "Seed =  7\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4389\t Train: 37.39%\t Valid: 33.18%\tTest: 35.87%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.3992\t Train: 43.05%\t Valid: 34.32%\tTest: 34.95%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3838\t Train: 50.01%\t Valid: 38.98%\tTest: 37.50%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3694\t Train: 54.98%\t Valid: 46.31%\tTest: 42.34%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3608\t Train: 57.07%\t Valid: 48.34%\tTest: 43.08%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3513\t Train: 61.09%\t Valid: 52.98%\tTest: 46.06%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3416\t Train: 64.37%\t Valid: 57.20%\tTest: 48.27%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3356\t Train: 64.52%\t Valid: 58.58%\tTest: 47.93%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3307\t Train: 69.13%\t Valid: 66.04%\tTest: 53.55%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3262\t Train: 69.48%\t Valid: 66.80%\tTest: 54.00%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3243\t Train: 69.18%\t Valid: 66.39%\tTest: 53.10%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3226\t Train: 72.04%\t Valid: 70.77%\tTest: 57.37%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3201\t Train: 70.57%\t Valid: 68.60%\tTest: 55.25%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3179\t Train: 71.96%\t Valid: 70.61%\tTest: 57.21%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3155\t Train: 71.84%\t Valid: 70.28%\tTest: 56.62%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3134\t Train: 72.51%\t Valid: 70.87%\tTest: 56.82%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3160\t Train: 70.04%\t Valid: 67.13%\tTest: 55.23%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3157\t Train: 72.94%\t Valid: 71.22%\tTest: 58.95%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3110\t Train: 73.43%\t Valid: 71.92%\tTest: 60.03%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3089\t Train: 73.85%\t Valid: 72.23%\tTest: 59.37%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3071\t Train: 74.32%\t Valid: 72.75%\tTest: 59.23%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3053\t Train: 74.61%\t Valid: 72.81%\tTest: 59.00%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3043\t Train: 75.36%\t Valid: 73.56%\tTest: 59.39%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3042\t Train: 74.79%\t Valid: 72.26%\tTest: 57.71%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3018\t Train: 75.73%\t Valid: 73.11%\tTest: 58.02%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3004\t Train: 76.65%\t Valid: 74.41%\tTest: 59.78%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3020\t Train: 75.22%\t Valid: 71.26%\tTest: 53.49%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2977\t Train: 77.12%\t Valid: 74.68%\tTest: 59.25%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2962\t Train: 77.31%\t Valid: 74.15%\tTest: 58.11%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2957\t Train: 76.93%\t Valid: 73.59%\tTest: 56.85%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2938\t Train: 78.00%\t Valid: 74.54%\tTest: 57.78%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2970\t Train: 76.47%\t Valid: 72.97%\tTest: 56.60%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2950\t Train: 78.27%\t Valid: 74.45%\tTest: 57.31%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2911\t Train: 78.74%\t Valid: 74.88%\tTest: 56.56%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2896\t Train: 78.91%\t Valid: 75.43%\tTest: 58.30%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2890\t Train: 79.33%\t Valid: 75.57%\tTest: 57.77%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2878\t Train: 79.28%\t Valid: 75.24%\tTest: 56.82%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2871\t Train: 79.71%\t Valid: 76.47%\tTest: 60.14%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2875\t Train: 79.75%\t Valid: 75.99%\tTest: 58.66%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2855\t Train: 79.87%\t Valid: 75.55%\tTest: 57.19%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2848\t Train: 80.27%\t Valid: 76.14%\tTest: 58.18%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2859\t Train: 79.45%\t Valid: 75.19%\tTest: 56.38%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2832\t Train: 80.04%\t Valid: 75.64%\tTest: 57.75%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2820\t Train: 80.66%\t Valid: 76.44%\tTest: 58.18%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2841\t Train: 79.83%\t Valid: 74.97%\tTest: 56.48%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2806\t Train: 80.62%\t Valid: 76.77%\tTest: 58.54%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2826\t Train: 80.19%\t Valid: 74.98%\tTest: 56.34%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2799\t Train: 80.80%\t Valid: 76.55%\tTest: 58.99%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2817\t Train: 81.38%\t Valid: 76.84%\tTest: 58.83%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2789\t Train: 81.07%\t Valid: 75.95%\tTest: 57.29%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2817\t Train: 80.67%\t Valid: 76.21%\tTest: 57.73%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2792\t Train: 81.55%\t Valid: 77.52%\tTest: 62.48%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2777\t Train: 81.75%\t Valid: 77.23%\tTest: 60.43%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2764\t Train: 81.89%\t Valid: 77.01%\tTest: 59.15%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2751\t Train: 81.93%\t Valid: 76.92%\tTest: 59.32%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2746\t Train: 81.91%\t Valid: 76.98%\tTest: 59.49%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2774\t Train: 81.12%\t Valid: 75.90%\tTest: 58.44%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2756\t Train: 81.67%\t Valid: 76.93%\tTest: 59.61%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2736\t Train: 82.37%\t Valid: 77.66%\tTest: 62.05%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2746\t Train: 82.42%\t Valid: 77.59%\tTest: 60.98%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2726\t Train: 82.33%\t Valid: 77.38%\tTest: 60.81%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2722\t Train: 82.42%\t Valid: 77.39%\tTest: 60.74%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2731\t Train: 82.22%\t Valid: 77.37%\tTest: 60.43%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2742\t Train: 82.50%\t Valid: 77.63%\tTest: 61.54%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2735\t Train: 82.75%\t Valid: 77.40%\tTest: 62.46%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2707\t Train: 82.66%\t Valid: 76.87%\tTest: 60.11%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2700\t Train: 82.88%\t Valid: 76.88%\tTest: 60.17%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2698\t Train: 82.84%\t Valid: 76.88%\tTest: 60.11%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2693\t Train: 83.15%\t Valid: 77.24%\tTest: 60.90%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2682\t Train: 83.14%\t Valid: 77.18%\tTest: 60.67%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2681\t Train: 83.28%\t Valid: 77.45%\tTest: 61.06%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2680\t Train: 83.15%\t Valid: 77.05%\tTest: 60.75%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2688\t Train: 82.93%\t Valid: 77.15%\tTest: 60.84%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2694\t Train: 83.07%\t Valid: 77.61%\tTest: 62.30%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2678\t Train: 83.47%\t Valid: 77.93%\tTest: 63.52%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2666\t Train: 83.55%\t Valid: 77.90%\tTest: 63.14%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2666\t Train: 83.48%\t Valid: 77.48%\tTest: 62.01%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2671\t Train: 83.38%\t Valid: 77.02%\tTest: 61.30%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2656\t Train: 83.73%\t Valid: 77.42%\tTest: 62.00%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2658\t Train: 83.56%\t Valid: 77.60%\tTest: 62.09%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2659\t Train: 83.80%\t Valid: 77.23%\tTest: 62.01%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2650\t Train: 83.63%\t Valid: 77.33%\tTest: 62.08%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2676\t Train: 83.63%\t Valid: 77.36%\tTest: 61.86%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2719\t Train: 83.77%\t Valid: 77.70%\tTest: 61.45%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2655\t Train: 83.87%\t Valid: 77.67%\tTest: 62.02%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2644\t Train: 84.02%\t Valid: 77.53%\tTest: 61.98%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2639\t Train: 84.03%\t Valid: 77.57%\tTest: 61.80%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2639\t Train: 84.13%\t Valid: 77.29%\tTest: 62.15%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2630\t Train: 84.24%\t Valid: 77.37%\tTest: 61.94%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2634\t Train: 84.27%\t Valid: 77.56%\tTest: 61.97%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2633\t Train: 84.38%\t Valid: 77.70%\tTest: 62.19%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2628\t Train: 84.43%\t Valid: 77.46%\tTest: 62.29%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2660\t Train: 84.14%\t Valid: 77.73%\tTest: 64.17%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2626\t Train: 84.28%\t Valid: 77.63%\tTest: 63.19%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2624\t Train: 84.38%\t Valid: 77.39%\tTest: 62.82%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2626\t Train: 84.42%\t Valid: 77.30%\tTest: 62.24%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2617\t Train: 84.44%\t Valid: 77.27%\tTest: 62.30%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2615\t Train: 84.40%\t Valid: 77.44%\tTest: 62.48%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2608\t Train: 84.58%\t Valid: 77.42%\tTest: 62.65%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2610\t Train: 84.65%\t Valid: 77.51%\tTest: 62.56%\n",
      "Seed =  8\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4441\t Train: 37.10%\t Valid: 33.16%\tTest: 35.98%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4065\t Train: 42.66%\t Valid: 33.93%\tTest: 34.78%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3843\t Train: 47.90%\t Valid: 36.95%\tTest: 36.08%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3721\t Train: 56.24%\t Valid: 47.31%\tTest: 42.92%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3615\t Train: 57.68%\t Valid: 48.72%\tTest: 43.07%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3497\t Train: 61.94%\t Valid: 53.36%\tTest: 45.28%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3404\t Train: 63.42%\t Valid: 55.26%\tTest: 45.12%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3358\t Train: 66.07%\t Valid: 60.07%\tTest: 47.16%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3360\t Train: 68.84%\t Valid: 65.43%\tTest: 52.57%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3288\t Train: 67.91%\t Valid: 63.52%\tTest: 50.16%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3273\t Train: 68.44%\t Valid: 64.56%\tTest: 50.30%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3232\t Train: 70.75%\t Valid: 68.18%\tTest: 54.06%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3215\t Train: 71.00%\t Valid: 68.53%\tTest: 53.97%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3197\t Train: 70.53%\t Valid: 67.80%\tTest: 52.89%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3197\t Train: 69.86%\t Valid: 66.80%\tTest: 51.88%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3168\t Train: 71.11%\t Valid: 68.82%\tTest: 54.12%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3148\t Train: 71.98%\t Valid: 70.25%\tTest: 55.43%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3129\t Train: 73.09%\t Valid: 71.70%\tTest: 56.81%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3199\t Train: 73.89%\t Valid: 72.76%\tTest: 58.60%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3099\t Train: 73.79%\t Valid: 72.50%\tTest: 58.06%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3080\t Train: 74.02%\t Valid: 72.55%\tTest: 57.85%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3063\t Train: 74.46%\t Valid: 72.91%\tTest: 58.04%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3058\t Train: 73.43%\t Valid: 71.59%\tTest: 57.97%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3045\t Train: 75.25%\t Valid: 73.67%\tTest: 59.43%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3023\t Train: 75.31%\t Valid: 73.23%\tTest: 58.36%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3009\t Train: 75.31%\t Valid: 72.91%\tTest: 57.71%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3015\t Train: 76.40%\t Valid: 74.14%\tTest: 59.10%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2989\t Train: 76.38%\t Valid: 73.94%\tTest: 58.64%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2976\t Train: 77.24%\t Valid: 75.14%\tTest: 60.51%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2958\t Train: 77.23%\t Valid: 74.88%\tTest: 59.38%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3082\t Train: 75.22%\t Valid: 72.53%\tTest: 56.68%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3001\t Train: 76.66%\t Valid: 74.01%\tTest: 57.07%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2937\t Train: 77.40%\t Valid: 74.63%\tTest: 59.00%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2929\t Train: 78.14%\t Valid: 75.29%\tTest: 58.98%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2913\t Train: 78.25%\t Valid: 75.26%\tTest: 58.76%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2903\t Train: 78.33%\t Valid: 75.06%\tTest: 57.93%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2891\t Train: 78.99%\t Valid: 75.70%\tTest: 58.69%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2891\t Train: 78.92%\t Valid: 75.60%\tTest: 59.07%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2876\t Train: 78.92%\t Valid: 75.22%\tTest: 58.22%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2866\t Train: 79.00%\t Valid: 75.47%\tTest: 58.49%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2863\t Train: 79.58%\t Valid: 75.91%\tTest: 58.41%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2854\t Train: 79.66%\t Valid: 76.47%\tTest: 59.85%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2904\t Train: 78.62%\t Valid: 74.57%\tTest: 57.73%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2853\t Train: 80.21%\t Valid: 76.88%\tTest: 59.70%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2826\t Train: 80.26%\t Valid: 76.25%\tTest: 58.69%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2831\t Train: 80.81%\t Valid: 76.41%\tTest: 58.85%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2815\t Train: 80.96%\t Valid: 76.92%\tTest: 60.49%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2813\t Train: 80.88%\t Valid: 76.59%\tTest: 59.39%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2794\t Train: 80.84%\t Valid: 76.68%\tTest: 59.59%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2786\t Train: 81.21%\t Valid: 76.81%\tTest: 59.64%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2814\t Train: 80.41%\t Valid: 75.21%\tTest: 56.64%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2792\t Train: 81.10%\t Valid: 77.24%\tTest: 60.97%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2811\t Train: 81.62%\t Valid: 77.20%\tTest: 60.56%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2844\t Train: 80.55%\t Valid: 77.19%\tTest: 61.84%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2792\t Train: 81.39%\t Valid: 76.27%\tTest: 60.92%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2772\t Train: 81.51%\t Valid: 77.13%\tTest: 60.00%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2757\t Train: 81.72%\t Valid: 76.34%\tTest: 59.97%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2748\t Train: 81.87%\t Valid: 76.84%\tTest: 59.76%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2741\t Train: 82.10%\t Valid: 77.34%\tTest: 60.67%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2732\t Train: 82.12%\t Valid: 77.08%\tTest: 59.60%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2737\t Train: 82.28%\t Valid: 77.27%\tTest: 60.40%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2745\t Train: 82.42%\t Valid: 77.33%\tTest: 60.42%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2736\t Train: 82.57%\t Valid: 77.72%\tTest: 61.63%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2725\t Train: 82.59%\t Valid: 77.35%\tTest: 60.52%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2717\t Train: 82.68%\t Valid: 77.31%\tTest: 60.77%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2708\t Train: 82.59%\t Valid: 77.25%\tTest: 60.42%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2713\t Train: 82.42%\t Valid: 76.71%\tTest: 59.32%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2722\t Train: 82.92%\t Valid: 77.60%\tTest: 60.91%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2713\t Train: 82.81%\t Valid: 78.00%\tTest: 62.97%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2698\t Train: 82.67%\t Valid: 77.26%\tTest: 61.26%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2693\t Train: 82.95%\t Valid: 77.79%\tTest: 61.65%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2691\t Train: 83.11%\t Valid: 77.48%\tTest: 60.92%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2693\t Train: 82.85%\t Valid: 77.01%\tTest: 60.31%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2699\t Train: 82.84%\t Valid: 77.73%\tTest: 61.45%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2688\t Train: 83.19%\t Valid: 77.07%\tTest: 61.02%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2691\t Train: 83.42%\t Valid: 77.81%\tTest: 62.50%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2672\t Train: 83.41%\t Valid: 77.64%\tTest: 61.65%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2668\t Train: 83.46%\t Valid: 77.84%\tTest: 61.93%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2664\t Train: 83.48%\t Valid: 77.96%\tTest: 61.96%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2660\t Train: 83.52%\t Valid: 78.02%\tTest: 61.63%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2672\t Train: 83.70%\t Valid: 77.67%\tTest: 61.29%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2657\t Train: 83.70%\t Valid: 78.07%\tTest: 62.04%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2658\t Train: 83.70%\t Valid: 77.35%\tTest: 60.95%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2650\t Train: 83.76%\t Valid: 77.77%\tTest: 60.97%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2655\t Train: 83.66%\t Valid: 78.25%\tTest: 62.67%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2689\t Train: 83.31%\t Valid: 76.35%\tTest: 60.44%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2651\t Train: 83.75%\t Valid: 78.21%\tTest: 61.96%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2657\t Train: 83.96%\t Valid: 77.73%\tTest: 61.05%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2641\t Train: 83.86%\t Valid: 78.08%\tTest: 62.33%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2632\t Train: 84.05%\t Valid: 77.85%\tTest: 60.96%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2635\t Train: 83.99%\t Valid: 77.77%\tTest: 60.97%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2629\t Train: 84.11%\t Valid: 78.20%\tTest: 62.18%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2638\t Train: 84.11%\t Valid: 77.80%\tTest: 61.79%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2647\t Train: 84.18%\t Valid: 78.30%\tTest: 62.61%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2629\t Train: 84.26%\t Valid: 78.05%\tTest: 61.61%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2630\t Train: 84.34%\t Valid: 78.26%\tTest: 61.60%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2623\t Train: 84.32%\t Valid: 78.19%\tTest: 61.76%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2622\t Train: 84.28%\t Valid: 77.89%\tTest: 61.14%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2625\t Train: 84.26%\t Valid: 77.89%\tTest: 60.56%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2616\t Train: 84.48%\t Valid: 78.42%\tTest: 61.80%\n",
      "Seed =  9\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4447\t Train: 37.39%\t Valid: 33.51%\tTest: 36.28%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4058\t Train: 42.72%\t Valid: 33.78%\tTest: 34.70%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3848\t Train: 48.06%\t Valid: 36.88%\tTest: 35.98%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3730\t Train: 55.87%\t Valid: 46.72%\tTest: 42.51%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3633\t Train: 57.88%\t Valid: 49.22%\tTest: 43.46%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3523\t Train: 60.31%\t Valid: 51.72%\tTest: 44.23%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3399\t Train: 64.53%\t Valid: 57.63%\tTest: 46.83%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3369\t Train: 65.34%\t Valid: 59.53%\tTest: 49.28%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3318\t Train: 67.28%\t Valid: 62.83%\tTest: 50.65%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3272\t Train: 68.71%\t Valid: 64.76%\tTest: 50.81%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3689\t Train: 64.57%\t Valid: 59.38%\tTest: 48.40%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3319\t Train: 65.85%\t Valid: 60.21%\tTest: 51.19%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3261\t Train: 70.35%\t Valid: 67.18%\tTest: 56.22%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3220\t Train: 70.18%\t Valid: 66.94%\tTest: 55.63%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3197\t Train: 71.30%\t Valid: 69.13%\tTest: 56.08%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3177\t Train: 71.38%\t Valid: 69.06%\tTest: 55.34%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3157\t Train: 72.03%\t Valid: 69.91%\tTest: 55.79%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3139\t Train: 72.45%\t Valid: 70.38%\tTest: 55.77%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3122\t Train: 72.80%\t Valid: 70.95%\tTest: 55.90%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3102\t Train: 73.17%\t Valid: 71.35%\tTest: 55.86%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3101\t Train: 72.48%\t Valid: 70.03%\tTest: 54.41%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3078\t Train: 73.96%\t Valid: 71.98%\tTest: 56.54%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3056\t Train: 74.95%\t Valid: 73.15%\tTest: 58.04%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3046\t Train: 75.55%\t Valid: 73.79%\tTest: 58.75%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3025\t Train: 75.16%\t Valid: 72.84%\tTest: 57.57%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3054\t Train: 76.02%\t Valid: 74.08%\tTest: 60.00%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3006\t Train: 75.88%\t Valid: 72.73%\tTest: 56.27%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2989\t Train: 76.20%\t Valid: 74.06%\tTest: 59.12%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2992\t Train: 75.76%\t Valid: 72.52%\tTest: 56.63%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2984\t Train: 76.32%\t Valid: 72.57%\tTest: 56.05%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2955\t Train: 77.39%\t Valid: 74.64%\tTest: 58.49%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2940\t Train: 77.57%\t Valid: 74.39%\tTest: 57.96%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2937\t Train: 78.36%\t Valid: 75.47%\tTest: 59.42%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2957\t Train: 78.18%\t Valid: 74.76%\tTest: 57.24%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2922\t Train: 78.54%\t Valid: 75.09%\tTest: 58.16%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2902\t Train: 78.52%\t Valid: 74.73%\tTest: 57.43%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2891\t Train: 78.93%\t Valid: 75.49%\tTest: 58.66%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2904\t Train: 79.17%\t Valid: 75.79%\tTest: 59.48%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2875\t Train: 79.00%\t Valid: 74.81%\tTest: 57.33%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2891\t Train: 78.92%\t Valid: 75.83%\tTest: 59.31%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2879\t Train: 79.57%\t Valid: 75.82%\tTest: 58.13%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2867\t Train: 79.58%\t Valid: 75.79%\tTest: 58.79%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2843\t Train: 80.05%\t Valid: 76.11%\tTest: 58.48%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2862\t Train: 79.92%\t Valid: 75.98%\tTest: 58.02%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2833\t Train: 80.43%\t Valid: 76.33%\tTest: 58.81%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2825\t Train: 80.63%\t Valid: 76.58%\tTest: 59.37%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2816\t Train: 80.55%\t Valid: 76.59%\tTest: 59.48%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2855\t Train: 80.35%\t Valid: 75.70%\tTest: 58.98%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2839\t Train: 80.50%\t Valid: 75.81%\tTest: 57.66%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2807\t Train: 80.88%\t Valid: 76.52%\tTest: 59.96%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2793\t Train: 81.13%\t Valid: 76.55%\tTest: 58.79%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2785\t Train: 81.13%\t Valid: 76.96%\tTest: 59.39%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2792\t Train: 81.50%\t Valid: 77.11%\tTest: 59.45%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2772\t Train: 81.52%\t Valid: 77.08%\tTest: 59.23%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2764\t Train: 81.41%\t Valid: 76.71%\tTest: 58.40%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2823\t Train: 81.88%\t Valid: 77.37%\tTest: 59.89%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2784\t Train: 81.86%\t Valid: 77.09%\tTest: 59.44%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2758\t Train: 81.83%\t Valid: 77.42%\tTest: 60.31%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2748\t Train: 81.70%\t Valid: 76.56%\tTest: 58.21%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2743\t Train: 81.93%\t Valid: 77.32%\tTest: 59.48%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2741\t Train: 81.89%\t Valid: 77.41%\tTest: 59.70%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2733\t Train: 82.08%\t Valid: 77.23%\tTest: 59.42%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2734\t Train: 82.02%\t Valid: 77.37%\tTest: 59.49%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2740\t Train: 82.33%\t Valid: 77.22%\tTest: 59.36%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2762\t Train: 82.65%\t Valid: 77.94%\tTest: 60.93%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2733\t Train: 82.01%\t Valid: 76.37%\tTest: 58.90%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2715\t Train: 82.54%\t Valid: 77.36%\tTest: 60.84%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2718\t Train: 82.55%\t Valid: 77.49%\tTest: 60.09%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2714\t Train: 82.55%\t Valid: 77.02%\tTest: 59.12%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2707\t Train: 82.93%\t Valid: 77.81%\tTest: 61.04%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2706\t Train: 82.94%\t Valid: 78.14%\tTest: 61.38%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2689\t Train: 82.78%\t Valid: 77.95%\tTest: 60.81%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2689\t Train: 82.96%\t Valid: 77.72%\tTest: 60.51%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2695\t Train: 82.95%\t Valid: 77.93%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2682\t Train: 83.09%\t Valid: 78.06%\tTest: 60.81%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2686\t Train: 82.80%\t Valid: 77.33%\tTest: 59.47%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2695\t Train: 83.23%\t Valid: 78.02%\tTest: 60.39%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2701\t Train: 83.43%\t Valid: 78.49%\tTest: 62.20%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2688\t Train: 83.11%\t Valid: 78.05%\tTest: 60.95%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2678\t Train: 83.43%\t Valid: 78.20%\tTest: 60.87%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2674\t Train: 83.60%\t Valid: 77.80%\tTest: 60.56%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2664\t Train: 83.52%\t Valid: 78.28%\tTest: 61.11%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2669\t Train: 83.54%\t Valid: 78.05%\tTest: 61.43%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2657\t Train: 83.69%\t Valid: 78.39%\tTest: 61.61%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2651\t Train: 83.69%\t Valid: 78.18%\tTest: 61.56%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2655\t Train: 83.68%\t Valid: 78.42%\tTest: 61.54%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2652\t Train: 83.80%\t Valid: 78.50%\tTest: 61.91%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2651\t Train: 83.85%\t Valid: 78.24%\tTest: 60.91%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2648\t Train: 83.77%\t Valid: 78.20%\tTest: 61.49%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2639\t Train: 84.00%\t Valid: 78.26%\tTest: 61.77%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2704\t Train: 83.96%\t Valid: 78.29%\tTest: 62.86%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2678\t Train: 83.58%\t Valid: 78.38%\tTest: 63.61%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2651\t Train: 84.03%\t Valid: 78.01%\tTest: 61.55%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2634\t Train: 84.06%\t Valid: 78.23%\tTest: 61.45%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2634\t Train: 84.02%\t Valid: 78.55%\tTest: 61.67%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2643\t Train: 84.27%\t Valid: 78.55%\tTest: 62.51%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2634\t Train: 84.10%\t Valid: 78.30%\tTest: 61.59%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2650\t Train: 84.08%\t Valid: 78.04%\tTest: 61.37%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2634\t Train: 84.40%\t Valid: 78.67%\tTest: 63.13%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2621\t Train: 84.32%\t Valid: 78.54%\tTest: 62.36%\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute GCN normalization.\n",
    "adj_t = data.adj_t.set_diag()\n",
    "deg = adj_t.sum(dim=1).to(torch.float)\n",
    "deg_inv_sqrt = deg.pow(-0.5)\n",
    "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
    "data.adj_t = adj_t\n",
    "    \n",
    "evaluator = Evaluator(name='ogbn-proteins')\n",
    "logger = Logger(args.runs, args)\n",
    "best_test_score = 0\n",
    "\n",
    "\n",
    "for seed in range(10):\n",
    "    print(\"Seed = \",seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    model = GCN(data.num_features, args.hidden_channels, 112, args.num_layers, args.dropout).to(device)\n",
    "    model.reset_parameters()\n",
    "    for run in range(args.runs):        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "\n",
    "            if epoch % args.eval_steps == 0:\n",
    "                result = test(model, data, split_idx, evaluator)\n",
    "                logger.add_result(run, result)\n",
    "\n",
    "                if epoch % args.log_steps == 0:                \n",
    "                    train_rocauc, valid_rocauc, test_rocauc = result\n",
    "                    print(f'Run: {run + 1:02d}\\t '\n",
    "                          f'Epoch: {epoch:02d}\\t '\n",
    "                          f'Loss: {loss:.4f}\\t '\n",
    "                          f'Train: {100 * train_rocauc:.2f}%\\t '\n",
    "                          f'Valid: {100 * valid_rocauc:.2f}%\\t'\n",
    "                          f'Test: {100 * test_rocauc:.2f}%')\n",
    "                    if(test_rocauc > best_test_score):\n",
    "                        best_test_score = test_rocauc\n",
    "                        save_path = \"gcn.pth\"\n",
    "                        torch.save(model, save_path)\n",
    "                        print(\"Model saved.\")\n",
    "        # logger.print_statistics(run)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.10200963597043\n"
     ]
    }
   ],
   "source": [
    "print(best_test_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 hours\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, optimizer\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
