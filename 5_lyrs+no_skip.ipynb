{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "class args:\n",
    "    num_layers = 5\n",
    "    device = 'cuda:0'\n",
    "    log_steps = 1\n",
    "    hidden_channels = 64*4\n",
    "    dropout = 0.2\n",
    "    lr = 0.01\n",
    "    epochs = 1000\n",
    "    eval_steps = 10\n",
    "    runs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import humanize\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\r\n",
      "  Downloading ogb-1.2.1-py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 891 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.1)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.1)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.0.3)\r\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.18.5)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Installing collected packages: ogb\r\n",
      "Successfully installed ogb-1.2.1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-scatter==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 528 kB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\r\n",
      "Successfully installed torch-scatter-2.0.5\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-sparse==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.6 MB 3.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==latest+cu101) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==latest+cu101) (1.18.5)\r\n",
      "Installing collected packages: torch-sparse\r\n",
      "Successfully installed torch-sparse-0.6.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-cluster==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.3 MB 4.2 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-cluster\r\n",
      "Successfully installed torch-cluster-1.5.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-spline-conv==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 441 kB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\r\n",
      "Successfully installed torch-spline-conv-1.2.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-1.6.0.tar.gz (172 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 172 kB 2.9 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.5.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.18.5)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (4.45.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.4.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.23.1)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.48.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.23.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.0.3)\r\n",
      "Collecting rdflib\r\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 231 kB 9.6 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.10.0)\r\n",
      "Collecting googledrivedownloader\r\n",
      "  Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB)\r\n",
      "Collecting ase\r\n",
      "  Downloading ase-3.19.2-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 13.0 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.11.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torch-geometric) (0.18.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->torch-geometric) (4.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (0.14.1)\r\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (0.31.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (1.24.3)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2.8.1)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (2.4.7)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (1.14.0)\r\n",
      "Collecting isodate\r\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.7 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ase->torch-geometric) (3.2.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric) (1.1.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (1.2.0)\r\n",
      "Building wheels for collected packages: torch-geometric\r\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-1.6.0-py3-none-any.whl size=296336 sha256=c56410e20c825875c54fb32b90a9d0a23d91d497d1fe32b9666f74e4e24cfc71\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/69/d6/b8ed45222466048e1efc27af604aded6825217f0caa6dff569\r\n",
      "Successfully built torch-geometric\r\n",
      "Installing collected packages: isodate, rdflib, googledrivedownloader, ase, torch-geometric\r\n",
      "Successfully installed ase-3.19.2 googledrivedownloader-0.4 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# install Open Graph Benchmark\n",
    "! pip install ogb\n",
    "\n",
    "# install PyTorch Geometric\n",
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR,CosineAnnealingLR\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://snap.stanford.edu/ogb/data/nodeproppred/proteinfunc.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:07<00:00, 28.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/proteinfunc.zip\n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 382.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n",
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = f'{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-proteins',\n",
    "                                 transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "\n",
    "# Move edge features to node features.\n",
    "data.x = data.adj_t.mean(dim=1)\n",
    "data.adj_t.set_value_(None)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train'].to(device)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 minutes\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            GCNConv(in_channels, hidden_channels, normalize=False))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, normalize=False))\n",
    "        self.convs.append(\n",
    "            GCNConv(hidden_channels, out_channels, normalize=False))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideResGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
    "        super(WideResGCN, self).__init__()\n",
    "        self.init = GCNConv(in_channels, in_channels, normalize=True)\n",
    "        self.left = GCN(in_channels, hidden_channels, out_channels, num_layers, dropout)\n",
    "        self.right = SAGE(in_channels, hidden_channels, out_channels, num_layers, dropout)\n",
    "        self.final = GCNConv(out_channels*2, out_channels, normalize=True)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.init.reset_parameters()\n",
    "        self.left.reset_parameters()\n",
    "        self.right.reset_parameters()\n",
    "        self.final.reset_parameters()\n",
    "        \n",
    "    def forward(self, x, adj_t):\n",
    "        x = self.init(x, adj_t)        \n",
    "        x_l = self.left(x, adj_t)\n",
    "        x_r = self.right(x, adj_t)\n",
    "        x = torch.cat([x_l, x_r],1)\n",
    "        x = self.final(x, adj_t)\n",
    "        return x                "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = criterion(out, data.y[train_idx].to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(data.x, data.adj_t)\n",
    "\n",
    "    train_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['rocauc']\n",
    "    valid_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['rocauc']\n",
    "    test_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['rocauc']\n",
    "\n",
    "    return train_rocauc, valid_rocauc, test_rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed =  0\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4590\t Train: 36.74%\t Valid: 35.20%\tTest: 38.55%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4214\t Train: 37.25%\t Valid: 33.70%\tTest: 36.68%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4039\t Train: 39.79%\t Valid: 34.34%\tTest: 36.73%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3932\t Train: 43.54%\t Valid: 34.65%\tTest: 36.76%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3850\t Train: 49.30%\t Valid: 35.90%\tTest: 37.18%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3788\t Train: 50.14%\t Valid: 35.87%\tTest: 36.79%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3717\t Train: 52.46%\t Valid: 37.97%\tTest: 37.59%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3667\t Train: 53.82%\t Valid: 40.30%\tTest: 38.38%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3579\t Train: 56.89%\t Valid: 45.64%\tTest: 41.10%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3541\t Train: 58.07%\t Valid: 49.22%\tTest: 42.29%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3498\t Train: 58.94%\t Valid: 52.06%\tTest: 43.24%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3461\t Train: 61.43%\t Valid: 56.03%\tTest: 46.07%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3487\t Train: 61.10%\t Valid: 56.14%\tTest: 45.84%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3412\t Train: 63.48%\t Valid: 59.35%\tTest: 48.60%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3400\t Train: 64.41%\t Valid: 60.73%\tTest: 49.80%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3380\t Train: 64.45%\t Valid: 60.57%\tTest: 49.60%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3839\t Train: 63.44%\t Valid: 59.03%\tTest: 49.30%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3461\t Train: 56.68%\t Valid: 46.16%\tTest: 41.25%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3412\t Train: 62.86%\t Valid: 53.21%\tTest: 46.51%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3359\t Train: 63.89%\t Valid: 56.93%\tTest: 47.80%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3331\t Train: 65.46%\t Valid: 60.99%\tTest: 50.63%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3314\t Train: 65.72%\t Valid: 61.52%\tTest: 51.02%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3284\t Train: 67.50%\t Valid: 63.61%\tTest: 52.80%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3243\t Train: 67.86%\t Valid: 65.64%\tTest: 55.22%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3239\t Train: 68.75%\t Valid: 65.83%\tTest: 54.70%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3203\t Train: 69.55%\t Valid: 67.83%\tTest: 56.99%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3182\t Train: 70.45%\t Valid: 68.90%\tTest: 58.86%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3164\t Train: 70.58%\t Valid: 69.71%\tTest: 60.14%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3153\t Train: 71.56%\t Valid: 70.61%\tTest: 61.24%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3135\t Train: 71.98%\t Valid: 71.17%\tTest: 62.02%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3227\t Train: 72.50%\t Valid: 71.12%\tTest: 61.72%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3134\t Train: 71.99%\t Valid: 70.68%\tTest: 61.18%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3114\t Train: 72.77%\t Valid: 70.72%\tTest: 62.05%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3095\t Train: 73.39%\t Valid: 72.11%\tTest: 63.68%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3085\t Train: 73.26%\t Valid: 72.05%\tTest: 63.91%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3070\t Train: 74.07%\t Valid: 72.61%\tTest: 64.15%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3089\t Train: 73.91%\t Valid: 71.61%\tTest: 63.34%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3073\t Train: 74.19%\t Valid: 71.68%\tTest: 63.52%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3054\t Train: 74.40%\t Valid: 72.83%\tTest: 64.73%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3044\t Train: 75.29%\t Valid: 73.59%\tTest: 65.28%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3037\t Train: 75.43%\t Valid: 73.83%\tTest: 65.51%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3018\t Train: 75.42%\t Valid: 73.01%\tTest: 64.81%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3012\t Train: 75.60%\t Valid: 73.65%\tTest: 65.68%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3000\t Train: 76.40%\t Valid: 74.37%\tTest: 66.71%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3000\t Train: 76.34%\t Valid: 74.58%\tTest: 66.76%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3038\t Train: 76.85%\t Valid: 74.84%\tTest: 67.53%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2986\t Train: 77.11%\t Valid: 75.28%\tTest: 67.77%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2970\t Train: 77.22%\t Valid: 75.49%\tTest: 68.32%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2998\t Train: 76.77%\t Valid: 74.45%\tTest: 66.82%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2981\t Train: 77.05%\t Valid: 74.97%\tTest: 68.34%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2947\t Train: 77.94%\t Valid: 75.33%\tTest: 68.82%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2925\t Train: 78.10%\t Valid: 76.26%\tTest: 70.02%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2894\t Train: 78.90%\t Valid: 76.64%\tTest: 70.26%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 540\t Loss: 0.3021\t Train: 76.02%\t Valid: 75.58%\tTest: 70.42%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2965\t Train: 77.67%\t Valid: 76.72%\tTest: 69.58%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2920\t Train: 78.39%\t Valid: 76.32%\tTest: 69.73%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2887\t Train: 79.16%\t Valid: 76.97%\tTest: 70.75%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2933\t Train: 78.53%\t Valid: 75.74%\tTest: 69.63%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2894\t Train: 79.91%\t Valid: 78.08%\tTest: 71.82%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2854\t Train: 80.10%\t Valid: 77.63%\tTest: 71.55%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2826\t Train: 80.28%\t Valid: 77.63%\tTest: 72.18%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2915\t Train: 79.08%\t Valid: 77.55%\tTest: 70.42%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2872\t Train: 79.64%\t Valid: 77.93%\tTest: 71.85%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2841\t Train: 80.36%\t Valid: 78.01%\tTest: 71.99%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2818\t Train: 80.59%\t Valid: 78.04%\tTest: 72.12%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2818\t Train: 80.65%\t Valid: 78.52%\tTest: 72.85%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2816\t Train: 80.26%\t Valid: 78.38%\tTest: 72.95%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2817\t Train: 81.37%\t Valid: 78.38%\tTest: 72.62%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2801\t Train: 81.35%\t Valid: 78.42%\tTest: 72.65%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2769\t Train: 81.74%\t Valid: 78.91%\tTest: 73.25%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2756\t Train: 81.63%\t Valid: 78.84%\tTest: 73.69%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2759\t Train: 81.93%\t Valid: 78.85%\tTest: 73.34%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2765\t Train: 81.95%\t Valid: 77.59%\tTest: 71.63%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2751\t Train: 81.67%\t Valid: 78.84%\tTest: 73.19%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2794\t Train: 82.42%\t Valid: 78.63%\tTest: 72.54%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2746\t Train: 82.52%\t Valid: 78.65%\tTest: 72.90%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2734\t Train: 82.05%\t Valid: 78.44%\tTest: 72.57%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2728\t Train: 82.26%\t Valid: 78.31%\tTest: 71.94%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2719\t Train: 82.68%\t Valid: 78.39%\tTest: 71.88%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2708\t Train: 82.91%\t Valid: 77.53%\tTest: 70.81%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2718\t Train: 82.64%\t Valid: 78.62%\tTest: 72.60%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2810\t Train: 81.48%\t Valid: 78.65%\tTest: 73.64%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2729\t Train: 82.52%\t Valid: 78.52%\tTest: 72.69%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2701\t Train: 83.10%\t Valid: 78.18%\tTest: 71.28%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2682\t Train: 83.35%\t Valid: 78.61%\tTest: 71.39%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2797\t Train: 81.90%\t Valid: 76.77%\tTest: 69.98%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2729\t Train: 83.19%\t Valid: 78.85%\tTest: 71.14%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2692\t Train: 83.07%\t Valid: 79.23%\tTest: 72.81%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2673\t Train: 83.49%\t Valid: 78.52%\tTest: 71.50%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2675\t Train: 83.59%\t Valid: 76.93%\tTest: 68.18%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2686\t Train: 82.93%\t Valid: 79.27%\tTest: 73.09%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2687\t Train: 83.66%\t Valid: 79.11%\tTest: 69.96%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2652\t Train: 83.70%\t Valid: 77.35%\tTest: 68.67%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2651\t Train: 83.91%\t Valid: 77.44%\tTest: 67.34%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2636\t Train: 83.80%\t Valid: 78.19%\tTest: 67.58%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2641\t Train: 83.85%\t Valid: 76.67%\tTest: 65.49%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2633\t Train: 83.70%\t Valid: 79.09%\tTest: 68.17%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2665\t Train: 84.15%\t Valid: 78.43%\tTest: 68.42%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2704\t Train: 83.72%\t Valid: 77.66%\tTest: 68.04%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2644\t Train: 83.86%\t Valid: 78.84%\tTest: 69.06%\n",
      "Seed =  1\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4765\t Train: 37.35%\t Valid: 35.35%\tTest: 38.73%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4253\t Train: 36.62%\t Valid: 33.57%\tTest: 36.65%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4059\t Train: 37.83%\t Valid: 33.52%\tTest: 36.22%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3968\t Train: 40.50%\t Valid: 33.90%\tTest: 36.29%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3874\t Train: 46.32%\t Valid: 35.34%\tTest: 37.19%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3810\t Train: 50.07%\t Valid: 36.53%\tTest: 37.56%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3737\t Train: 51.49%\t Valid: 38.71%\tTest: 38.18%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3676\t Train: 53.02%\t Valid: 43.09%\tTest: 39.59%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3625\t Train: 55.79%\t Valid: 46.84%\tTest: 41.37%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3545\t Train: 56.79%\t Valid: 49.45%\tTest: 42.49%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3490\t Train: 60.17%\t Valid: 54.70%\tTest: 45.16%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3460\t Train: 62.31%\t Valid: 57.55%\tTest: 46.72%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3403\t Train: 63.10%\t Valid: 58.24%\tTest: 46.99%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3408\t Train: 65.19%\t Valid: 59.62%\tTest: 48.54%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3360\t Train: 64.74%\t Valid: 60.10%\tTest: 48.39%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3316\t Train: 65.82%\t Valid: 61.99%\tTest: 49.65%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3295\t Train: 66.04%\t Valid: 62.31%\tTest: 50.26%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3261\t Train: 67.67%\t Valid: 64.95%\tTest: 53.11%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3352\t Train: 70.10%\t Valid: 68.76%\tTest: 57.60%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3257\t Train: 68.56%\t Valid: 65.87%\tTest: 54.07%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3224\t Train: 69.00%\t Valid: 66.55%\tTest: 54.89%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3207\t Train: 70.43%\t Valid: 68.78%\tTest: 57.69%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3185\t Train: 70.22%\t Valid: 68.82%\tTest: 57.88%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3175\t Train: 70.96%\t Valid: 69.76%\tTest: 59.10%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3183\t Train: 70.72%\t Valid: 69.99%\tTest: 59.38%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3170\t Train: 71.46%\t Valid: 69.53%\tTest: 59.19%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3149\t Train: 72.02%\t Valid: 70.28%\tTest: 59.99%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3130\t Train: 72.23%\t Valid: 70.86%\tTest: 60.66%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3116\t Train: 72.82%\t Valid: 71.03%\tTest: 60.90%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3138\t Train: 72.80%\t Valid: 70.45%\tTest: 61.77%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3124\t Train: 72.98%\t Valid: 71.31%\tTest: 61.20%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3101\t Train: 72.89%\t Valid: 71.35%\tTest: 61.20%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3089\t Train: 73.57%\t Valid: 71.88%\tTest: 62.30%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3078\t Train: 73.76%\t Valid: 71.49%\tTest: 62.68%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3071\t Train: 74.29%\t Valid: 71.82%\tTest: 62.95%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3065\t Train: 74.15%\t Valid: 71.64%\tTest: 62.49%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3160\t Train: 74.23%\t Valid: 71.73%\tTest: 64.09%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3057\t Train: 74.19%\t Valid: 71.28%\tTest: 61.99%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3050\t Train: 74.68%\t Valid: 72.09%\tTest: 63.52%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3041\t Train: 75.31%\t Valid: 72.75%\tTest: 64.35%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3030\t Train: 75.31%\t Valid: 72.66%\tTest: 64.19%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3041\t Train: 74.18%\t Valid: 71.67%\tTest: 63.16%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3038\t Train: 75.92%\t Valid: 72.85%\tTest: 64.74%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3024\t Train: 75.91%\t Valid: 73.02%\tTest: 65.50%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3009\t Train: 75.74%\t Valid: 72.35%\tTest: 64.10%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2992\t Train: 76.07%\t Valid: 72.59%\tTest: 64.36%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.3000\t Train: 76.85%\t Valid: 73.20%\tTest: 66.06%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2974\t Train: 76.69%\t Valid: 72.59%\tTest: 64.97%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2994\t Train: 76.12%\t Valid: 73.59%\tTest: 65.10%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2987\t Train: 77.01%\t Valid: 73.79%\tTest: 65.88%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2962\t Train: 77.66%\t Valid: 74.33%\tTest: 67.33%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2978\t Train: 77.87%\t Valid: 74.38%\tTest: 67.41%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2937\t Train: 77.63%\t Valid: 73.93%\tTest: 67.40%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.3001\t Train: 77.92%\t Valid: 74.93%\tTest: 68.51%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2952\t Train: 77.85%\t Valid: 74.22%\tTest: 67.36%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2934\t Train: 77.97%\t Valid: 74.45%\tTest: 68.29%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2904\t Train: 78.38%\t Valid: 75.24%\tTest: 68.65%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.3080\t Train: 78.19%\t Valid: 75.34%\tTest: 69.05%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2941\t Train: 77.99%\t Valid: 76.29%\tTest: 69.86%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2911\t Train: 78.82%\t Valid: 75.82%\tTest: 69.45%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2892\t Train: 79.17%\t Valid: 76.58%\tTest: 69.99%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2885\t Train: 78.71%\t Valid: 75.61%\tTest: 68.84%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2872\t Train: 79.97%\t Valid: 76.87%\tTest: 70.64%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2855\t Train: 79.91%\t Valid: 77.18%\tTest: 70.57%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2840\t Train: 80.38%\t Valid: 77.47%\tTest: 71.24%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.3069\t Train: 77.84%\t Valid: 74.07%\tTest: 68.68%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2905\t Train: 78.78%\t Valid: 76.51%\tTest: 71.85%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2883\t Train: 79.48%\t Valid: 77.35%\tTest: 71.94%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2849\t Train: 79.95%\t Valid: 77.73%\tTest: 71.77%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2826\t Train: 80.39%\t Valid: 77.27%\tTest: 71.61%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2822\t Train: 80.69%\t Valid: 77.69%\tTest: 71.72%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2808\t Train: 81.09%\t Valid: 78.22%\tTest: 72.37%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2795\t Train: 81.22%\t Valid: 78.10%\tTest: 72.15%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2788\t Train: 81.16%\t Valid: 77.74%\tTest: 72.02%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2815\t Train: 81.05%\t Valid: 77.74%\tTest: 72.40%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2798\t Train: 81.50%\t Valid: 77.68%\tTest: 72.65%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2775\t Train: 81.41%\t Valid: 78.55%\tTest: 72.99%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2774\t Train: 81.64%\t Valid: 78.06%\tTest: 72.55%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2768\t Train: 81.55%\t Valid: 77.29%\tTest: 72.08%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2756\t Train: 81.95%\t Valid: 78.40%\tTest: 72.83%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2820\t Train: 81.10%\t Valid: 77.49%\tTest: 72.40%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2782\t Train: 81.51%\t Valid: 77.84%\tTest: 72.60%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2760\t Train: 82.01%\t Valid: 79.23%\tTest: 73.50%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2774\t Train: 81.20%\t Valid: 77.88%\tTest: 72.82%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2741\t Train: 81.90%\t Valid: 78.49%\tTest: 72.99%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2726\t Train: 82.42%\t Valid: 79.13%\tTest: 73.38%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2716\t Train: 82.65%\t Valid: 79.27%\tTest: 73.74%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2711\t Train: 82.68%\t Valid: 78.89%\tTest: 73.22%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2748\t Train: 82.30%\t Valid: 78.27%\tTest: 72.99%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2709\t Train: 82.60%\t Valid: 78.90%\tTest: 73.06%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2712\t Train: 82.91%\t Valid: 79.30%\tTest: 73.86%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2749\t Train: 82.65%\t Valid: 79.13%\tTest: 73.43%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2709\t Train: 82.78%\t Valid: 79.01%\tTest: 73.96%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2721\t Train: 83.07%\t Valid: 79.53%\tTest: 74.04%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2703\t Train: 82.66%\t Valid: 78.33%\tTest: 73.29%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2688\t Train: 83.01%\t Valid: 79.35%\tTest: 73.60%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2695\t Train: 83.27%\t Valid: 78.89%\tTest: 73.15%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2701\t Train: 83.05%\t Valid: 79.07%\tTest: 73.50%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2681\t Train: 83.49%\t Valid: 79.29%\tTest: 73.64%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2689\t Train: 83.17%\t Valid: 79.02%\tTest: 73.39%\n",
      "Seed =  2\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4460\t Train: 37.80%\t Valid: 35.62%\tTest: 38.82%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4181\t Train: 37.61%\t Valid: 33.75%\tTest: 36.87%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4029\t Train: 40.34%\t Valid: 34.46%\tTest: 36.79%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3917\t Train: 45.26%\t Valid: 35.09%\tTest: 37.13%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3841\t Train: 50.13%\t Valid: 36.75%\tTest: 37.69%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3750\t Train: 51.55%\t Valid: 38.51%\tTest: 38.09%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3670\t Train: 54.30%\t Valid: 41.66%\tTest: 39.50%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3587\t Train: 56.26%\t Valid: 44.39%\tTest: 40.54%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3600\t Train: 56.55%\t Valid: 45.32%\tTest: 41.20%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3500\t Train: 58.28%\t Valid: 47.19%\tTest: 42.00%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3445\t Train: 62.01%\t Valid: 53.22%\tTest: 45.60%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3416\t Train: 61.69%\t Valid: 55.47%\tTest: 45.52%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3386\t Train: 63.14%\t Valid: 57.89%\tTest: 47.45%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3354\t Train: 66.51%\t Valid: 63.27%\tTest: 52.64%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3325\t Train: 65.97%\t Valid: 62.47%\tTest: 51.96%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3311\t Train: 67.38%\t Valid: 64.48%\tTest: 54.02%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3270\t Train: 67.02%\t Valid: 63.81%\tTest: 53.96%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3265\t Train: 67.16%\t Valid: 64.89%\tTest: 54.74%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3241\t Train: 69.07%\t Valid: 67.67%\tTest: 57.73%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3240\t Train: 68.99%\t Valid: 67.76%\tTest: 57.76%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3215\t Train: 69.65%\t Valid: 68.43%\tTest: 59.14%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3206\t Train: 69.46%\t Valid: 68.44%\tTest: 59.26%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3185\t Train: 69.95%\t Valid: 69.19%\tTest: 60.06%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3214\t Train: 69.87%\t Valid: 69.96%\tTest: 60.65%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3179\t Train: 71.16%\t Valid: 70.69%\tTest: 61.28%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3159\t Train: 71.03%\t Valid: 69.65%\tTest: 60.85%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3149\t Train: 71.52%\t Valid: 70.53%\tTest: 62.09%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3196\t Train: 72.01%\t Valid: 71.08%\tTest: 62.69%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3153\t Train: 70.40%\t Valid: 67.62%\tTest: 59.31%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3136\t Train: 71.40%\t Valid: 69.30%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3126\t Train: 72.20%\t Valid: 70.78%\tTest: 62.42%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3115\t Train: 72.22%\t Valid: 71.50%\tTest: 63.12%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3103\t Train: 72.02%\t Valid: 71.51%\tTest: 63.27%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3098\t Train: 73.23%\t Valid: 72.48%\tTest: 64.37%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3087\t Train: 73.66%\t Valid: 72.81%\tTest: 64.64%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3128\t Train: 73.84%\t Valid: 72.87%\tTest: 64.90%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3080\t Train: 73.22%\t Valid: 72.28%\tTest: 63.87%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3064\t Train: 73.93%\t Valid: 72.37%\tTest: 64.50%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3055\t Train: 74.01%\t Valid: 72.80%\tTest: 64.70%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3067\t Train: 74.67%\t Valid: 73.17%\tTest: 65.59%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3059\t Train: 74.13%\t Valid: 72.91%\tTest: 65.14%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3041\t Train: 74.89%\t Valid: 73.12%\tTest: 65.39%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3040\t Train: 74.96%\t Valid: 72.76%\tTest: 64.98%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3093\t Train: 74.33%\t Valid: 71.95%\tTest: 64.34%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3032\t Train: 74.96%\t Valid: 73.06%\tTest: 65.62%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3025\t Train: 75.44%\t Valid: 73.78%\tTest: 66.17%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.3011\t Train: 75.82%\t Valid: 74.15%\tTest: 66.43%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.3012\t Train: 76.01%\t Valid: 74.25%\tTest: 66.60%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2987\t Train: 76.63%\t Valid: 74.16%\tTest: 66.94%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2985\t Train: 76.85%\t Valid: 74.82%\tTest: 67.43%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2995\t Train: 76.67%\t Valid: 74.47%\tTest: 66.98%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2962\t Train: 76.66%\t Valid: 74.40%\tTest: 67.30%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2960\t Train: 77.67%\t Valid: 75.44%\tTest: 68.24%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.3016\t Train: 77.66%\t Valid: 75.54%\tTest: 68.43%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2966\t Train: 77.73%\t Valid: 75.95%\tTest: 69.39%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2986\t Train: 77.77%\t Valid: 76.25%\tTest: 69.52%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2944\t Train: 77.84%\t Valid: 75.15%\tTest: 68.82%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2919\t Train: 78.50%\t Valid: 76.23%\tTest: 69.31%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2912\t Train: 78.51%\t Valid: 76.37%\tTest: 69.63%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2966\t Train: 78.42%\t Valid: 76.54%\tTest: 69.99%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2955\t Train: 77.56%\t Valid: 74.36%\tTest: 69.13%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2915\t Train: 78.55%\t Valid: 76.23%\tTest: 70.33%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2896\t Train: 78.98%\t Valid: 76.89%\tTest: 70.80%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2874\t Train: 79.24%\t Valid: 76.93%\tTest: 70.80%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.3134\t Train: 78.76%\t Valid: 76.78%\tTest: 71.10%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2931\t Train: 79.20%\t Valid: 77.10%\tTest: 71.31%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2881\t Train: 79.38%\t Valid: 77.45%\tTest: 71.50%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2860\t Train: 79.62%\t Valid: 77.45%\tTest: 71.48%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2870\t Train: 79.93%\t Valid: 77.58%\tTest: 72.04%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2841\t Train: 80.34%\t Valid: 77.89%\tTest: 71.91%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2853\t Train: 80.06%\t Valid: 77.98%\tTest: 72.46%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2875\t Train: 79.65%\t Valid: 77.69%\tTest: 72.07%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2837\t Train: 80.46%\t Valid: 77.79%\tTest: 72.18%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2814\t Train: 80.87%\t Valid: 78.30%\tTest: 72.50%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2812\t Train: 80.67%\t Valid: 77.71%\tTest: 72.79%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2822\t Train: 81.13%\t Valid: 78.27%\tTest: 73.39%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2790\t Train: 81.45%\t Valid: 78.34%\tTest: 72.85%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2791\t Train: 81.37%\t Valid: 78.12%\tTest: 72.88%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2876\t Train: 81.03%\t Valid: 78.74%\tTest: 73.44%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2774\t Train: 81.51%\t Valid: 78.59%\tTest: 73.23%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2776\t Train: 81.59%\t Valid: 78.60%\tTest: 73.56%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2794\t Train: 82.04%\t Valid: 78.76%\tTest: 73.23%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2743\t Train: 82.19%\t Valid: 78.98%\tTest: 73.78%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2763\t Train: 81.45%\t Valid: 77.99%\tTest: 73.28%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2758\t Train: 81.40%\t Valid: 78.37%\tTest: 74.34%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2746\t Train: 82.11%\t Valid: 78.59%\tTest: 73.37%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2746\t Train: 82.43%\t Valid: 79.07%\tTest: 74.17%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2782\t Train: 82.28%\t Valid: 78.20%\tTest: 73.20%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2734\t Train: 82.27%\t Valid: 78.75%\tTest: 73.76%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2723\t Train: 82.87%\t Valid: 79.35%\tTest: 74.18%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2698\t Train: 82.77%\t Valid: 79.02%\tTest: 74.29%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2794\t Train: 81.15%\t Valid: 77.56%\tTest: 72.98%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2754\t Train: 82.13%\t Valid: 78.98%\tTest: 73.79%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2721\t Train: 82.43%\t Valid: 78.41%\tTest: 73.39%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2691\t Train: 82.93%\t Valid: 78.87%\tTest: 73.87%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2702\t Train: 81.95%\t Valid: 77.48%\tTest: 72.90%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2700\t Train: 82.89%\t Valid: 79.10%\tTest: 73.94%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2690\t Train: 82.87%\t Valid: 78.97%\tTest: 73.83%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2681\t Train: 83.00%\t Valid: 78.69%\tTest: 73.54%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2697\t Train: 83.30%\t Valid: 78.96%\tTest: 73.56%\n",
      "Seed =  3\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4408\t Train: 37.79%\t Valid: 35.58%\tTest: 38.80%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4157\t Train: 37.57%\t Valid: 33.46%\tTest: 36.65%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4018\t Train: 40.46%\t Valid: 34.13%\tTest: 36.45%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3907\t Train: 45.03%\t Valid: 34.69%\tTest: 36.98%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3846\t Train: 50.39%\t Valid: 36.03%\tTest: 37.23%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3762\t Train: 52.62%\t Valid: 38.61%\tTest: 37.86%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3660\t Train: 55.09%\t Valid: 43.28%\tTest: 40.20%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3576\t Train: 56.91%\t Valid: 46.80%\tTest: 41.75%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3518\t Train: 57.20%\t Valid: 47.87%\tTest: 41.63%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3455\t Train: 59.85%\t Valid: 52.53%\tTest: 44.29%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3406\t Train: 61.84%\t Valid: 57.88%\tTest: 47.27%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3402\t Train: 63.54%\t Valid: 58.13%\tTest: 49.25%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3355\t Train: 65.09%\t Valid: 61.66%\tTest: 50.89%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3332\t Train: 66.27%\t Valid: 63.76%\tTest: 52.33%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3300\t Train: 67.40%\t Valid: 65.39%\tTest: 53.93%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3272\t Train: 68.30%\t Valid: 66.56%\tTest: 55.33%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3256\t Train: 68.83%\t Valid: 67.75%\tTest: 57.06%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3232\t Train: 68.43%\t Valid: 66.93%\tTest: 56.28%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3217\t Train: 68.82%\t Valid: 67.73%\tTest: 57.11%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3218\t Train: 68.47%\t Valid: 66.71%\tTest: 55.96%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3219\t Train: 69.31%\t Valid: 68.26%\tTest: 58.31%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3193\t Train: 70.01%\t Valid: 68.89%\tTest: 58.71%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3180\t Train: 70.06%\t Valid: 69.13%\tTest: 58.64%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3171\t Train: 71.27%\t Valid: 69.89%\tTest: 59.53%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3159\t Train: 71.48%\t Valid: 70.77%\tTest: 60.64%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3152\t Train: 70.93%\t Valid: 69.54%\tTest: 59.11%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3155\t Train: 72.46%\t Valid: 71.43%\tTest: 61.43%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3142\t Train: 71.97%\t Valid: 71.26%\tTest: 61.19%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3122\t Train: 72.12%\t Valid: 71.52%\tTest: 61.51%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3117\t Train: 72.91%\t Valid: 72.44%\tTest: 62.76%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3192\t Train: 71.52%\t Valid: 70.55%\tTest: 60.99%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3135\t Train: 71.98%\t Valid: 70.63%\tTest: 61.37%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3113\t Train: 72.44%\t Valid: 71.38%\tTest: 62.09%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3100\t Train: 73.22%\t Valid: 72.46%\tTest: 62.98%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3087\t Train: 73.45%\t Valid: 72.56%\tTest: 62.93%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3080\t Train: 73.78%\t Valid: 73.07%\tTest: 63.52%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3073\t Train: 73.67%\t Valid: 72.93%\tTest: 63.33%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3063\t Train: 73.87%\t Valid: 73.14%\tTest: 63.63%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3058\t Train: 74.05%\t Valid: 73.48%\tTest: 63.96%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3096\t Train: 74.45%\t Valid: 74.33%\tTest: 65.56%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3053\t Train: 74.61%\t Valid: 73.47%\tTest: 64.27%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3042\t Train: 74.33%\t Valid: 73.78%\tTest: 64.66%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3031\t Train: 74.77%\t Valid: 73.97%\tTest: 64.97%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3029\t Train: 75.27%\t Valid: 74.77%\tTest: 66.03%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3028\t Train: 75.27%\t Valid: 74.75%\tTest: 66.01%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3022\t Train: 75.56%\t Valid: 74.88%\tTest: 66.28%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.3008\t Train: 75.79%\t Valid: 75.01%\tTest: 66.48%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.3014\t Train: 75.39%\t Valid: 74.57%\tTest: 65.78%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.3007\t Train: 75.36%\t Valid: 74.50%\tTest: 65.49%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.3001\t Train: 76.16%\t Valid: 75.06%\tTest: 66.77%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.3014\t Train: 74.96%\t Valid: 73.67%\tTest: 64.47%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2994\t Train: 76.12%\t Valid: 75.23%\tTest: 66.68%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2981\t Train: 76.22%\t Valid: 75.13%\tTest: 66.65%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2976\t Train: 76.78%\t Valid: 75.46%\tTest: 67.13%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2977\t Train: 76.24%\t Valid: 75.14%\tTest: 66.81%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2979\t Train: 76.70%\t Valid: 75.76%\tTest: 67.81%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2981\t Train: 77.31%\t Valid: 76.11%\tTest: 68.45%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2967\t Train: 76.97%\t Valid: 75.22%\tTest: 66.89%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2959\t Train: 77.50%\t Valid: 76.35%\tTest: 68.68%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2962\t Train: 77.53%\t Valid: 75.52%\tTest: 67.82%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2934\t Train: 77.74%\t Valid: 76.21%\tTest: 68.28%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2933\t Train: 77.94%\t Valid: 75.55%\tTest: 67.70%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2973\t Train: 77.08%\t Valid: 74.40%\tTest: 65.71%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2927\t Train: 78.26%\t Valid: 75.94%\tTest: 68.69%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2928\t Train: 78.52%\t Valid: 76.42%\tTest: 68.93%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2920\t Train: 78.71%\t Valid: 76.50%\tTest: 69.38%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2953\t Train: 77.82%\t Valid: 75.40%\tTest: 67.77%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2926\t Train: 78.30%\t Valid: 76.02%\tTest: 68.44%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2898\t Train: 78.94%\t Valid: 76.72%\tTest: 69.33%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2890\t Train: 79.20%\t Valid: 76.79%\tTest: 69.55%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2900\t Train: 79.18%\t Valid: 77.04%\tTest: 70.38%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2918\t Train: 79.51%\t Valid: 76.88%\tTest: 70.11%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2871\t Train: 79.42%\t Valid: 76.72%\tTest: 69.81%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2872\t Train: 79.65%\t Valid: 76.98%\tTest: 70.38%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2936\t Train: 77.98%\t Valid: 75.73%\tTest: 70.23%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2913\t Train: 79.18%\t Valid: 77.25%\tTest: 70.15%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2882\t Train: 79.38%\t Valid: 76.77%\tTest: 69.89%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2857\t Train: 79.90%\t Valid: 77.10%\tTest: 70.05%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2918\t Train: 79.52%\t Valid: 76.87%\tTest: 70.52%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2866\t Train: 79.68%\t Valid: 77.11%\tTest: 70.64%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2853\t Train: 80.09%\t Valid: 77.02%\tTest: 70.83%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2836\t Train: 80.43%\t Valid: 77.28%\tTest: 70.77%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2873\t Train: 79.60%\t Valid: 76.49%\tTest: 69.17%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2839\t Train: 80.12%\t Valid: 77.24%\tTest: 70.41%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2823\t Train: 80.59%\t Valid: 77.51%\tTest: 71.02%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2824\t Train: 80.90%\t Valid: 77.70%\tTest: 71.48%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2846\t Train: 80.75%\t Valid: 77.36%\tTest: 71.84%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2822\t Train: 80.83%\t Valid: 77.88%\tTest: 71.81%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2817\t Train: 81.05%\t Valid: 77.44%\tTest: 71.25%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2875\t Train: 81.23%\t Valid: 77.78%\tTest: 72.26%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2803\t Train: 81.46%\t Valid: 78.11%\tTest: 71.98%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2804\t Train: 81.24%\t Valid: 77.38%\tTest: 70.69%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2826\t Train: 81.55%\t Valid: 77.81%\tTest: 71.26%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2802\t Train: 81.69%\t Valid: 77.84%\tTest: 72.15%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2776\t Train: 81.63%\t Valid: 78.03%\tTest: 71.56%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2779\t Train: 81.86%\t Valid: 77.99%\tTest: 71.85%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2766\t Train: 81.98%\t Valid: 78.36%\tTest: 72.50%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2773\t Train: 82.16%\t Valid: 77.91%\tTest: 71.67%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2750\t Train: 82.03%\t Valid: 78.56%\tTest: 72.35%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2761\t Train: 82.40%\t Valid: 78.10%\tTest: 71.63%\n",
      "Seed =  4\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5090\t Train: 38.26%\t Valid: 36.26%\tTest: 39.22%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4256\t Train: 36.77%\t Valid: 33.29%\tTest: 36.41%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4048\t Train: 39.04%\t Valid: 34.36%\tTest: 36.76%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3944\t Train: 42.75%\t Valid: 34.41%\tTest: 36.69%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3867\t Train: 48.09%\t Valid: 36.23%\tTest: 37.65%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3806\t Train: 50.89%\t Valid: 38.25%\tTest: 37.48%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3708\t Train: 53.16%\t Valid: 42.08%\tTest: 39.19%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3630\t Train: 55.18%\t Valid: 46.50%\tTest: 41.00%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3598\t Train: 55.96%\t Valid: 47.37%\tTest: 41.34%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3531\t Train: 58.24%\t Valid: 51.41%\tTest: 43.44%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3474\t Train: 60.51%\t Valid: 55.59%\tTest: 45.78%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3446\t Train: 61.99%\t Valid: 57.79%\tTest: 47.33%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3425\t Train: 64.94%\t Valid: 61.82%\tTest: 51.37%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3398\t Train: 64.27%\t Valid: 60.21%\tTest: 50.30%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3359\t Train: 66.34%\t Valid: 63.59%\tTest: 52.73%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3343\t Train: 66.22%\t Valid: 63.68%\tTest: 52.88%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3288\t Train: 65.51%\t Valid: 62.60%\tTest: 52.04%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3260\t Train: 66.80%\t Valid: 64.79%\tTest: 54.40%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3240\t Train: 67.49%\t Valid: 66.20%\tTest: 55.94%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3232\t Train: 68.66%\t Valid: 67.48%\tTest: 57.51%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3216\t Train: 69.70%\t Valid: 69.32%\tTest: 59.02%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3201\t Train: 69.46%\t Valid: 69.20%\tTest: 59.10%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3210\t Train: 71.17%\t Valid: 71.40%\tTest: 61.29%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3220\t Train: 69.87%\t Valid: 69.75%\tTest: 59.59%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3191\t Train: 69.92%\t Valid: 69.95%\tTest: 59.84%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3173\t Train: 70.77%\t Valid: 69.47%\tTest: 59.85%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3158\t Train: 71.02%\t Valid: 70.31%\tTest: 60.49%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3147\t Train: 71.86%\t Valid: 71.72%\tTest: 61.89%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3140\t Train: 72.19%\t Valid: 72.29%\tTest: 62.47%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3128\t Train: 72.25%\t Valid: 72.17%\tTest: 62.46%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3116\t Train: 72.70%\t Valid: 72.51%\tTest: 62.88%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3117\t Train: 73.07%\t Valid: 72.98%\tTest: 63.46%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3104\t Train: 72.84%\t Valid: 72.88%\tTest: 63.24%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3095\t Train: 72.97%\t Valid: 72.66%\tTest: 63.07%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3090\t Train: 73.09%\t Valid: 71.33%\tTest: 62.19%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3113\t Train: 72.46%\t Valid: 71.57%\tTest: 62.13%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3087\t Train: 74.46%\t Valid: 73.30%\tTest: 64.22%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3099\t Train: 73.90%\t Valid: 73.27%\tTest: 63.78%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3057\t Train: 73.33%\t Valid: 71.99%\tTest: 62.43%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3078\t Train: 73.40%\t Valid: 73.33%\tTest: 64.28%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3054\t Train: 74.69%\t Valid: 73.47%\tTest: 64.34%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3037\t Train: 75.11%\t Valid: 73.86%\tTest: 64.87%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3039\t Train: 75.07%\t Valid: 73.02%\tTest: 64.07%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3051\t Train: 75.13%\t Valid: 73.63%\tTest: 64.90%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3022\t Train: 75.58%\t Valid: 74.38%\tTest: 65.61%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3005\t Train: 75.63%\t Valid: 74.37%\tTest: 65.60%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.3016\t Train: 75.62%\t Valid: 75.06%\tTest: 66.60%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2994\t Train: 75.73%\t Valid: 74.80%\tTest: 66.19%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2984\t Train: 76.48%\t Valid: 74.92%\tTest: 66.47%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2987\t Train: 76.17%\t Valid: 73.43%\tTest: 65.17%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.3020\t Train: 76.44%\t Valid: 74.98%\tTest: 67.27%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2995\t Train: 76.67%\t Valid: 75.01%\tTest: 67.10%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2985\t Train: 76.74%\t Valid: 74.91%\tTest: 66.78%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2970\t Train: 76.62%\t Valid: 75.36%\tTest: 67.28%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2991\t Train: 75.21%\t Valid: 73.19%\tTest: 64.75%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2991\t Train: 76.97%\t Valid: 75.99%\tTest: 68.43%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2966\t Train: 77.34%\t Valid: 75.51%\tTest: 67.88%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2947\t Train: 77.34%\t Valid: 75.88%\tTest: 68.13%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2939\t Train: 77.77%\t Valid: 76.48%\tTest: 68.98%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2929\t Train: 77.99%\t Valid: 76.59%\tTest: 69.33%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2927\t Train: 78.17%\t Valid: 76.87%\tTest: 69.79%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.3009\t Train: 78.39%\t Valid: 76.43%\tTest: 69.26%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2926\t Train: 78.35%\t Valid: 76.11%\tTest: 68.98%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2917\t Train: 78.10%\t Valid: 76.78%\tTest: 69.57%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2911\t Train: 78.79%\t Valid: 76.83%\tTest: 69.60%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2899\t Train: 78.95%\t Valid: 77.03%\tTest: 69.97%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2902\t Train: 79.16%\t Valid: 77.17%\tTest: 70.24%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2897\t Train: 79.17%\t Valid: 77.09%\tTest: 69.97%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2891\t Train: 79.46%\t Valid: 77.26%\tTest: 70.26%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2879\t Train: 79.22%\t Valid: 76.94%\tTest: 69.70%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2901\t Train: 79.15%\t Valid: 76.80%\tTest: 69.77%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2977\t Train: 79.40%\t Valid: 77.17%\tTest: 70.73%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2884\t Train: 79.36%\t Valid: 77.62%\tTest: 70.73%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2864\t Train: 79.67%\t Valid: 77.21%\tTest: 70.18%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2862\t Train: 80.26%\t Valid: 77.94%\tTest: 71.04%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2867\t Train: 79.82%\t Valid: 77.98%\tTest: 71.34%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2869\t Train: 80.27%\t Valid: 77.80%\tTest: 71.11%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2862\t Train: 80.54%\t Valid: 77.90%\tTest: 70.84%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2872\t Train: 80.50%\t Valid: 78.15%\tTest: 71.87%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2868\t Train: 80.26%\t Valid: 77.57%\tTest: 70.86%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2824\t Train: 80.65%\t Valid: 77.79%\tTest: 71.31%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2833\t Train: 80.83%\t Valid: 78.30%\tTest: 71.97%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2925\t Train: 80.31%\t Valid: 78.07%\tTest: 71.29%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2848\t Train: 80.19%\t Valid: 77.59%\tTest: 70.42%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2817\t Train: 80.76%\t Valid: 78.13%\tTest: 71.47%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2813\t Train: 81.15%\t Valid: 78.32%\tTest: 71.62%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2827\t Train: 80.34%\t Valid: 77.18%\tTest: 70.91%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2824\t Train: 81.01%\t Valid: 77.92%\tTest: 71.01%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2810\t Train: 81.44%\t Valid: 78.35%\tTest: 72.15%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2788\t Train: 81.71%\t Valid: 78.61%\tTest: 72.22%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2798\t Train: 81.19%\t Valid: 77.75%\tTest: 71.76%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2845\t Train: 81.47%\t Valid: 78.33%\tTest: 71.81%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2760\t Train: 81.92%\t Valid: 78.60%\tTest: 72.34%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2774\t Train: 81.66%\t Valid: 77.93%\tTest: 71.81%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2768\t Train: 82.09%\t Valid: 78.81%\tTest: 72.56%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2771\t Train: 82.15%\t Valid: 78.53%\tTest: 72.04%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2743\t Train: 82.39%\t Valid: 78.73%\tTest: 72.44%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2750\t Train: 82.50%\t Valid: 78.89%\tTest: 72.73%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2753\t Train: 82.41%\t Valid: 78.84%\tTest: 72.61%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2724\t Train: 82.47%\t Valid: 79.02%\tTest: 73.01%\n",
      "Seed =  5\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4967\t Train: 37.59%\t Valid: 35.45%\tTest: 38.79%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4254\t Train: 36.90%\t Valid: 33.79%\tTest: 36.91%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4066\t Train: 38.24%\t Valid: 33.96%\tTest: 36.63%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3958\t Train: 41.45%\t Valid: 34.20%\tTest: 36.50%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3849\t Train: 49.66%\t Valid: 36.40%\tTest: 37.48%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3778\t Train: 49.91%\t Valid: 38.17%\tTest: 37.67%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3690\t Train: 53.30%\t Valid: 42.83%\tTest: 39.09%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3665\t Train: 56.30%\t Valid: 47.51%\tTest: 41.41%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3566\t Train: 56.43%\t Valid: 48.31%\tTest: 41.56%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3529\t Train: 59.55%\t Valid: 53.63%\tTest: 44.46%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3475\t Train: 59.63%\t Valid: 54.12%\tTest: 44.62%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3444\t Train: 61.84%\t Valid: 57.76%\tTest: 47.00%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3457\t Train: 62.45%\t Valid: 58.74%\tTest: 47.58%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3400\t Train: 63.57%\t Valid: 60.50%\tTest: 49.13%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3371\t Train: 64.43%\t Valid: 61.75%\tTest: 50.48%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3400\t Train: 65.62%\t Valid: 63.97%\tTest: 52.55%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3362\t Train: 64.81%\t Valid: 61.49%\tTest: 50.63%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3335\t Train: 66.22%\t Valid: 63.56%\tTest: 52.55%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3291\t Train: 65.91%\t Valid: 62.74%\tTest: 51.75%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3280\t Train: 66.08%\t Valid: 62.33%\tTest: 51.93%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3262\t Train: 69.08%\t Valid: 66.53%\tTest: 56.12%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3232\t Train: 69.24%\t Valid: 66.40%\tTest: 56.36%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3220\t Train: 69.56%\t Valid: 67.39%\tTest: 57.21%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3246\t Train: 69.88%\t Valid: 67.13%\tTest: 56.90%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3210\t Train: 70.91%\t Valid: 68.78%\tTest: 58.68%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3188\t Train: 71.21%\t Valid: 68.66%\tTest: 58.65%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3160\t Train: 71.50%\t Valid: 69.67%\tTest: 59.36%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3157\t Train: 71.10%\t Valid: 68.87%\tTest: 58.34%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3136\t Train: 72.06%\t Valid: 69.75%\tTest: 59.55%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3122\t Train: 72.36%\t Valid: 70.13%\tTest: 59.95%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3135\t Train: 71.56%\t Valid: 70.02%\tTest: 59.85%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3116\t Train: 72.72%\t Valid: 70.81%\tTest: 60.92%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3102\t Train: 72.97%\t Valid: 70.99%\tTest: 61.22%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3100\t Train: 73.91%\t Valid: 72.13%\tTest: 62.57%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3087\t Train: 73.14%\t Valid: 70.92%\tTest: 60.71%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3071\t Train: 73.88%\t Valid: 71.88%\tTest: 61.97%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3055\t Train: 74.40%\t Valid: 72.60%\tTest: 62.76%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3064\t Train: 73.57%\t Valid: 71.71%\tTest: 61.67%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3037\t Train: 74.79%\t Valid: 73.47%\tTest: 63.79%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3138\t Train: 74.10%\t Valid: 73.37%\tTest: 63.93%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3068\t Train: 73.93%\t Valid: 71.16%\tTest: 61.07%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3042\t Train: 74.76%\t Valid: 73.27%\tTest: 63.74%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3011\t Train: 75.36%\t Valid: 73.88%\tTest: 64.19%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3001\t Train: 75.95%\t Valid: 74.94%\tTest: 65.76%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3107\t Train: 74.84%\t Valid: 74.15%\tTest: 66.22%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3015\t Train: 76.08%\t Valid: 72.94%\tTest: 64.57%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2979\t Train: 76.37%\t Valid: 75.06%\tTest: 66.78%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2988\t Train: 74.95%\t Valid: 71.56%\tTest: 62.88%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2963\t Train: 77.17%\t Valid: 74.96%\tTest: 66.54%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2943\t Train: 77.32%\t Valid: 74.82%\tTest: 66.51%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2981\t Train: 77.54%\t Valid: 75.19%\tTest: 66.69%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2955\t Train: 78.11%\t Valid: 76.54%\tTest: 68.81%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2918\t Train: 77.89%\t Valid: 75.04%\tTest: 66.72%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2913\t Train: 79.01%\t Valid: 76.49%\tTest: 69.26%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2905\t Train: 78.19%\t Valid: 74.57%\tTest: 66.10%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2900\t Train: 79.18%\t Valid: 76.46%\tTest: 69.29%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2945\t Train: 77.23%\t Valid: 73.83%\tTest: 66.99%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2924\t Train: 79.60%\t Valid: 76.20%\tTest: 69.02%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2872\t Train: 79.16%\t Valid: 75.23%\tTest: 67.46%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2860\t Train: 79.81%\t Valid: 77.06%\tTest: 70.12%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2936\t Train: 79.31%\t Valid: 76.03%\tTest: 68.43%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2868\t Train: 79.99%\t Valid: 76.71%\tTest: 69.47%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2845\t Train: 80.23%\t Valid: 77.22%\tTest: 70.38%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2903\t Train: 79.98%\t Valid: 77.67%\tTest: 70.84%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2815\t Train: 80.60%\t Valid: 76.38%\tTest: 68.77%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.3096\t Train: 78.61%\t Valid: 74.08%\tTest: 66.50%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2922\t Train: 79.18%\t Valid: 76.74%\tTest: 70.29%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2866\t Train: 80.07%\t Valid: 76.99%\tTest: 69.72%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2831\t Train: 80.52%\t Valid: 76.97%\tTest: 69.84%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2822\t Train: 80.96%\t Valid: 77.10%\tTest: 70.26%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2798\t Train: 81.16%\t Valid: 76.98%\tTest: 69.99%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2783\t Train: 81.45%\t Valid: 77.07%\tTest: 69.56%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2802\t Train: 81.58%\t Valid: 77.64%\tTest: 70.26%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2781\t Train: 81.24%\t Valid: 77.35%\tTest: 70.36%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2780\t Train: 81.23%\t Valid: 75.78%\tTest: 68.75%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2776\t Train: 81.56%\t Valid: 78.04%\tTest: 71.47%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2815\t Train: 80.96%\t Valid: 77.46%\tTest: 70.84%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2781\t Train: 81.54%\t Valid: 77.63%\tTest: 70.46%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2774\t Train: 81.94%\t Valid: 77.43%\tTest: 70.54%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2753\t Train: 82.16%\t Valid: 77.05%\tTest: 69.96%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2807\t Train: 81.00%\t Valid: 74.97%\tTest: 67.90%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2743\t Train: 82.26%\t Valid: 77.22%\tTest: 69.64%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2738\t Train: 82.42%\t Valid: 77.01%\tTest: 69.47%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2744\t Train: 82.49%\t Valid: 77.93%\tTest: 70.70%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2726\t Train: 82.54%\t Valid: 77.75%\tTest: 70.40%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2945\t Train: 81.32%\t Valid: 77.85%\tTest: 70.47%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2756\t Train: 82.02%\t Valid: 77.60%\tTest: 69.89%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2734\t Train: 82.43%\t Valid: 77.28%\tTest: 69.55%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2843\t Train: 81.85%\t Valid: 77.10%\tTest: 69.90%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2774\t Train: 81.85%\t Valid: 77.61%\tTest: 70.42%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2746\t Train: 82.47%\t Valid: 78.32%\tTest: 71.18%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2723\t Train: 82.80%\t Valid: 77.75%\tTest: 70.26%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2712\t Train: 82.85%\t Valid: 77.63%\tTest: 69.53%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2709\t Train: 83.07%\t Valid: 77.85%\tTest: 70.16%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2736\t Train: 83.01%\t Valid: 78.22%\tTest: 71.35%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2778\t Train: 82.94%\t Valid: 78.14%\tTest: 71.15%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2703\t Train: 83.20%\t Valid: 77.76%\tTest: 69.96%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2702\t Train: 82.47%\t Valid: 75.43%\tTest: 66.54%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2794\t Train: 82.63%\t Valid: 78.18%\tTest: 70.64%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2706\t Train: 82.86%\t Valid: 77.53%\tTest: 69.63%\n",
      "Seed =  6\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5238\t Train: 38.28%\t Valid: 36.60%\tTest: 39.58%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4223\t Train: 35.87%\t Valid: 33.39%\tTest: 36.68%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4097\t Train: 36.58%\t Valid: 33.60%\tTest: 36.39%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3997\t Train: 39.14%\t Valid: 34.40%\tTest: 36.56%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3870\t Train: 46.66%\t Valid: 35.48%\tTest: 37.15%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3816\t Train: 49.48%\t Valid: 37.02%\tTest: 37.43%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3753\t Train: 50.13%\t Valid: 40.01%\tTest: 37.81%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3669\t Train: 54.57%\t Valid: 46.45%\tTest: 40.94%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3609\t Train: 54.85%\t Valid: 47.06%\tTest: 40.72%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3555\t Train: 59.69%\t Valid: 54.19%\tTest: 44.68%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3513\t Train: 58.65%\t Valid: 52.70%\tTest: 43.51%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3454\t Train: 60.12%\t Valid: 55.03%\tTest: 45.02%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3440\t Train: 61.32%\t Valid: 56.83%\tTest: 46.03%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3416\t Train: 62.25%\t Valid: 58.12%\tTest: 47.09%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3361\t Train: 64.38%\t Valid: 61.83%\tTest: 50.47%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3341\t Train: 66.10%\t Valid: 63.70%\tTest: 52.10%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3313\t Train: 66.91%\t Valid: 64.68%\tTest: 53.25%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3319\t Train: 67.17%\t Valid: 64.70%\tTest: 53.32%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3277\t Train: 67.18%\t Valid: 64.58%\tTest: 53.35%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3340\t Train: 70.29%\t Valid: 69.10%\tTest: 57.73%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3248\t Train: 69.12%\t Valid: 67.02%\tTest: 55.37%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3221\t Train: 69.22%\t Valid: 66.47%\tTest: 56.24%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3210\t Train: 69.85%\t Valid: 68.57%\tTest: 57.78%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3188\t Train: 70.14%\t Valid: 68.71%\tTest: 58.06%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3180\t Train: 70.40%\t Valid: 69.33%\tTest: 58.74%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3165\t Train: 71.13%\t Valid: 70.10%\tTest: 59.61%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3152\t Train: 71.48%\t Valid: 70.32%\tTest: 59.83%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3176\t Train: 71.56%\t Valid: 70.48%\tTest: 60.09%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3152\t Train: 71.50%\t Valid: 70.15%\tTest: 59.78%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3127\t Train: 71.95%\t Valid: 70.81%\tTest: 60.41%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3156\t Train: 72.05%\t Valid: 70.79%\tTest: 61.23%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3115\t Train: 72.82%\t Valid: 71.03%\tTest: 60.42%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3097\t Train: 73.57%\t Valid: 71.90%\tTest: 61.38%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3083\t Train: 73.75%\t Valid: 72.39%\tTest: 62.13%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3568\t Train: 60.35%\t Valid: 52.06%\tTest: 51.01%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3349\t Train: 65.43%\t Valid: 59.70%\tTest: 52.56%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3253\t Train: 69.07%\t Valid: 65.14%\tTest: 56.47%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3197\t Train: 69.77%\t Valid: 67.07%\tTest: 58.27%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3168\t Train: 71.11%\t Valid: 69.35%\tTest: 60.24%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3139\t Train: 71.67%\t Valid: 70.33%\tTest: 61.54%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3120\t Train: 72.19%\t Valid: 70.73%\tTest: 62.24%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3108\t Train: 72.07%\t Valid: 69.51%\tTest: 60.96%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3114\t Train: 72.85%\t Valid: 72.52%\tTest: 63.80%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3096\t Train: 73.30%\t Valid: 71.53%\tTest: 63.00%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3081\t Train: 73.73%\t Valid: 72.50%\tTest: 64.22%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3065\t Train: 74.04%\t Valid: 72.99%\tTest: 64.34%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.3097\t Train: 73.75%\t Valid: 73.81%\tTest: 65.56%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.3061\t Train: 74.95%\t Valid: 74.17%\tTest: 65.91%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.3038\t Train: 75.32%\t Valid: 74.12%\tTest: 65.56%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.3022\t Train: 75.23%\t Valid: 74.64%\tTest: 66.60%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.3027\t Train: 75.51%\t Valid: 73.08%\tTest: 64.11%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.3005\t Train: 76.14%\t Valid: 76.11%\tTest: 69.08%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2992\t Train: 76.25%\t Valid: 76.29%\tTest: 69.65%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2975\t Train: 76.94%\t Valid: 76.05%\tTest: 69.05%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2953\t Train: 77.32%\t Valid: 76.74%\tTest: 70.15%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2944\t Train: 77.95%\t Valid: 77.10%\tTest: 70.49%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2975\t Train: 77.73%\t Valid: 77.29%\tTest: 71.37%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2936\t Train: 78.19%\t Valid: 77.24%\tTest: 70.97%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2903\t Train: 78.68%\t Valid: 77.44%\tTest: 71.17%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2898\t Train: 78.19%\t Valid: 77.52%\tTest: 72.19%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2912\t Train: 78.97%\t Valid: 77.42%\tTest: 71.25%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2869\t Train: 79.32%\t Valid: 77.54%\tTest: 71.47%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.3077\t Train: 78.64%\t Valid: 77.24%\tTest: 71.10%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2934\t Train: 78.81%\t Valid: 77.80%\tTest: 72.54%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2896\t Train: 79.19%\t Valid: 77.44%\tTest: 71.12%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2868\t Train: 79.51%\t Valid: 78.25%\tTest: 72.36%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2847\t Train: 80.05%\t Valid: 78.12%\tTest: 71.84%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2833\t Train: 80.37%\t Valid: 78.44%\tTest: 72.02%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2830\t Train: 80.47%\t Valid: 78.36%\tTest: 72.29%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2846\t Train: 80.66%\t Valid: 78.69%\tTest: 72.48%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2833\t Train: 80.57%\t Valid: 78.25%\tTest: 71.47%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2830\t Train: 80.31%\t Valid: 78.62%\tTest: 73.17%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2816\t Train: 80.74%\t Valid: 78.06%\tTest: 71.53%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2802\t Train: 80.71%\t Valid: 78.88%\tTest: 72.77%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2797\t Train: 81.24%\t Valid: 78.82%\tTest: 72.22%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2874\t Train: 79.35%\t Valid: 78.69%\tTest: 73.42%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2826\t Train: 80.33%\t Valid: 78.86%\tTest: 73.21%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2804\t Train: 81.15%\t Valid: 78.72%\tTest: 72.28%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2794\t Train: 81.46%\t Valid: 78.71%\tTest: 72.11%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2770\t Train: 81.44%\t Valid: 78.68%\tTest: 71.67%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2770\t Train: 81.30%\t Valid: 78.18%\tTest: 70.97%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2760\t Train: 81.76%\t Valid: 78.88%\tTest: 72.01%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2798\t Train: 81.74%\t Valid: 78.88%\tTest: 72.23%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2756\t Train: 81.68%\t Valid: 78.76%\tTest: 71.68%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2768\t Train: 81.50%\t Valid: 78.22%\tTest: 70.99%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2784\t Train: 81.79%\t Valid: 78.28%\tTest: 70.79%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2756\t Train: 82.09%\t Valid: 78.74%\tTest: 71.36%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2740\t Train: 82.26%\t Valid: 78.85%\tTest: 71.42%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2745\t Train: 82.08%\t Valid: 79.08%\tTest: 71.62%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2734\t Train: 82.25%\t Valid: 79.51%\tTest: 72.71%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2749\t Train: 81.68%\t Valid: 78.03%\tTest: 70.51%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2744\t Train: 82.35%\t Valid: 79.10%\tTest: 71.58%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2717\t Train: 82.54%\t Valid: 79.44%\tTest: 71.88%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2718\t Train: 82.40%\t Valid: 79.26%\tTest: 71.74%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2716\t Train: 82.70%\t Valid: 79.04%\tTest: 71.00%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2784\t Train: 82.22%\t Valid: 78.56%\tTest: 70.66%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2718\t Train: 82.74%\t Valid: 79.70%\tTest: 72.70%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2719\t Train: 82.72%\t Valid: 79.83%\tTest: 72.55%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2706\t Train: 82.96%\t Valid: 79.50%\tTest: 71.83%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2693\t Train: 83.00%\t Valid: 79.76%\tTest: 71.74%\n",
      "Seed =  7\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5472\t Train: 38.30%\t Valid: 36.51%\tTest: 39.26%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4227\t Train: 36.35%\t Valid: 33.75%\tTest: 36.75%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4074\t Train: 37.83%\t Valid: 34.36%\tTest: 36.86%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3966\t Train: 41.04%\t Valid: 34.72%\tTest: 36.88%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3875\t Train: 47.04%\t Valid: 35.64%\tTest: 37.36%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3820\t Train: 50.08%\t Valid: 37.66%\tTest: 37.95%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3750\t Train: 50.03%\t Valid: 40.00%\tTest: 38.16%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3683\t Train: 51.99%\t Valid: 43.59%\tTest: 39.78%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3649\t Train: 51.88%\t Valid: 44.08%\tTest: 39.38%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3602\t Train: 56.74%\t Valid: 48.41%\tTest: 41.79%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3546\t Train: 58.69%\t Valid: 51.98%\tTest: 44.10%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3538\t Train: 58.96%\t Valid: 52.78%\tTest: 44.38%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3463\t Train: 58.69%\t Valid: 52.52%\tTest: 44.07%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3425\t Train: 60.63%\t Valid: 55.41%\tTest: 46.09%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3388\t Train: 63.13%\t Valid: 59.06%\tTest: 48.67%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3366\t Train: 63.38%\t Valid: 59.88%\tTest: 49.45%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3339\t Train: 59.78%\t Valid: 54.64%\tTest: 45.38%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3353\t Train: 61.77%\t Valid: 56.10%\tTest: 47.21%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3320\t Train: 65.25%\t Valid: 61.44%\tTest: 51.21%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3287\t Train: 66.40%\t Valid: 63.53%\tTest: 52.93%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3259\t Train: 68.07%\t Valid: 66.54%\tTest: 56.06%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3235\t Train: 68.76%\t Valid: 67.95%\tTest: 57.86%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3215\t Train: 69.04%\t Valid: 68.35%\tTest: 58.60%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3201\t Train: 69.16%\t Valid: 68.31%\tTest: 58.62%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3191\t Train: 68.97%\t Valid: 68.18%\tTest: 58.75%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3185\t Train: 70.28%\t Valid: 70.02%\tTest: 60.25%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3169\t Train: 70.20%\t Valid: 69.83%\tTest: 60.14%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3157\t Train: 71.34%\t Valid: 70.86%\tTest: 61.60%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3162\t Train: 71.01%\t Valid: 71.00%\tTest: 61.55%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3140\t Train: 71.61%\t Valid: 71.41%\tTest: 61.80%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3130\t Train: 72.50%\t Valid: 72.55%\tTest: 63.36%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3129\t Train: 72.05%\t Valid: 70.94%\tTest: 62.27%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3114\t Train: 72.59%\t Valid: 71.61%\tTest: 62.87%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3103\t Train: 72.85%\t Valid: 71.91%\tTest: 63.26%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3096\t Train: 72.90%\t Valid: 72.37%\tTest: 63.72%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3083\t Train: 73.24%\t Valid: 72.84%\tTest: 64.17%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3127\t Train: 73.66%\t Valid: 73.68%\tTest: 65.03%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3078\t Train: 73.83%\t Valid: 73.61%\tTest: 65.23%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3069\t Train: 74.30%\t Valid: 73.87%\tTest: 65.56%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3060\t Train: 73.59%\t Valid: 73.06%\tTest: 64.51%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3056\t Train: 74.49%\t Valid: 74.07%\tTest: 65.89%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3041\t Train: 74.00%\t Valid: 72.92%\tTest: 64.51%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3043\t Train: 74.47%\t Valid: 73.91%\tTest: 65.79%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3027\t Train: 75.12%\t Valid: 74.12%\tTest: 66.21%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3032\t Train: 75.59%\t Valid: 74.57%\tTest: 67.04%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3017\t Train: 75.23%\t Valid: 74.54%\tTest: 66.99%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.3015\t Train: 75.76%\t Valid: 74.88%\tTest: 67.53%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.3002\t Train: 75.67%\t Valid: 74.37%\tTest: 66.77%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2996\t Train: 75.73%\t Valid: 74.47%\tTest: 66.96%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.3057\t Train: 75.56%\t Valid: 74.61%\tTest: 67.18%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.3008\t Train: 75.80%\t Valid: 74.33%\tTest: 66.81%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2997\t Train: 75.93%\t Valid: 74.39%\tTest: 67.01%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2982\t Train: 76.40%\t Valid: 74.89%\tTest: 67.63%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.3024\t Train: 76.40%\t Valid: 74.05%\tTest: 66.67%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2982\t Train: 76.31%\t Valid: 74.44%\tTest: 67.23%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2974\t Train: 76.53%\t Valid: 74.61%\tTest: 67.24%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2979\t Train: 76.73%\t Valid: 74.16%\tTest: 66.68%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2958\t Train: 76.97%\t Valid: 75.37%\tTest: 68.66%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2946\t Train: 77.44%\t Valid: 75.31%\tTest: 68.41%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2946\t Train: 77.02%\t Valid: 74.43%\tTest: 67.25%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2974\t Train: 76.86%\t Valid: 75.01%\tTest: 67.64%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2958\t Train: 77.14%\t Valid: 74.98%\tTest: 68.14%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2938\t Train: 77.49%\t Valid: 75.44%\tTest: 68.94%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2933\t Train: 77.73%\t Valid: 75.89%\tTest: 69.20%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2939\t Train: 77.77%\t Valid: 75.94%\tTest: 69.70%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2925\t Train: 77.97%\t Valid: 75.40%\tTest: 68.23%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2953\t Train: 77.79%\t Valid: 75.42%\tTest: 68.66%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2960\t Train: 77.67%\t Valid: 74.42%\tTest: 66.99%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2941\t Train: 77.97%\t Valid: 75.83%\tTest: 69.33%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2911\t Train: 78.60%\t Valid: 75.70%\tTest: 69.00%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2914\t Train: 78.52%\t Valid: 75.74%\tTest: 68.50%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2925\t Train: 78.73%\t Valid: 75.70%\tTest: 67.93%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2893\t Train: 79.00%\t Valid: 76.29%\tTest: 69.17%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2907\t Train: 79.01%\t Valid: 75.96%\tTest: 68.28%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2892\t Train: 78.66%\t Valid: 74.64%\tTest: 66.21%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2954\t Train: 78.35%\t Valid: 75.75%\tTest: 68.88%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2912\t Train: 78.79%\t Valid: 76.13%\tTest: 69.34%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2890\t Train: 79.17%\t Valid: 76.64%\tTest: 70.17%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2873\t Train: 79.54%\t Valid: 76.65%\tTest: 69.51%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2865\t Train: 79.61%\t Valid: 76.47%\tTest: 68.78%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2889\t Train: 79.31%\t Valid: 76.61%\tTest: 69.34%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2863\t Train: 79.98%\t Valid: 76.60%\tTest: 69.11%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2862\t Train: 79.55%\t Valid: 76.11%\tTest: 68.68%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2844\t Train: 79.87%\t Valid: 76.49%\tTest: 69.36%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2975\t Train: 78.47%\t Valid: 76.36%\tTest: 70.48%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2885\t Train: 79.72%\t Valid: 76.10%\tTest: 69.74%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2857\t Train: 79.88%\t Valid: 76.80%\tTest: 70.36%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2843\t Train: 80.12%\t Valid: 76.61%\tTest: 69.31%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2860\t Train: 80.13%\t Valid: 76.36%\tTest: 68.56%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2846\t Train: 80.32%\t Valid: 76.38%\tTest: 68.40%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2833\t Train: 80.59%\t Valid: 77.20%\tTest: 70.50%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2846\t Train: 80.66%\t Valid: 77.44%\tTest: 70.96%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2846\t Train: 80.82%\t Valid: 77.06%\tTest: 69.22%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2820\t Train: 81.02%\t Valid: 76.84%\tTest: 69.70%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2820\t Train: 81.18%\t Valid: 77.27%\tTest: 69.69%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2847\t Train: 81.14%\t Valid: 76.78%\tTest: 68.63%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2803\t Train: 81.29%\t Valid: 77.21%\tTest: 69.27%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2870\t Train: 80.79%\t Valid: 76.23%\tTest: 68.90%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2859\t Train: 81.38%\t Valid: 77.10%\tTest: 68.36%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2830\t Train: 81.41%\t Valid: 77.13%\tTest: 68.63%\n",
      "Seed =  8\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5196\t Train: 37.62%\t Valid: 35.72%\tTest: 38.99%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4301\t Train: 36.59%\t Valid: 33.97%\tTest: 37.08%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4076\t Train: 37.57%\t Valid: 34.06%\tTest: 36.76%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3984\t Train: 40.79%\t Valid: 34.76%\tTest: 36.94%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3877\t Train: 46.67%\t Valid: 35.68%\tTest: 37.41%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3813\t Train: 49.60%\t Valid: 37.12%\tTest: 37.91%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3719\t Train: 53.34%\t Valid: 41.31%\tTest: 39.09%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3652\t Train: 54.86%\t Valid: 45.24%\tTest: 40.75%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3578\t Train: 56.40%\t Valid: 49.19%\tTest: 42.52%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3517\t Train: 58.49%\t Valid: 51.59%\tTest: 43.58%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3477\t Train: 61.10%\t Valid: 55.89%\tTest: 45.97%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3434\t Train: 62.23%\t Valid: 57.61%\tTest: 46.78%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3377\t Train: 62.84%\t Valid: 58.35%\tTest: 47.54%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3390\t Train: 60.08%\t Valid: 51.95%\tTest: 44.56%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3365\t Train: 62.58%\t Valid: 57.54%\tTest: 46.95%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3332\t Train: 65.91%\t Valid: 62.52%\tTest: 50.50%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3285\t Train: 66.82%\t Valid: 64.45%\tTest: 53.11%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3296\t Train: 66.03%\t Valid: 64.01%\tTest: 53.57%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3253\t Train: 68.48%\t Valid: 67.23%\tTest: 56.85%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3240\t Train: 68.07%\t Valid: 66.50%\tTest: 55.84%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3227\t Train: 68.28%\t Valid: 67.07%\tTest: 56.53%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3217\t Train: 69.64%\t Valid: 68.58%\tTest: 58.49%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3217\t Train: 70.09%\t Valid: 69.09%\tTest: 59.07%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3197\t Train: 69.31%\t Valid: 68.75%\tTest: 58.64%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3201\t Train: 69.39%\t Valid: 67.10%\tTest: 57.06%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3186\t Train: 69.78%\t Valid: 69.03%\tTest: 58.95%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3169\t Train: 70.41%\t Valid: 68.97%\tTest: 58.97%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3194\t Train: 70.82%\t Valid: 69.72%\tTest: 59.86%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3160\t Train: 71.32%\t Valid: 70.05%\tTest: 60.20%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3147\t Train: 71.45%\t Valid: 70.56%\tTest: 60.81%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3142\t Train: 71.85%\t Valid: 71.24%\tTest: 61.60%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3162\t Train: 71.75%\t Valid: 69.17%\tTest: 59.68%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3164\t Train: 71.28%\t Valid: 70.06%\tTest: 60.54%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3138\t Train: 71.56%\t Valid: 70.91%\tTest: 61.48%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3123\t Train: 72.18%\t Valid: 70.86%\tTest: 61.46%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3108\t Train: 72.46%\t Valid: 71.47%\tTest: 61.92%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3102\t Train: 72.81%\t Valid: 71.61%\tTest: 61.95%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3098\t Train: 73.15%\t Valid: 72.30%\tTest: 62.73%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3096\t Train: 73.52%\t Valid: 72.90%\tTest: 63.64%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3079\t Train: 73.21%\t Valid: 72.63%\tTest: 63.04%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3084\t Train: 73.36%\t Valid: 72.89%\tTest: 63.76%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3104\t Train: 73.20%\t Valid: 70.57%\tTest: 60.71%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3066\t Train: 73.44%\t Valid: 72.86%\tTest: 63.81%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3057\t Train: 74.22%\t Valid: 72.46%\tTest: 62.94%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3085\t Train: 73.39%\t Valid: 72.28%\tTest: 62.34%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3043\t Train: 74.75%\t Valid: 73.77%\tTest: 65.15%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.3043\t Train: 74.84%\t Valid: 73.71%\tTest: 64.60%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.3028\t Train: 75.24%\t Valid: 74.29%\tTest: 65.62%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.3025\t Train: 75.34%\t Valid: 74.45%\tTest: 66.10%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.3023\t Train: 75.64%\t Valid: 74.76%\tTest: 66.27%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.3021\t Train: 75.80%\t Valid: 74.90%\tTest: 66.88%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.3017\t Train: 74.67%\t Valid: 73.13%\tTest: 63.94%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.3011\t Train: 74.88%\t Valid: 73.20%\tTest: 64.58%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2999\t Train: 76.29%\t Valid: 75.23%\tTest: 67.39%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2995\t Train: 76.60%\t Valid: 75.01%\tTest: 67.05%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.3013\t Train: 76.16%\t Valid: 73.71%\tTest: 65.77%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2976\t Train: 76.54%\t Valid: 75.25%\tTest: 67.64%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2970\t Train: 77.22%\t Valid: 75.73%\tTest: 68.47%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2974\t Train: 77.44%\t Valid: 75.27%\tTest: 68.02%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.3011\t Train: 76.78%\t Valid: 74.98%\tTest: 67.71%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2954\t Train: 77.29%\t Valid: 74.97%\tTest: 67.44%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2941\t Train: 77.73%\t Valid: 75.48%\tTest: 68.22%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2934\t Train: 77.82%\t Valid: 76.05%\tTest: 68.94%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.3012\t Train: 76.44%\t Valid: 75.59%\tTest: 68.33%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2946\t Train: 77.44%\t Valid: 75.59%\tTest: 68.38%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2932\t Train: 78.27%\t Valid: 76.41%\tTest: 69.29%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2911\t Train: 78.49%\t Valid: 76.55%\tTest: 69.41%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2910\t Train: 78.79%\t Valid: 76.57%\tTest: 69.65%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2897\t Train: 79.21%\t Valid: 77.15%\tTest: 70.30%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.3045\t Train: 77.52%\t Valid: 75.76%\tTest: 69.32%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2950\t Train: 77.80%\t Valid: 75.84%\tTest: 69.25%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2915\t Train: 78.54%\t Valid: 76.76%\tTest: 69.73%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2894\t Train: 79.16%\t Valid: 77.04%\tTest: 70.38%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2915\t Train: 79.27%\t Valid: 77.21%\tTest: 70.30%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2885\t Train: 79.29%\t Valid: 77.45%\tTest: 70.85%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2870\t Train: 79.70%\t Valid: 77.56%\tTest: 70.95%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2878\t Train: 79.58%\t Valid: 77.44%\tTest: 70.79%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2854\t Train: 80.04%\t Valid: 77.70%\tTest: 71.13%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2848\t Train: 80.41%\t Valid: 77.87%\tTest: 71.29%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2914\t Train: 79.67%\t Valid: 77.86%\tTest: 71.09%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2879\t Train: 80.06%\t Valid: 77.65%\tTest: 71.45%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2845\t Train: 80.42%\t Valid: 77.75%\tTest: 71.46%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2844\t Train: 80.58%\t Valid: 77.89%\tTest: 71.13%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2863\t Train: 80.64%\t Valid: 78.04%\tTest: 71.97%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2814\t Train: 80.80%\t Valid: 77.49%\tTest: 71.17%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2816\t Train: 81.05%\t Valid: 78.25%\tTest: 71.95%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2818\t Train: 81.30%\t Valid: 78.18%\tTest: 72.04%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2857\t Train: 80.93%\t Valid: 77.94%\tTest: 72.24%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2835\t Train: 81.03%\t Valid: 78.07%\tTest: 72.03%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2806\t Train: 81.42%\t Valid: 78.36%\tTest: 72.46%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2789\t Train: 81.41%\t Valid: 78.04%\tTest: 72.16%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2816\t Train: 81.61%\t Valid: 78.69%\tTest: 72.43%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2783\t Train: 81.62%\t Valid: 78.35%\tTest: 72.33%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2819\t Train: 81.52%\t Valid: 78.45%\tTest: 72.47%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2779\t Train: 81.89%\t Valid: 78.65%\tTest: 72.86%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2761\t Train: 81.99%\t Valid: 78.48%\tTest: 72.55%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2749\t Train: 82.34%\t Valid: 78.66%\tTest: 72.66%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2926\t Train: 79.13%\t Valid: 77.16%\tTest: 71.50%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2841\t Train: 81.37%\t Valid: 78.46%\tTest: 72.76%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2787\t Train: 81.68%\t Valid: 78.64%\tTest: 72.85%\n",
      "Seed =  9\n",
      "Run: 01\t Epoch: 10\t Loss: 0.5511\t Train: 38.23%\t Valid: 36.54%\tTest: 39.52%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4177\t Train: 36.20%\t Valid: 33.92%\tTest: 36.99%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4126\t Train: 36.86%\t Valid: 33.89%\tTest: 36.57%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3999\t Train: 38.64%\t Valid: 34.49%\tTest: 36.50%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3892\t Train: 43.75%\t Valid: 35.28%\tTest: 36.86%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3817\t Train: 49.82%\t Valid: 36.78%\tTest: 37.47%\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3766\t Train: 49.81%\t Valid: 38.06%\tTest: 37.63%\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3699\t Train: 53.44%\t Valid: 42.71%\tTest: 38.96%\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3640\t Train: 55.87%\t Valid: 46.86%\tTest: 40.89%\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3584\t Train: 57.12%\t Valid: 49.30%\tTest: 42.09%\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3520\t Train: 59.27%\t Valid: 52.74%\tTest: 44.09%\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3488\t Train: 60.07%\t Valid: 53.42%\tTest: 44.89%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3488\t Train: 59.83%\t Valid: 52.53%\tTest: 44.21%\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3442\t Train: 61.77%\t Valid: 56.34%\tTest: 46.19%\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3406\t Train: 63.04%\t Valid: 58.87%\tTest: 47.57%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3379\t Train: 63.92%\t Valid: 59.94%\tTest: 48.46%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3452\t Train: 62.64%\t Valid: 53.73%\tTest: 46.92%\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3421\t Train: 62.94%\t Valid: 53.88%\tTest: 46.67%\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3355\t Train: 64.71%\t Valid: 58.28%\tTest: 49.22%\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3329\t Train: 65.97%\t Valid: 62.30%\tTest: 50.39%\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3294\t Train: 67.10%\t Valid: 64.59%\tTest: 53.22%\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3593\t Train: 66.78%\t Valid: 61.47%\tTest: 54.70%\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3471\t Train: 58.71%\t Valid: 49.43%\tTest: 44.55%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3357\t Train: 63.66%\t Valid: 56.07%\tTest: 49.40%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3331\t Train: 65.21%\t Valid: 58.75%\tTest: 50.87%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3297\t Train: 66.81%\t Valid: 61.13%\tTest: 52.41%\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3271\t Train: 67.30%\t Valid: 62.45%\tTest: 53.05%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3249\t Train: 68.35%\t Valid: 64.42%\tTest: 54.62%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3233\t Train: 68.28%\t Valid: 64.79%\tTest: 54.69%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3216\t Train: 68.84%\t Valid: 66.34%\tTest: 55.98%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3271\t Train: 67.31%\t Valid: 64.17%\tTest: 54.41%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3220\t Train: 68.17%\t Valid: 64.87%\tTest: 54.50%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3195\t Train: 69.26%\t Valid: 67.49%\tTest: 56.29%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3179\t Train: 69.95%\t Valid: 68.83%\tTest: 57.58%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3164\t Train: 70.43%\t Valid: 69.38%\tTest: 58.33%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3171\t Train: 70.93%\t Valid: 70.22%\tTest: 59.42%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3150\t Train: 71.21%\t Valid: 69.84%\tTest: 58.97%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3139\t Train: 71.57%\t Valid: 70.31%\tTest: 59.50%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3136\t Train: 72.08%\t Valid: 71.04%\tTest: 60.55%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3128\t Train: 72.37%\t Valid: 71.46%\tTest: 61.31%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3115\t Train: 71.53%\t Valid: 69.73%\tTest: 58.68%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3107\t Train: 72.44%\t Valid: 71.35%\tTest: 61.20%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3102\t Train: 72.66%\t Valid: 71.02%\tTest: 60.28%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3095\t Train: 72.99%\t Valid: 71.36%\tTest: 60.59%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3086\t Train: 73.07%\t Valid: 71.25%\tTest: 60.29%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3082\t Train: 73.20%\t Valid: 71.88%\tTest: 62.08%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.3089\t Train: 73.18%\t Valid: 70.90%\tTest: 59.93%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.3074\t Train: 74.06%\t Valid: 72.31%\tTest: 62.01%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.3065\t Train: 74.03%\t Valid: 72.09%\tTest: 61.40%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.3066\t Train: 74.05%\t Valid: 72.43%\tTest: 62.53%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.3062\t Train: 74.20%\t Valid: 72.60%\tTest: 62.56%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.3050\t Train: 73.95%\t Valid: 72.19%\tTest: 61.57%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.3043\t Train: 74.61%\t Valid: 72.56%\tTest: 61.82%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.3055\t Train: 74.55%\t Valid: 72.43%\tTest: 61.52%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.3030\t Train: 75.10%\t Valid: 72.83%\tTest: 62.25%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.3033\t Train: 74.41%\t Valid: 72.22%\tTest: 60.88%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.3035\t Train: 74.80%\t Valid: 72.52%\tTest: 61.35%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.3022\t Train: 75.41%\t Valid: 73.32%\tTest: 62.83%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.3016\t Train: 75.60%\t Valid: 73.82%\tTest: 64.34%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.3022\t Train: 74.42%\t Valid: 71.31%\tTest: 59.60%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.3012\t Train: 75.49%\t Valid: 74.00%\tTest: 64.75%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.3001\t Train: 75.83%\t Valid: 74.04%\tTest: 64.34%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2998\t Train: 75.83%\t Valid: 73.42%\tTest: 62.38%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2990\t Train: 76.17%\t Valid: 74.25%\tTest: 64.10%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.3026\t Train: 75.52%\t Valid: 71.80%\tTest: 59.91%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.3007\t Train: 75.88%\t Valid: 73.62%\tTest: 62.63%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2989\t Train: 76.34%\t Valid: 74.31%\tTest: 64.19%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2976\t Train: 76.53%\t Valid: 74.75%\tTest: 65.12%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2972\t Train: 76.91%\t Valid: 74.88%\tTest: 65.03%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2976\t Train: 76.68%\t Valid: 74.64%\tTest: 64.32%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2988\t Train: 76.88%\t Valid: 74.43%\tTest: 63.72%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2970\t Train: 77.14%\t Valid: 75.27%\tTest: 65.96%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2959\t Train: 77.12%\t Valid: 75.11%\tTest: 65.16%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2963\t Train: 77.69%\t Valid: 75.53%\tTest: 66.40%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2960\t Train: 77.30%\t Valid: 74.43%\tTest: 63.41%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2968\t Train: 77.34%\t Valid: 74.62%\tTest: 64.03%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2959\t Train: 77.78%\t Valid: 75.46%\tTest: 65.88%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2949\t Train: 77.66%\t Valid: 75.32%\tTest: 65.93%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2937\t Train: 77.98%\t Valid: 75.81%\tTest: 67.09%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2935\t Train: 77.75%\t Valid: 75.51%\tTest: 66.01%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2930\t Train: 77.90%\t Valid: 75.60%\tTest: 66.82%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2928\t Train: 78.07%\t Valid: 75.70%\tTest: 66.98%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2919\t Train: 78.40%\t Valid: 75.78%\tTest: 67.15%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2927\t Train: 78.27%\t Valid: 75.47%\tTest: 65.96%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2944\t Train: 78.55%\t Valid: 75.68%\tTest: 66.43%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2916\t Train: 78.56%\t Valid: 75.90%\tTest: 67.32%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2913\t Train: 78.93%\t Valid: 76.43%\tTest: 68.96%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2896\t Train: 79.09%\t Valid: 76.51%\tTest: 68.91%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2912\t Train: 78.97%\t Valid: 76.44%\tTest: 69.02%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2895\t Train: 79.09%\t Valid: 76.50%\tTest: 68.87%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2890\t Train: 79.37%\t Valid: 76.32%\tTest: 68.29%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2885\t Train: 79.44%\t Valid: 76.85%\tTest: 69.80%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2885\t Train: 79.46%\t Valid: 76.96%\tTest: 70.56%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2878\t Train: 79.50%\t Valid: 76.96%\tTest: 70.20%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2880\t Train: 79.74%\t Valid: 76.97%\tTest: 70.54%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2863\t Train: 79.79%\t Valid: 77.04%\tTest: 70.26%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2875\t Train: 79.72%\t Valid: 76.83%\tTest: 69.41%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2889\t Train: 79.97%\t Valid: 77.27%\tTest: 71.28%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2858\t Train: 80.23%\t Valid: 77.26%\tTest: 70.86%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2888\t Train: 79.78%\t Valid: 77.33%\tTest: 70.79%\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute GCN normalization.\n",
    "adj_t = data.adj_t.set_diag()\n",
    "deg = adj_t.sum(dim=1).to(torch.float)\n",
    "deg_inv_sqrt = deg.pow(-0.5)\n",
    "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
    "data.adj_t = adj_t\n",
    "    \n",
    "evaluator = Evaluator(name='ogbn-proteins')\n",
    "logger = Logger(args.runs, args)\n",
    "best_test_score = 0\n",
    "\n",
    "\n",
    "for seed in range(10):\n",
    "    print(\"Seed = \",seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    model = GCN(data.num_features, args.hidden_channels, 112, args.num_layers, args.dropout).to(device)\n",
    "    model.reset_parameters()\n",
    "    for run in range(args.runs):        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "\n",
    "            loss = train(model, data, train_idx, optimizer)\n",
    "\n",
    "            if epoch % args.eval_steps == 0:\n",
    "                result = test(model, data, split_idx, evaluator)\n",
    "                logger.add_result(run, result)\n",
    "\n",
    "                if epoch % args.log_steps == 0:                \n",
    "                    train_rocauc, valid_rocauc, test_rocauc = result\n",
    "                    print(f'Run: {run + 1:02d}\\t '\n",
    "                          f'Epoch: {epoch:02d}\\t '\n",
    "                          f'Loss: {loss:.4f}\\t '\n",
    "                          f'Train: {100 * train_rocauc:.2f}%\\t '\n",
    "                          f'Valid: {100 * valid_rocauc:.2f}%\\t'\n",
    "                          f'Test: {100 * test_rocauc:.2f}%')\n",
    "                    if(test_rocauc > best_test_score):\n",
    "                        best_test_score = test_rocauc\n",
    "                        save_path = \"gcn.pth\"\n",
    "                        torch.save(model, save_path)\n",
    "                        print(\"Model saved.\")\n",
    "        # logger.print_statistics(run)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.33782905428401\n"
     ]
    }
   ],
   "source": [
    "print(best_test_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 hours\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, optimizer\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
