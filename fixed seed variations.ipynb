{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    num_layers = 3\n",
    "    device = 'cuda:0'\n",
    "    log_steps = 1\n",
    "    hidden_channels = 64*4\n",
    "    dropout = 0.2\n",
    "    lr = 0.01\n",
    "    epochs = 1000\n",
    "    eval_steps = 10\n",
    "    runs = 10\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import humanize\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\r\n",
      "  Downloading ogb-1.2.1-py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 1.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.1)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.1)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.0.3)\r\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.18.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Installing collected packages: ogb\r\n",
      "Successfully installed ogb-1.2.1\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-scatter==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 932 kB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\r\n",
      "Successfully installed torch-scatter-2.0.5\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-sparse==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.6 MB 3.0 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==latest+cu101) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==latest+cu101) (1.18.5)\r\n",
      "Installing collected packages: torch-sparse\r\n",
      "Successfully installed torch-sparse-0.6.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-cluster==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.3 MB 3.1 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-cluster\r\n",
      "Successfully installed torch-cluster-1.5.6\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-spline-conv==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 2.7 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\r\n",
      "Successfully installed torch-spline-conv-1.2.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-1.6.0.tar.gz (172 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 172 kB 5.0 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.5.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.18.5)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (4.45.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.4.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.23.1)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.48.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.23.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.0.3)\r\n",
      "Collecting rdflib\r\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 231 kB 11.0 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.10.0)\r\n",
      "Collecting googledrivedownloader\r\n",
      "  Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB)\r\n",
      "Collecting ase\r\n",
      "  Downloading ase-3.19.2-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 14.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.11.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torch-geometric) (0.18.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->torch-geometric) (4.4.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (2.1.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (0.31.0)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (1.24.3)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2020.6.20)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2.8.1)\r\n",
      "Collecting isodate\r\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 3.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (1.14.0)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (2.4.7)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ase->torch-geometric) (3.2.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric) (1.1.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (1.2.0)\r\n",
      "Building wheels for collected packages: torch-geometric\r\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-1.6.0-py3-none-any.whl size=296336 sha256=f8795650b2105f8e38615def0d1afdb72e1cf10d6ea82fe1bb889ed9ef0981bd\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/69/d6/b8ed45222466048e1efc27af604aded6825217f0caa6dff569\r\n",
      "Successfully built torch-geometric\r\n",
      "Installing collected packages: isodate, rdflib, googledrivedownloader, ase, torch-geometric\r\n",
      "Successfully installed ase-3.19.2 googledrivedownloader-0.4 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# install Open Graph Benchmark\n",
    "!pip install ogb\n",
    "\n",
    "# install PyTorch Geometric\n",
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR,CosineAnnealingLR\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/216 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://snap.stanford.edu/ogb/data/nodeproppred/proteinfunc.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:05<00:00, 38.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/proteinfunc.zip\n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 420.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n",
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = f'{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-proteins',\n",
    "                                 transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "\n",
    "# Move edge features to node features.\n",
    "data.x = data.adj_t.mean(dim=1)\n",
    "data.adj_t.set_value_(None)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx = split_idx['train'].to(device)\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 minutes\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            GCNConv(in_channels, hidden_channels, normalize=False))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, normalize=False))\n",
    "        self.convs.append(\n",
    "            GCNConv(hidden_channels, out_channels, normalize=False))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        x = self.convs[0](x, adj_t)\n",
    "        for conv in self.convs[1:-1]:\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = criterion(out, data.y[train_idx].to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(data.x, data.adj_t)\n",
    "\n",
    "    train_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['rocauc']\n",
    "    valid_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['rocauc']\n",
    "    test_rocauc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['rocauc']\n",
    "\n",
    "    return train_rocauc, valid_rocauc, test_rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed =  0\n",
      "Run: 01\t Epoch: 10\t Loss: 0.4255\t Train: 37.38%\t Valid: 34.81%\tTest: 37.36%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4174\t Train: 38.24%\t Valid: 33.55%\tTest: 35.76%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4001\t Train: 41.97%\t Valid: 33.81%\tTest: 34.89%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3863\t Train: 47.66%\t Valid: 36.32%\tTest: 35.61%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3733\t Train: 54.59%\t Valid: 43.88%\tTest: 40.97%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3658\t Train: 56.07%\t Valid: 47.09%\tTest: 42.74%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3577\t Train: 57.57%\t Valid: 49.81%\tTest: 44.64%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3481\t Train: 60.80%\t Valid: 53.35%\tTest: 47.08%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3395\t Train: 64.35%\t Valid: 58.79%\tTest: 50.22%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3332\t Train: 66.65%\t Valid: 63.09%\tTest: 52.71%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3313\t Train: 68.67%\t Valid: 66.44%\tTest: 56.16%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3258\t Train: 68.04%\t Valid: 65.67%\tTest: 54.04%\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3231\t Train: 69.82%\t Valid: 68.45%\tTest: 56.27%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3207\t Train: 71.25%\t Valid: 70.23%\tTest: 57.75%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3238\t Train: 69.64%\t Valid: 67.56%\tTest: 54.86%\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3164\t Train: 71.21%\t Valid: 69.64%\tTest: 57.60%\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3141\t Train: 72.06%\t Valid: 70.62%\tTest: 58.03%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3120\t Train: 72.57%\t Valid: 71.20%\tTest: 58.12%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3108\t Train: 72.53%\t Valid: 70.90%\tTest: 58.35%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3092\t Train: 74.11%\t Valid: 72.90%\tTest: 60.40%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3068\t Train: 74.78%\t Valid: 73.72%\tTest: 61.24%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3058\t Train: 75.47%\t Valid: 74.13%\tTest: 62.09%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3040\t Train: 75.07%\t Valid: 73.28%\tTest: 60.00%\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3021\t Train: 75.67%\t Valid: 73.83%\tTest: 60.70%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3011\t Train: 75.59%\t Valid: 73.40%\tTest: 60.08%\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3021\t Train: 77.02%\t Valid: 75.52%\tTest: 63.45%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 270\t Loss: 0.2978\t Train: 76.43%\t Valid: 73.76%\tTest: 59.72%\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2972\t Train: 77.11%\t Valid: 74.66%\tTest: 60.66%\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2990\t Train: 76.96%\t Valid: 74.57%\tTest: 60.58%\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2942\t Train: 77.31%\t Valid: 75.14%\tTest: 60.85%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2935\t Train: 77.94%\t Valid: 75.41%\tTest: 60.68%\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2993\t Train: 77.19%\t Valid: 74.76%\tTest: 62.07%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2916\t Train: 78.05%\t Valid: 75.04%\tTest: 60.21%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2901\t Train: 79.00%\t Valid: 76.25%\tTest: 61.61%\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2886\t Train: 78.90%\t Valid: 75.49%\tTest: 59.99%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2880\t Train: 79.45%\t Valid: 76.28%\tTest: 60.98%\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2889\t Train: 79.88%\t Valid: 76.58%\tTest: 60.87%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2856\t Train: 79.30%\t Valid: 75.52%\tTest: 58.72%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2849\t Train: 79.42%\t Valid: 75.39%\tTest: 58.17%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2867\t Train: 79.99%\t Valid: 76.23%\tTest: 59.78%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2836\t Train: 80.23%\t Valid: 76.17%\tTest: 59.22%\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2828\t Train: 79.71%\t Valid: 75.26%\tTest: 58.28%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2827\t Train: 80.44%\t Valid: 76.66%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2816\t Train: 80.90%\t Valid: 76.90%\tTest: 60.80%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2802\t Train: 80.50%\t Valid: 75.50%\tTest: 58.55%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2802\t Train: 81.02%\t Valid: 76.83%\tTest: 61.20%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2786\t Train: 81.04%\t Valid: 76.77%\tTest: 61.08%\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2781\t Train: 81.43%\t Valid: 77.01%\tTest: 60.68%\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2773\t Train: 81.16%\t Valid: 76.62%\tTest: 60.42%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2763\t Train: 81.67%\t Valid: 77.32%\tTest: 61.62%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2766\t Train: 81.25%\t Valid: 76.54%\tTest: 60.89%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2753\t Train: 81.45%\t Valid: 77.10%\tTest: 61.18%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2756\t Train: 81.70%\t Valid: 77.17%\tTest: 61.14%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2744\t Train: 81.83%\t Valid: 77.04%\tTest: 61.17%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2733\t Train: 81.93%\t Valid: 77.01%\tTest: 60.78%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2745\t Train: 82.16%\t Valid: 77.52%\tTest: 61.73%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2757\t Train: 81.49%\t Valid: 75.96%\tTest: 59.51%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2734\t Train: 82.27%\t Valid: 77.33%\tTest: 61.56%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2767\t Train: 82.21%\t Valid: 77.06%\tTest: 60.98%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2724\t Train: 82.22%\t Valid: 77.05%\tTest: 60.94%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2718\t Train: 82.60%\t Valid: 77.91%\tTest: 62.41%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2723\t Train: 82.61%\t Valid: 77.94%\tTest: 62.15%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2704\t Train: 82.51%\t Valid: 77.59%\tTest: 61.91%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2698\t Train: 82.68%\t Valid: 77.85%\tTest: 62.15%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2695\t Train: 82.76%\t Valid: 77.71%\tTest: 61.84%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2696\t Train: 82.85%\t Valid: 77.74%\tTest: 61.57%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2691\t Train: 82.90%\t Valid: 77.97%\tTest: 62.07%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2694\t Train: 82.85%\t Valid: 77.67%\tTest: 61.98%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2707\t Train: 82.32%\t Valid: 75.97%\tTest: 59.63%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2687\t Train: 83.09%\t Valid: 77.96%\tTest: 62.06%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2678\t Train: 83.20%\t Valid: 77.93%\tTest: 61.99%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2675\t Train: 83.16%\t Valid: 77.78%\tTest: 61.67%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2669\t Train: 83.19%\t Valid: 77.86%\tTest: 61.69%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2690\t Train: 83.01%\t Valid: 77.69%\tTest: 61.03%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2668\t Train: 83.39%\t Valid: 78.06%\tTest: 61.92%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2660\t Train: 83.53%\t Valid: 77.75%\tTest: 61.09%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2665\t Train: 83.52%\t Valid: 78.64%\tTest: 62.87%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2672\t Train: 83.51%\t Valid: 78.07%\tTest: 61.59%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2667\t Train: 83.46%\t Valid: 77.06%\tTest: 60.48%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2653\t Train: 83.67%\t Valid: 78.00%\tTest: 62.10%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2652\t Train: 83.80%\t Valid: 78.24%\tTest: 62.40%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2652\t Train: 83.74%\t Valid: 78.44%\tTest: 61.99%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2649\t Train: 83.83%\t Valid: 78.40%\tTest: 62.25%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2645\t Train: 83.82%\t Valid: 78.25%\tTest: 62.60%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2639\t Train: 84.02%\t Valid: 78.38%\tTest: 62.17%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2651\t Train: 84.08%\t Valid: 78.54%\tTest: 62.46%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2683\t Train: 83.93%\t Valid: 77.51%\tTest: 60.45%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2638\t Train: 83.92%\t Valid: 77.91%\tTest: 62.17%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2635\t Train: 84.04%\t Valid: 78.18%\tTest: 61.83%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2642\t Train: 83.88%\t Valid: 77.84%\tTest: 61.45%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2631\t Train: 84.22%\t Valid: 78.70%\tTest: 63.04%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2624\t Train: 84.13%\t Valid: 78.29%\tTest: 61.97%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2626\t Train: 84.24%\t Valid: 78.15%\tTest: 61.51%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2626\t Train: 84.16%\t Valid: 77.47%\tTest: 61.12%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2625\t Train: 84.43%\t Valid: 78.24%\tTest: 62.24%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2622\t Train: 84.18%\t Valid: 78.35%\tTest: 61.87%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2621\t Train: 84.42%\t Valid: 78.49%\tTest: 62.99%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2613\t Train: 84.49%\t Valid: 78.14%\tTest: 62.08%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2624\t Train: 84.55%\t Valid: 78.64%\tTest: 62.74%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2622\t Train: 84.51%\t Valid: 78.18%\tTest: 61.58%\n",
      "Run 01:\n",
      "Highest Train: 84.55\n",
      "Highest Valid: 78.70\n",
      "  Final Train: 84.22\n",
      "   Final Test: 63.04\n",
      "Run: 02\t Epoch: 10\t Loss: 0.4247\t Train: 37.85%\t Valid: 35.01%\tTest: 37.22%\n",
      "Run: 02\t Epoch: 20\t Loss: 0.4169\t Train: 38.86%\t Valid: 34.14%\tTest: 36.17%\n",
      "Run: 02\t Epoch: 30\t Loss: 0.4000\t Train: 42.10%\t Valid: 33.81%\tTest: 34.86%\n",
      "Run: 02\t Epoch: 40\t Loss: 0.3861\t Train: 47.88%\t Valid: 36.50%\tTest: 35.74%\n",
      "Run: 02\t Epoch: 50\t Loss: 0.3733\t Train: 54.79%\t Valid: 44.50%\tTest: 41.37%\n",
      "Run: 02\t Epoch: 60\t Loss: 0.3658\t Train: 56.63%\t Valid: 47.99%\tTest: 43.25%\n",
      "Run: 02\t Epoch: 70\t Loss: 0.3585\t Train: 57.54%\t Valid: 49.86%\tTest: 44.68%\n",
      "Run: 02\t Epoch: 80\t Loss: 0.3504\t Train: 59.73%\t Valid: 52.59%\tTest: 46.84%\n",
      "Run: 02\t Epoch: 90\t Loss: 0.3411\t Train: 63.72%\t Valid: 58.28%\tTest: 50.82%\n",
      "Run: 02\t Epoch: 100\t Loss: 0.3339\t Train: 66.39%\t Valid: 63.56%\tTest: 54.26%\n",
      "Run: 02\t Epoch: 110\t Loss: 0.3291\t Train: 67.65%\t Valid: 65.90%\tTest: 55.61%\n",
      "Run: 02\t Epoch: 120\t Loss: 0.3245\t Train: 69.67%\t Valid: 68.55%\tTest: 57.48%\n",
      "Run: 02\t Epoch: 130\t Loss: 0.3218\t Train: 71.29%\t Valid: 70.35%\tTest: 58.97%\n",
      "Run: 02\t Epoch: 140\t Loss: 0.3192\t Train: 72.01%\t Valid: 71.25%\tTest: 59.81%\n",
      "Run: 02\t Epoch: 150\t Loss: 0.3173\t Train: 72.63%\t Valid: 71.94%\tTest: 60.38%\n",
      "Run: 02\t Epoch: 160\t Loss: 0.3147\t Train: 73.08%\t Valid: 72.39%\tTest: 60.42%\n",
      "Run: 02\t Epoch: 170\t Loss: 0.3134\t Train: 72.08%\t Valid: 70.63%\tTest: 57.86%\n",
      "Run: 02\t Epoch: 180\t Loss: 0.3114\t Train: 72.94%\t Valid: 71.57%\tTest: 58.68%\n",
      "Run: 02\t Epoch: 190\t Loss: 0.3084\t Train: 73.74%\t Valid: 72.44%\tTest: 59.47%\n",
      "Run: 02\t Epoch: 200\t Loss: 0.3073\t Train: 73.63%\t Valid: 71.79%\tTest: 58.47%\n",
      "Run: 02\t Epoch: 210\t Loss: 0.3056\t Train: 75.01%\t Valid: 73.80%\tTest: 60.85%\n",
      "Run: 02\t Epoch: 220\t Loss: 0.3037\t Train: 75.53%\t Valid: 74.07%\tTest: 60.97%\n",
      "Run: 02\t Epoch: 230\t Loss: 0.3089\t Train: 74.15%\t Valid: 72.49%\tTest: 60.64%\n",
      "Run: 02\t Epoch: 240\t Loss: 0.3037\t Train: 75.69%\t Valid: 73.86%\tTest: 61.54%\n",
      "Run: 02\t Epoch: 250\t Loss: 0.3003\t Train: 76.50%\t Valid: 74.35%\tTest: 60.33%\n",
      "Run: 02\t Epoch: 260\t Loss: 0.2988\t Train: 76.34%\t Valid: 73.41%\tTest: 59.10%\n",
      "Run: 02\t Epoch: 270\t Loss: 0.2981\t Train: 77.80%\t Valid: 75.52%\tTest: 62.35%\n",
      "Run: 02\t Epoch: 280\t Loss: 0.2963\t Train: 78.15%\t Valid: 75.91%\tTest: 62.86%\n",
      "Run: 02\t Epoch: 290\t Loss: 0.2955\t Train: 78.17%\t Valid: 75.46%\tTest: 61.05%\n",
      "Run: 02\t Epoch: 300\t Loss: 0.2933\t Train: 78.45%\t Valid: 75.14%\tTest: 59.85%\n",
      "Run: 02\t Epoch: 310\t Loss: 0.2921\t Train: 78.29%\t Valid: 75.28%\tTest: 60.68%\n",
      "Run: 02\t Epoch: 320\t Loss: 0.2934\t Train: 79.21%\t Valid: 76.31%\tTest: 61.48%\n",
      "Run: 02\t Epoch: 330\t Loss: 0.2917\t Train: 78.86%\t Valid: 76.26%\tTest: 62.35%\n",
      "Run: 02\t Epoch: 340\t Loss: 0.2902\t Train: 78.87%\t Valid: 75.35%\tTest: 59.52%\n",
      "Run: 02\t Epoch: 350\t Loss: 0.2879\t Train: 79.19%\t Valid: 75.46%\tTest: 58.84%\n",
      "Run: 02\t Epoch: 360\t Loss: 0.2869\t Train: 79.36%\t Valid: 75.59%\tTest: 59.59%\n",
      "Run: 02\t Epoch: 370\t Loss: 0.2855\t Train: 79.55%\t Valid: 75.74%\tTest: 59.77%\n",
      "Run: 02\t Epoch: 380\t Loss: 0.2854\t Train: 80.18%\t Valid: 76.43%\tTest: 60.11%\n",
      "Run: 02\t Epoch: 390\t Loss: 0.2834\t Train: 80.09%\t Valid: 76.03%\tTest: 59.39%\n",
      "Run: 02\t Epoch: 400\t Loss: 0.2902\t Train: 80.08%\t Valid: 76.86%\tTest: 62.46%\n",
      "Run: 02\t Epoch: 410\t Loss: 0.2864\t Train: 80.69%\t Valid: 76.82%\tTest: 59.37%\n",
      "Run: 02\t Epoch: 420\t Loss: 0.2823\t Train: 80.57%\t Valid: 76.66%\tTest: 59.32%\n",
      "Run: 02\t Epoch: 430\t Loss: 0.2817\t Train: 80.73%\t Valid: 76.76%\tTest: 59.67%\n",
      "Run: 02\t Epoch: 440\t Loss: 0.2802\t Train: 80.78%\t Valid: 76.68%\tTest: 59.14%\n",
      "Run: 02\t Epoch: 450\t Loss: 0.2792\t Train: 80.90%\t Valid: 76.80%\tTest: 59.39%\n",
      "Run: 02\t Epoch: 460\t Loss: 0.2797\t Train: 81.17%\t Valid: 76.83%\tTest: 59.05%\n",
      "Run: 02\t Epoch: 470\t Loss: 0.2797\t Train: 81.37%\t Valid: 77.14%\tTest: 59.95%\n",
      "Run: 02\t Epoch: 480\t Loss: 0.2863\t Train: 80.85%\t Valid: 77.54%\tTest: 61.46%\n",
      "Run: 02\t Epoch: 490\t Loss: 0.2782\t Train: 81.29%\t Valid: 76.51%\tTest: 58.18%\n",
      "Run: 02\t Epoch: 500\t Loss: 0.2778\t Train: 81.28%\t Valid: 76.45%\tTest: 58.96%\n",
      "Run: 02\t Epoch: 510\t Loss: 0.2766\t Train: 81.54%\t Valid: 77.09%\tTest: 60.12%\n",
      "Run: 02\t Epoch: 520\t Loss: 0.2761\t Train: 81.87%\t Valid: 77.67%\tTest: 60.64%\n",
      "Run: 02\t Epoch: 530\t Loss: 0.2756\t Train: 81.69%\t Valid: 76.89%\tTest: 59.16%\n",
      "Run: 02\t Epoch: 540\t Loss: 0.2753\t Train: 81.30%\t Valid: 76.08%\tTest: 58.21%\n",
      "Run: 02\t Epoch: 550\t Loss: 0.2747\t Train: 81.91%\t Valid: 76.53%\tTest: 58.70%\n",
      "Run: 02\t Epoch: 560\t Loss: 0.2743\t Train: 81.72%\t Valid: 76.34%\tTest: 58.59%\n",
      "Run: 02\t Epoch: 570\t Loss: 0.2735\t Train: 82.15%\t Valid: 76.89%\tTest: 59.32%\n",
      "Run: 02\t Epoch: 580\t Loss: 0.2728\t Train: 81.94%\t Valid: 76.65%\tTest: 58.89%\n",
      "Run: 02\t Epoch: 590\t Loss: 0.2723\t Train: 82.42%\t Valid: 76.93%\tTest: 59.08%\n",
      "Run: 02\t Epoch: 600\t Loss: 0.2721\t Train: 82.46%\t Valid: 77.37%\tTest: 59.84%\n",
      "Run: 02\t Epoch: 610\t Loss: 0.2713\t Train: 82.42%\t Valid: 77.07%\tTest: 59.79%\n",
      "Run: 02\t Epoch: 620\t Loss: 0.2711\t Train: 82.60%\t Valid: 77.31%\tTest: 59.97%\n",
      "Run: 02\t Epoch: 630\t Loss: 0.2711\t Train: 82.41%\t Valid: 76.94%\tTest: 59.31%\n",
      "Run: 02\t Epoch: 640\t Loss: 0.2705\t Train: 82.41%\t Valid: 76.83%\tTest: 59.22%\n",
      "Run: 02\t Epoch: 650\t Loss: 0.2726\t Train: 82.46%\t Valid: 77.68%\tTest: 60.84%\n",
      "Run: 02\t Epoch: 660\t Loss: 0.2729\t Train: 82.73%\t Valid: 77.34%\tTest: 59.61%\n",
      "Run: 02\t Epoch: 670\t Loss: 0.2702\t Train: 83.14%\t Valid: 77.66%\tTest: 60.32%\n",
      "Run: 02\t Epoch: 680\t Loss: 0.2691\t Train: 82.88%\t Valid: 77.45%\tTest: 60.05%\n",
      "Run: 02\t Epoch: 690\t Loss: 0.2687\t Train: 83.08%\t Valid: 77.71%\tTest: 60.70%\n",
      "Run: 02\t Epoch: 700\t Loss: 0.2683\t Train: 82.91%\t Valid: 77.22%\tTest: 60.01%\n",
      "Run: 02\t Epoch: 710\t Loss: 0.2692\t Train: 83.05%\t Valid: 76.98%\tTest: 59.76%\n",
      "Run: 02\t Epoch: 720\t Loss: 0.2692\t Train: 83.31%\t Valid: 77.75%\tTest: 61.08%\n",
      "Run: 02\t Epoch: 730\t Loss: 0.2684\t Train: 83.33%\t Valid: 77.75%\tTest: 61.17%\n",
      "Run: 02\t Epoch: 740\t Loss: 0.2674\t Train: 83.22%\t Valid: 77.52%\tTest: 60.70%\n",
      "Run: 02\t Epoch: 750\t Loss: 0.2687\t Train: 83.31%\t Valid: 77.48%\tTest: 60.33%\n",
      "Run: 02\t Epoch: 760\t Loss: 0.2669\t Train: 83.42%\t Valid: 77.41%\tTest: 60.46%\n",
      "Run: 02\t Epoch: 770\t Loss: 0.2669\t Train: 83.50%\t Valid: 77.27%\tTest: 60.88%\n",
      "Run: 02\t Epoch: 780\t Loss: 0.2666\t Train: 83.42%\t Valid: 77.83%\tTest: 61.39%\n",
      "Run: 02\t Epoch: 790\t Loss: 0.2670\t Train: 83.68%\t Valid: 77.52%\tTest: 61.01%\n",
      "Run: 02\t Epoch: 800\t Loss: 0.2658\t Train: 83.54%\t Valid: 77.28%\tTest: 60.25%\n",
      "Run: 02\t Epoch: 810\t Loss: 0.2653\t Train: 83.71%\t Valid: 77.65%\tTest: 61.36%\n",
      "Run: 02\t Epoch: 820\t Loss: 0.2653\t Train: 83.74%\t Valid: 78.11%\tTest: 62.10%\n",
      "Run: 02\t Epoch: 830\t Loss: 0.2667\t Train: 83.78%\t Valid: 77.94%\tTest: 61.66%\n",
      "Run: 02\t Epoch: 840\t Loss: 0.2659\t Train: 83.70%\t Valid: 77.14%\tTest: 60.64%\n",
      "Run: 02\t Epoch: 850\t Loss: 0.2647\t Train: 83.88%\t Valid: 77.91%\tTest: 61.63%\n",
      "Run: 02\t Epoch: 860\t Loss: 0.2643\t Train: 83.95%\t Valid: 77.92%\tTest: 61.63%\n",
      "Run: 02\t Epoch: 870\t Loss: 0.2640\t Train: 83.92%\t Valid: 77.66%\tTest: 61.16%\n",
      "Run: 02\t Epoch: 880\t Loss: 0.2663\t Train: 84.01%\t Valid: 77.46%\tTest: 61.40%\n",
      "Run: 02\t Epoch: 890\t Loss: 0.2644\t Train: 83.69%\t Valid: 77.39%\tTest: 61.22%\n",
      "Run: 02\t Epoch: 900\t Loss: 0.2642\t Train: 83.83%\t Valid: 77.31%\tTest: 61.00%\n",
      "Run: 02\t Epoch: 910\t Loss: 0.2640\t Train: 84.12%\t Valid: 77.87%\tTest: 61.97%\n",
      "Run: 02\t Epoch: 920\t Loss: 0.2630\t Train: 84.22%\t Valid: 77.82%\tTest: 61.73%\n",
      "Run: 02\t Epoch: 930\t Loss: 0.2629\t Train: 84.17%\t Valid: 77.47%\tTest: 61.27%\n",
      "Run: 02\t Epoch: 940\t Loss: 0.2627\t Train: 84.29%\t Valid: 77.62%\tTest: 61.42%\n",
      "Run: 02\t Epoch: 950\t Loss: 0.2630\t Train: 84.22%\t Valid: 77.68%\tTest: 61.46%\n",
      "Run: 02\t Epoch: 960\t Loss: 0.2622\t Train: 84.35%\t Valid: 77.68%\tTest: 61.33%\n",
      "Run: 02\t Epoch: 970\t Loss: 0.2623\t Train: 84.28%\t Valid: 77.36%\tTest: 61.14%\n",
      "Run: 02\t Epoch: 980\t Loss: 0.2621\t Train: 84.33%\t Valid: 77.27%\tTest: 60.68%\n",
      "Run: 02\t Epoch: 990\t Loss: 0.2621\t Train: 84.41%\t Valid: 77.93%\tTest: 61.75%\n",
      "Run: 02\t Epoch: 1000\t Loss: 0.2624\t Train: 84.41%\t Valid: 78.46%\tTest: 63.14%\n",
      "Run 02:\n",
      "Highest Train: 84.41\n",
      "Highest Valid: 78.46\n",
      "  Final Train: 84.41\n",
      "   Final Test: 63.14\n",
      "Run: 03\t Epoch: 10\t Loss: 0.4263\t Train: 38.45%\t Valid: 35.22%\tTest: 37.39%\n",
      "Run: 03\t Epoch: 20\t Loss: 0.4140\t Train: 39.16%\t Valid: 33.35%\tTest: 35.31%\n",
      "Run: 03\t Epoch: 30\t Loss: 0.3981\t Train: 43.70%\t Valid: 34.26%\tTest: 34.83%\n",
      "Run: 03\t Epoch: 40\t Loss: 0.3823\t Train: 50.25%\t Valid: 38.52%\tTest: 37.11%\n",
      "Run: 03\t Epoch: 50\t Loss: 0.3725\t Train: 54.83%\t Valid: 44.83%\tTest: 41.53%\n",
      "Run: 03\t Epoch: 60\t Loss: 0.3652\t Train: 57.33%\t Valid: 49.10%\tTest: 44.14%\n",
      "Run: 03\t Epoch: 70\t Loss: 0.3586\t Train: 58.03%\t Valid: 51.00%\tTest: 45.39%\n",
      "Run: 03\t Epoch: 80\t Loss: 0.3509\t Train: 59.79%\t Valid: 53.26%\tTest: 47.22%\n",
      "Run: 03\t Epoch: 90\t Loss: 0.3426\t Train: 62.43%\t Valid: 57.02%\tTest: 49.46%\n",
      "Run: 03\t Epoch: 100\t Loss: 0.3347\t Train: 66.02%\t Valid: 63.20%\tTest: 53.17%\n",
      "Run: 03\t Epoch: 110\t Loss: 0.3303\t Train: 67.16%\t Valid: 65.60%\tTest: 54.62%\n",
      "Run: 03\t Epoch: 120\t Loss: 0.3263\t Train: 68.52%\t Valid: 67.45%\tTest: 56.07%\n",
      "Run: 03\t Epoch: 130\t Loss: 0.3242\t Train: 70.93%\t Valid: 70.65%\tTest: 58.74%\n",
      "Run: 03\t Epoch: 140\t Loss: 0.3228\t Train: 69.78%\t Valid: 68.63%\tTest: 57.06%\n",
      "Run: 03\t Epoch: 150\t Loss: 0.3195\t Train: 70.87%\t Valid: 69.94%\tTest: 57.85%\n",
      "Run: 03\t Epoch: 160\t Loss: 0.3171\t Train: 71.25%\t Valid: 70.39%\tTest: 57.62%\n",
      "Run: 03\t Epoch: 170\t Loss: 0.3154\t Train: 72.27%\t Valid: 71.57%\tTest: 58.54%\n",
      "Run: 03\t Epoch: 180\t Loss: 0.3138\t Train: 71.92%\t Valid: 70.83%\tTest: 57.36%\n",
      "Run: 03\t Epoch: 190\t Loss: 0.3128\t Train: 73.27%\t Valid: 72.30%\tTest: 59.40%\n",
      "Run: 03\t Epoch: 200\t Loss: 0.3109\t Train: 73.45%\t Valid: 72.73%\tTest: 60.05%\n",
      "Run: 03\t Epoch: 210\t Loss: 0.3091\t Train: 73.73%\t Valid: 72.67%\tTest: 59.84%\n",
      "Run: 03\t Epoch: 220\t Loss: 0.3077\t Train: 74.15%\t Valid: 73.03%\tTest: 60.32%\n",
      "Run: 03\t Epoch: 230\t Loss: 0.3067\t Train: 73.97%\t Valid: 72.45%\tTest: 59.67%\n",
      "Run: 03\t Epoch: 240\t Loss: 0.3044\t Train: 74.76%\t Valid: 73.17%\tTest: 60.23%\n",
      "Run: 03\t Epoch: 250\t Loss: 0.3032\t Train: 74.78%\t Valid: 72.79%\tTest: 60.00%\n",
      "Run: 03\t Epoch: 260\t Loss: 0.3023\t Train: 75.40%\t Valid: 73.30%\tTest: 60.20%\n",
      "Run: 03\t Epoch: 270\t Loss: 0.3008\t Train: 75.91%\t Valid: 73.96%\tTest: 61.01%\n",
      "Run: 03\t Epoch: 280\t Loss: 0.2996\t Train: 76.07%\t Valid: 73.85%\tTest: 60.58%\n",
      "Run: 03\t Epoch: 290\t Loss: 0.3003\t Train: 75.60%\t Valid: 72.92%\tTest: 60.71%\n",
      "Run: 03\t Epoch: 300\t Loss: 0.2986\t Train: 76.84%\t Valid: 74.38%\tTest: 61.45%\n",
      "Run: 03\t Epoch: 310\t Loss: 0.2966\t Train: 77.01%\t Valid: 74.51%\tTest: 61.47%\n",
      "Run: 03\t Epoch: 320\t Loss: 0.2952\t Train: 77.23%\t Valid: 74.44%\tTest: 60.77%\n",
      "Run: 03\t Epoch: 330\t Loss: 0.2952\t Train: 77.35%\t Valid: 74.17%\tTest: 60.09%\n",
      "Run: 03\t Epoch: 340\t Loss: 0.2937\t Train: 77.52%\t Valid: 74.45%\tTest: 60.84%\n",
      "Run: 03\t Epoch: 350\t Loss: 0.2924\t Train: 78.43%\t Valid: 75.20%\tTest: 61.18%\n",
      "Run: 03\t Epoch: 360\t Loss: 0.2926\t Train: 78.23%\t Valid: 74.84%\tTest: 61.02%\n",
      "Run: 03\t Epoch: 370\t Loss: 0.2912\t Train: 78.59%\t Valid: 74.68%\tTest: 60.38%\n",
      "Run: 03\t Epoch: 380\t Loss: 0.2894\t Train: 79.11%\t Valid: 75.46%\tTest: 60.69%\n",
      "Run: 03\t Epoch: 390\t Loss: 0.2885\t Train: 79.49%\t Valid: 76.05%\tTest: 61.54%\n",
      "Run: 03\t Epoch: 400\t Loss: 0.2874\t Train: 79.34%\t Valid: 75.41%\tTest: 59.66%\n",
      "Run: 03\t Epoch: 410\t Loss: 0.2868\t Train: 79.70%\t Valid: 75.94%\tTest: 60.61%\n",
      "Run: 03\t Epoch: 420\t Loss: 0.2878\t Train: 79.63%\t Valid: 75.50%\tTest: 59.38%\n",
      "Run: 03\t Epoch: 430\t Loss: 0.2865\t Train: 79.22%\t Valid: 74.93%\tTest: 59.26%\n",
      "Run: 03\t Epoch: 440\t Loss: 0.2846\t Train: 80.19%\t Valid: 76.48%\tTest: 60.22%\n",
      "Run: 03\t Epoch: 450\t Loss: 0.2840\t Train: 79.83%\t Valid: 75.45%\tTest: 59.35%\n",
      "Run: 03\t Epoch: 460\t Loss: 0.2824\t Train: 80.49%\t Valid: 76.69%\tTest: 61.55%\n",
      "Run: 03\t Epoch: 470\t Loss: 0.2839\t Train: 80.09%\t Valid: 75.37%\tTest: 58.66%\n",
      "Run: 03\t Epoch: 480\t Loss: 0.2816\t Train: 80.53%\t Valid: 76.41%\tTest: 59.66%\n",
      "Run: 03\t Epoch: 490\t Loss: 0.2808\t Train: 80.39%\t Valid: 76.05%\tTest: 59.91%\n",
      "Run: 03\t Epoch: 500\t Loss: 0.2808\t Train: 81.16%\t Valid: 76.90%\tTest: 60.60%\n",
      "Run: 03\t Epoch: 510\t Loss: 0.2800\t Train: 81.27%\t Valid: 77.20%\tTest: 61.29%\n",
      "Run: 03\t Epoch: 520\t Loss: 0.2793\t Train: 81.25%\t Valid: 76.90%\tTest: 60.68%\n",
      "Run: 03\t Epoch: 530\t Loss: 0.2783\t Train: 81.30%\t Valid: 77.11%\tTest: 61.68%\n",
      "Run: 03\t Epoch: 540\t Loss: 0.2773\t Train: 81.51%\t Valid: 77.08%\tTest: 61.07%\n",
      "Run: 03\t Epoch: 550\t Loss: 0.2804\t Train: 80.62%\t Valid: 76.08%\tTest: 59.98%\n",
      "Run: 03\t Epoch: 560\t Loss: 0.2782\t Train: 81.33%\t Valid: 77.40%\tTest: 63.24%\n",
      "Run: 03\t Epoch: 570\t Loss: 0.2770\t Train: 81.23%\t Valid: 76.06%\tTest: 59.51%\n",
      "Run: 03\t Epoch: 580\t Loss: 0.2766\t Train: 81.75%\t Valid: 76.98%\tTest: 61.16%\n",
      "Run: 03\t Epoch: 590\t Loss: 0.2752\t Train: 81.74%\t Valid: 77.37%\tTest: 61.63%\n",
      "Run: 03\t Epoch: 600\t Loss: 0.2751\t Train: 81.90%\t Valid: 77.15%\tTest: 60.83%\n",
      "Run: 03\t Epoch: 610\t Loss: 0.2743\t Train: 81.88%\t Valid: 77.26%\tTest: 61.81%\n",
      "Run: 03\t Epoch: 620\t Loss: 0.2770\t Train: 82.33%\t Valid: 77.23%\tTest: 60.87%\n",
      "Run: 03\t Epoch: 630\t Loss: 0.2806\t Train: 81.80%\t Valid: 76.90%\tTest: 60.44%\n",
      "Run: 03\t Epoch: 640\t Loss: 0.2736\t Train: 82.14%\t Valid: 76.72%\tTest: 60.96%\n",
      "Run: 03\t Epoch: 650\t Loss: 0.2734\t Train: 82.11%\t Valid: 77.23%\tTest: 60.80%\n",
      "Run: 03\t Epoch: 660\t Loss: 0.2727\t Train: 82.38%\t Valid: 77.51%\tTest: 61.60%\n",
      "Run: 03\t Epoch: 670\t Loss: 0.2724\t Train: 82.50%\t Valid: 77.52%\tTest: 61.41%\n",
      "Run: 03\t Epoch: 680\t Loss: 0.2719\t Train: 82.43%\t Valid: 77.72%\tTest: 61.48%\n",
      "Run: 03\t Epoch: 690\t Loss: 0.2717\t Train: 82.48%\t Valid: 77.85%\tTest: 62.11%\n",
      "Run: 03\t Epoch: 700\t Loss: 0.2717\t Train: 82.55%\t Valid: 77.71%\tTest: 61.55%\n",
      "Run: 03\t Epoch: 710\t Loss: 0.2704\t Train: 82.97%\t Valid: 78.16%\tTest: 62.05%\n",
      "Run: 03\t Epoch: 720\t Loss: 0.2701\t Train: 82.99%\t Valid: 78.17%\tTest: 62.27%\n",
      "Run: 03\t Epoch: 730\t Loss: 0.2693\t Train: 82.91%\t Valid: 77.77%\tTest: 61.52%\n",
      "Run: 03\t Epoch: 740\t Loss: 0.2699\t Train: 82.96%\t Valid: 77.58%\tTest: 61.07%\n",
      "Run: 03\t Epoch: 750\t Loss: 0.2691\t Train: 82.94%\t Valid: 77.68%\tTest: 62.12%\n",
      "Run: 03\t Epoch: 760\t Loss: 0.2690\t Train: 83.04%\t Valid: 77.55%\tTest: 61.63%\n",
      "Run: 03\t Epoch: 770\t Loss: 0.2685\t Train: 83.00%\t Valid: 77.75%\tTest: 62.03%\n",
      "Run: 03\t Epoch: 780\t Loss: 0.2688\t Train: 83.01%\t Valid: 77.62%\tTest: 61.92%\n",
      "Run: 03\t Epoch: 790\t Loss: 0.2671\t Train: 83.46%\t Valid: 77.89%\tTest: 61.82%\n",
      "Run: 03\t Epoch: 800\t Loss: 0.2672\t Train: 83.36%\t Valid: 77.83%\tTest: 62.40%\n",
      "Run: 03\t Epoch: 810\t Loss: 0.2683\t Train: 83.22%\t Valid: 77.67%\tTest: 62.35%\n",
      "Run: 03\t Epoch: 820\t Loss: 0.2678\t Train: 83.69%\t Valid: 77.86%\tTest: 62.09%\n",
      "Run: 03\t Epoch: 830\t Loss: 0.2708\t Train: 83.31%\t Valid: 77.35%\tTest: 61.11%\n",
      "Run: 03\t Epoch: 840\t Loss: 0.2680\t Train: 83.52%\t Valid: 77.67%\tTest: 62.08%\n",
      "Run: 03\t Epoch: 850\t Loss: 0.2663\t Train: 83.67%\t Valid: 77.92%\tTest: 62.52%\n",
      "Run: 03\t Epoch: 860\t Loss: 0.2676\t Train: 83.56%\t Valid: 78.09%\tTest: 63.78%\n",
      "Model saved.\n",
      "Run: 03\t Epoch: 870\t Loss: 0.2668\t Train: 83.74%\t Valid: 78.12%\tTest: 63.17%\n",
      "Run: 03\t Epoch: 880\t Loss: 0.2655\t Train: 83.77%\t Valid: 78.08%\tTest: 63.76%\n",
      "Run: 03\t Epoch: 890\t Loss: 0.2645\t Train: 83.84%\t Valid: 77.94%\tTest: 63.47%\n",
      "Run: 03\t Epoch: 900\t Loss: 0.2644\t Train: 83.87%\t Valid: 78.00%\tTest: 62.99%\n",
      "Run: 03\t Epoch: 910\t Loss: 0.2647\t Train: 83.93%\t Valid: 77.86%\tTest: 62.99%\n",
      "Run: 03\t Epoch: 920\t Loss: 0.2641\t Train: 84.11%\t Valid: 78.14%\tTest: 63.37%\n",
      "Run: 03\t Epoch: 930\t Loss: 0.2640\t Train: 83.92%\t Valid: 77.46%\tTest: 62.08%\n",
      "Run: 03\t Epoch: 940\t Loss: 0.2653\t Train: 84.05%\t Valid: 78.28%\tTest: 63.86%\n",
      "Model saved.\n",
      "Run: 03\t Epoch: 950\t Loss: 0.2646\t Train: 84.06%\t Valid: 77.90%\tTest: 63.60%\n",
      "Run: 03\t Epoch: 960\t Loss: 0.2646\t Train: 83.97%\t Valid: 77.43%\tTest: 62.75%\n",
      "Run: 03\t Epoch: 970\t Loss: 0.2630\t Train: 84.10%\t Valid: 77.78%\tTest: 63.30%\n",
      "Run: 03\t Epoch: 980\t Loss: 0.2634\t Train: 84.30%\t Valid: 78.41%\tTest: 64.27%\n",
      "Model saved.\n",
      "Run: 03\t Epoch: 990\t Loss: 0.2630\t Train: 84.21%\t Valid: 77.94%\tTest: 63.05%\n",
      "Run: 03\t Epoch: 1000\t Loss: 0.2628\t Train: 84.27%\t Valid: 77.81%\tTest: 63.67%\n",
      "Run 03:\n",
      "Highest Train: 84.30\n",
      "Highest Valid: 78.41\n",
      "  Final Train: 84.30\n",
      "   Final Test: 64.27\n",
      "Run: 04\t Epoch: 10\t Loss: 0.4264\t Train: 38.27%\t Valid: 35.90%\tTest: 38.04%\n",
      "Run: 04\t Epoch: 20\t Loss: 0.4142\t Train: 38.87%\t Valid: 33.64%\tTest: 35.69%\n",
      "Run: 04\t Epoch: 30\t Loss: 0.3988\t Train: 43.15%\t Valid: 34.31%\tTest: 35.03%\n",
      "Run: 04\t Epoch: 40\t Loss: 0.3829\t Train: 49.81%\t Valid: 38.16%\tTest: 36.91%\n",
      "Run: 04\t Epoch: 50\t Loss: 0.3722\t Train: 54.95%\t Valid: 45.30%\tTest: 42.00%\n",
      "Run: 04\t Epoch: 60\t Loss: 0.3641\t Train: 57.52%\t Valid: 49.41%\tTest: 44.44%\n",
      "Run: 04\t Epoch: 70\t Loss: 0.3559\t Train: 59.05%\t Valid: 51.94%\tTest: 46.30%\n",
      "Run: 04\t Epoch: 80\t Loss: 0.3465\t Train: 62.22%\t Valid: 55.54%\tTest: 49.07%\n",
      "Run: 04\t Epoch: 90\t Loss: 0.3387\t Train: 64.85%\t Valid: 60.27%\tTest: 52.44%\n",
      "Run: 04\t Epoch: 100\t Loss: 0.3323\t Train: 66.77%\t Valid: 64.16%\tTest: 54.66%\n",
      "Run: 04\t Epoch: 110\t Loss: 0.3294\t Train: 68.04%\t Valid: 66.48%\tTest: 56.13%\n",
      "Run: 04\t Epoch: 120\t Loss: 0.3246\t Train: 69.26%\t Valid: 68.17%\tTest: 57.33%\n",
      "Run: 04\t Epoch: 130\t Loss: 0.3220\t Train: 70.00%\t Valid: 68.95%\tTest: 57.83%\n",
      "Run: 04\t Epoch: 140\t Loss: 0.3196\t Train: 70.58%\t Valid: 69.54%\tTest: 58.30%\n",
      "Run: 04\t Epoch: 150\t Loss: 0.3173\t Train: 70.81%\t Valid: 69.51%\tTest: 58.20%\n",
      "Run: 04\t Epoch: 160\t Loss: 0.3152\t Train: 72.07%\t Valid: 71.00%\tTest: 59.90%\n",
      "Run: 04\t Epoch: 170\t Loss: 0.3131\t Train: 71.90%\t Valid: 70.45%\tTest: 59.20%\n",
      "Run: 04\t Epoch: 180\t Loss: 0.3115\t Train: 72.69%\t Valid: 71.15%\tTest: 60.04%\n",
      "Run: 04\t Epoch: 190\t Loss: 0.3152\t Train: 74.05%\t Valid: 73.01%\tTest: 62.74%\n",
      "Run: 04\t Epoch: 200\t Loss: 0.3082\t Train: 74.48%\t Valid: 73.36%\tTest: 63.09%\n",
      "Run: 04\t Epoch: 210\t Loss: 0.3070\t Train: 74.37%\t Valid: 72.68%\tTest: 62.32%\n",
      "Run: 04\t Epoch: 220\t Loss: 0.3052\t Train: 74.95%\t Valid: 73.15%\tTest: 62.92%\n",
      "Run: 04\t Epoch: 230\t Loss: 0.3036\t Train: 75.45%\t Valid: 73.69%\tTest: 63.61%\n",
      "Run: 04\t Epoch: 240\t Loss: 0.3020\t Train: 75.31%\t Valid: 73.07%\tTest: 62.83%\n",
      "Run: 04\t Epoch: 250\t Loss: 0.3008\t Train: 75.73%\t Valid: 73.24%\tTest: 63.13%\n",
      "Run: 04\t Epoch: 260\t Loss: 0.2993\t Train: 76.17%\t Valid: 73.54%\tTest: 63.27%\n",
      "Run: 04\t Epoch: 270\t Loss: 0.3022\t Train: 76.61%\t Valid: 73.75%\tTest: 64.35%\n",
      "Model saved.\n",
      "Run: 04\t Epoch: 280\t Loss: 0.2983\t Train: 77.12%\t Valid: 74.28%\tTest: 64.78%\n",
      "Model saved.\n",
      "Run: 04\t Epoch: 290\t Loss: 0.2960\t Train: 77.05%\t Valid: 74.18%\tTest: 64.01%\n",
      "Run: 04\t Epoch: 300\t Loss: 0.2961\t Train: 76.69%\t Valid: 73.19%\tTest: 62.14%\n",
      "Run: 04\t Epoch: 310\t Loss: 0.2941\t Train: 77.01%\t Valid: 73.28%\tTest: 62.30%\n",
      "Run: 04\t Epoch: 320\t Loss: 0.2940\t Train: 77.64%\t Valid: 73.88%\tTest: 61.94%\n",
      "Run: 04\t Epoch: 330\t Loss: 0.2916\t Train: 78.38%\t Valid: 75.08%\tTest: 64.01%\n",
      "Run: 04\t Epoch: 340\t Loss: 0.2913\t Train: 77.83%\t Valid: 73.87%\tTest: 61.24%\n",
      "Run: 04\t Epoch: 350\t Loss: 0.2897\t Train: 78.46%\t Valid: 74.72%\tTest: 62.39%\n",
      "Run: 04\t Epoch: 360\t Loss: 0.2905\t Train: 79.07%\t Valid: 75.56%\tTest: 62.85%\n",
      "Run: 04\t Epoch: 370\t Loss: 0.2896\t Train: 79.31%\t Valid: 75.63%\tTest: 61.19%\n",
      "Run: 04\t Epoch: 380\t Loss: 0.2874\t Train: 79.45%\t Valid: 76.23%\tTest: 62.93%\n",
      "Run: 04\t Epoch: 390\t Loss: 0.2864\t Train: 79.64%\t Valid: 76.05%\tTest: 61.84%\n",
      "Run: 04\t Epoch: 400\t Loss: 0.2871\t Train: 80.19%\t Valid: 76.61%\tTest: 61.96%\n",
      "Run: 04\t Epoch: 410\t Loss: 0.2852\t Train: 80.12%\t Valid: 76.40%\tTest: 61.49%\n",
      "Run: 04\t Epoch: 420\t Loss: 0.2856\t Train: 80.40%\t Valid: 76.69%\tTest: 60.21%\n",
      "Run: 04\t Epoch: 430\t Loss: 0.2835\t Train: 80.52%\t Valid: 76.74%\tTest: 60.86%\n",
      "Run: 04\t Epoch: 440\t Loss: 0.2827\t Train: 80.58%\t Valid: 76.27%\tTest: 58.98%\n",
      "Run: 04\t Epoch: 450\t Loss: 0.2813\t Train: 80.33%\t Valid: 75.49%\tTest: 57.77%\n",
      "Run: 04\t Epoch: 460\t Loss: 0.2790\t Train: 80.94%\t Valid: 76.37%\tTest: 59.00%\n",
      "Run: 04\t Epoch: 470\t Loss: 0.2814\t Train: 80.31%\t Valid: 76.06%\tTest: 60.22%\n",
      "Run: 04\t Epoch: 480\t Loss: 0.2805\t Train: 80.63%\t Valid: 75.57%\tTest: 57.76%\n",
      "Run: 04\t Epoch: 490\t Loss: 0.2783\t Train: 81.27%\t Valid: 76.25%\tTest: 58.10%\n",
      "Run: 04\t Epoch: 500\t Loss: 0.2773\t Train: 80.92%\t Valid: 75.76%\tTest: 57.94%\n",
      "Run: 04\t Epoch: 510\t Loss: 0.2773\t Train: 81.31%\t Valid: 75.68%\tTest: 57.50%\n",
      "Run: 04\t Epoch: 520\t Loss: 0.2765\t Train: 81.51%\t Valid: 76.74%\tTest: 59.59%\n",
      "Run: 04\t Epoch: 530\t Loss: 0.2759\t Train: 81.66%\t Valid: 76.73%\tTest: 58.90%\n",
      "Run: 04\t Epoch: 540\t Loss: 0.2756\t Train: 81.78%\t Valid: 77.31%\tTest: 60.33%\n",
      "Run: 04\t Epoch: 550\t Loss: 0.2747\t Train: 81.89%\t Valid: 77.01%\tTest: 58.89%\n",
      "Run: 04\t Epoch: 560\t Loss: 0.2753\t Train: 81.40%\t Valid: 76.55%\tTest: 58.55%\n",
      "Run: 04\t Epoch: 570\t Loss: 0.2765\t Train: 81.44%\t Valid: 76.73%\tTest: 59.69%\n",
      "Run: 04\t Epoch: 580\t Loss: 0.2735\t Train: 82.26%\t Valid: 77.10%\tTest: 58.79%\n",
      "Run: 04\t Epoch: 590\t Loss: 0.2729\t Train: 82.33%\t Valid: 77.30%\tTest: 59.49%\n",
      "Run: 04\t Epoch: 600\t Loss: 0.2726\t Train: 82.14%\t Valid: 77.07%\tTest: 58.20%\n",
      "Run: 04\t Epoch: 610\t Loss: 0.2748\t Train: 81.97%\t Valid: 76.67%\tTest: 57.96%\n",
      "Run: 04\t Epoch: 620\t Loss: 0.2724\t Train: 82.49%\t Valid: 76.83%\tTest: 58.20%\n",
      "Run: 04\t Epoch: 630\t Loss: 0.2718\t Train: 82.63%\t Valid: 77.47%\tTest: 59.54%\n",
      "Run: 04\t Epoch: 640\t Loss: 0.2708\t Train: 82.64%\t Valid: 77.25%\tTest: 58.66%\n",
      "Run: 04\t Epoch: 650\t Loss: 0.2698\t Train: 82.67%\t Valid: 76.99%\tTest: 58.45%\n",
      "Run: 04\t Epoch: 660\t Loss: 0.2703\t Train: 82.95%\t Valid: 77.74%\tTest: 59.41%\n",
      "Run: 04\t Epoch: 670\t Loss: 0.2713\t Train: 82.63%\t Valid: 77.16%\tTest: 58.67%\n",
      "Run: 04\t Epoch: 680\t Loss: 0.2704\t Train: 82.51%\t Valid: 76.25%\tTest: 57.52%\n",
      "Run: 04\t Epoch: 690\t Loss: 0.2717\t Train: 82.58%\t Valid: 77.53%\tTest: 58.27%\n",
      "Run: 04\t Epoch: 700\t Loss: 0.2697\t Train: 82.94%\t Valid: 78.09%\tTest: 60.61%\n",
      "Run: 04\t Epoch: 710\t Loss: 0.2687\t Train: 83.09%\t Valid: 77.38%\tTest: 58.29%\n",
      "Run: 04\t Epoch: 720\t Loss: 0.2693\t Train: 83.28%\t Valid: 77.64%\tTest: 58.96%\n",
      "Run: 04\t Epoch: 730\t Loss: 0.2702\t Train: 83.12%\t Valid: 76.61%\tTest: 58.40%\n",
      "Run: 04\t Epoch: 740\t Loss: 0.2680\t Train: 83.15%\t Valid: 77.07%\tTest: 58.65%\n",
      "Run: 04\t Epoch: 750\t Loss: 0.2679\t Train: 83.32%\t Valid: 77.45%\tTest: 58.74%\n",
      "Run: 04\t Epoch: 760\t Loss: 0.2680\t Train: 83.57%\t Valid: 77.99%\tTest: 59.80%\n",
      "Run: 04\t Epoch: 770\t Loss: 0.2667\t Train: 83.36%\t Valid: 77.58%\tTest: 59.00%\n",
      "Run: 04\t Epoch: 780\t Loss: 0.2664\t Train: 83.51%\t Valid: 77.43%\tTest: 58.52%\n",
      "Run: 04\t Epoch: 790\t Loss: 0.2668\t Train: 83.42%\t Valid: 76.69%\tTest: 57.75%\n",
      "Run: 04\t Epoch: 800\t Loss: 0.2668\t Train: 83.35%\t Valid: 76.46%\tTest: 57.35%\n",
      "Run: 04\t Epoch: 810\t Loss: 0.2663\t Train: 83.72%\t Valid: 77.83%\tTest: 59.48%\n",
      "Run: 04\t Epoch: 820\t Loss: 0.2651\t Train: 83.75%\t Valid: 77.46%\tTest: 59.23%\n",
      "Run: 04\t Epoch: 830\t Loss: 0.2658\t Train: 83.59%\t Valid: 76.24%\tTest: 57.82%\n",
      "Run: 04\t Epoch: 840\t Loss: 0.2647\t Train: 83.80%\t Valid: 77.67%\tTest: 59.33%\n",
      "Run: 04\t Epoch: 850\t Loss: 0.2651\t Train: 83.90%\t Valid: 77.16%\tTest: 58.66%\n",
      "Run: 04\t Epoch: 860\t Loss: 0.2643\t Train: 84.07%\t Valid: 77.73%\tTest: 59.27%\n",
      "Run: 04\t Epoch: 870\t Loss: 0.2663\t Train: 84.05%\t Valid: 77.22%\tTest: 58.95%\n",
      "Run: 04\t Epoch: 880\t Loss: 0.2642\t Train: 83.96%\t Valid: 77.06%\tTest: 58.39%\n",
      "Run: 04\t Epoch: 890\t Loss: 0.2633\t Train: 84.04%\t Valid: 77.28%\tTest: 58.87%\n",
      "Run: 04\t Epoch: 900\t Loss: 0.2630\t Train: 84.04%\t Valid: 76.77%\tTest: 58.13%\n",
      "Run: 04\t Epoch: 910\t Loss: 0.2631\t Train: 84.06%\t Valid: 77.61%\tTest: 59.27%\n",
      "Run: 04\t Epoch: 920\t Loss: 0.2629\t Train: 84.13%\t Valid: 76.02%\tTest: 57.14%\n",
      "Run: 04\t Epoch: 930\t Loss: 0.2634\t Train: 84.36%\t Valid: 77.78%\tTest: 59.06%\n",
      "Run: 04\t Epoch: 940\t Loss: 0.2628\t Train: 84.39%\t Valid: 77.41%\tTest: 58.90%\n",
      "Run: 04\t Epoch: 950\t Loss: 0.2630\t Train: 84.43%\t Valid: 78.01%\tTest: 60.06%\n",
      "Run: 04\t Epoch: 960\t Loss: 0.2616\t Train: 84.39%\t Valid: 76.98%\tTest: 58.33%\n",
      "Run: 04\t Epoch: 970\t Loss: 0.2628\t Train: 84.49%\t Valid: 77.77%\tTest: 59.49%\n",
      "Run: 04\t Epoch: 980\t Loss: 0.2630\t Train: 84.50%\t Valid: 78.07%\tTest: 59.60%\n",
      "Run: 04\t Epoch: 990\t Loss: 0.2619\t Train: 84.22%\t Valid: 76.03%\tTest: 57.00%\n",
      "Run: 04\t Epoch: 1000\t Loss: 0.2627\t Train: 84.52%\t Valid: 77.92%\tTest: 58.94%\n",
      "Run 04:\n",
      "Highest Train: 84.52\n",
      "Highest Valid: 78.09\n",
      "  Final Train: 82.94\n",
      "   Final Test: 60.61\n",
      "Run: 05\t Epoch: 10\t Loss: 0.4252\t Train: 37.91%\t Valid: 35.30%\tTest: 37.44%\n",
      "Run: 05\t Epoch: 20\t Loss: 0.4141\t Train: 39.02%\t Valid: 33.64%\tTest: 35.50%\n",
      "Run: 05\t Epoch: 30\t Loss: 0.3992\t Train: 42.97%\t Valid: 34.12%\tTest: 34.82%\n",
      "Run: 05\t Epoch: 40\t Loss: 0.3831\t Train: 49.69%\t Valid: 38.31%\tTest: 37.11%\n",
      "Run: 05\t Epoch: 50\t Loss: 0.3725\t Train: 54.85%\t Valid: 45.00%\tTest: 41.62%\n",
      "Run: 05\t Epoch: 60\t Loss: 0.3652\t Train: 57.49%\t Valid: 48.95%\tTest: 43.88%\n",
      "Run: 05\t Epoch: 70\t Loss: 0.3574\t Train: 58.38%\t Valid: 51.08%\tTest: 45.35%\n",
      "Run: 05\t Epoch: 80\t Loss: 0.3474\t Train: 61.06%\t Valid: 54.42%\tTest: 48.08%\n",
      "Run: 05\t Epoch: 90\t Loss: 0.3375\t Train: 65.32%\t Valid: 61.39%\tTest: 52.99%\n",
      "Run: 05\t Epoch: 100\t Loss: 0.3317\t Train: 67.92%\t Valid: 66.55%\tTest: 55.87%\n",
      "Run: 05\t Epoch: 110\t Loss: 0.3283\t Train: 67.98%\t Valid: 66.80%\tTest: 55.57%\n",
      "Run: 05\t Epoch: 120\t Loss: 0.3247\t Train: 68.90%\t Valid: 67.88%\tTest: 56.38%\n",
      "Run: 05\t Epoch: 130\t Loss: 0.3218\t Train: 69.32%\t Valid: 68.20%\tTest: 56.57%\n",
      "Run: 05\t Epoch: 140\t Loss: 0.3204\t Train: 70.60%\t Valid: 69.97%\tTest: 58.04%\n",
      "Run: 05\t Epoch: 150\t Loss: 0.3174\t Train: 71.60%\t Valid: 71.08%\tTest: 59.11%\n",
      "Run: 05\t Epoch: 160\t Loss: 0.3167\t Train: 71.20%\t Valid: 70.21%\tTest: 58.52%\n",
      "Run: 05\t Epoch: 170\t Loss: 0.3135\t Train: 72.26%\t Valid: 71.13%\tTest: 59.41%\n",
      "Run: 05\t Epoch: 180\t Loss: 0.3123\t Train: 73.54%\t Valid: 72.78%\tTest: 61.38%\n",
      "Run: 05\t Epoch: 190\t Loss: 0.3097\t Train: 73.79%\t Valid: 72.68%\tTest: 61.47%\n",
      "Run: 05\t Epoch: 200\t Loss: 0.3090\t Train: 73.75%\t Valid: 72.30%\tTest: 61.12%\n",
      "Run: 05\t Epoch: 210\t Loss: 0.3067\t Train: 74.03%\t Valid: 72.37%\tTest: 61.03%\n",
      "Run: 05\t Epoch: 220\t Loss: 0.3133\t Train: 75.33%\t Valid: 74.54%\tTest: 64.44%\n",
      "Run: 05\t Epoch: 230\t Loss: 0.3059\t Train: 73.95%\t Valid: 72.26%\tTest: 61.72%\n",
      "Run: 05\t Epoch: 240\t Loss: 0.3042\t Train: 74.81%\t Valid: 73.09%\tTest: 62.75%\n",
      "Run: 05\t Epoch: 250\t Loss: 0.3028\t Train: 75.23%\t Valid: 73.40%\tTest: 62.84%\n",
      "Run: 05\t Epoch: 260\t Loss: 0.3015\t Train: 75.58%\t Valid: 73.55%\tTest: 62.71%\n",
      "Run: 05\t Epoch: 270\t Loss: 0.3005\t Train: 75.39%\t Valid: 72.87%\tTest: 62.01%\n",
      "Run: 05\t Epoch: 280\t Loss: 0.3019\t Train: 75.26%\t Valid: 72.51%\tTest: 61.79%\n",
      "Run: 05\t Epoch: 290\t Loss: 0.2986\t Train: 76.59%\t Valid: 74.56%\tTest: 63.91%\n",
      "Run: 05\t Epoch: 300\t Loss: 0.2977\t Train: 76.59%\t Valid: 74.21%\tTest: 63.39%\n",
      "Run: 05\t Epoch: 310\t Loss: 0.2978\t Train: 77.20%\t Valid: 74.99%\tTest: 64.49%\n",
      "Run: 05\t Epoch: 320\t Loss: 0.2965\t Train: 77.02%\t Valid: 74.44%\tTest: 63.61%\n",
      "Run: 05\t Epoch: 330\t Loss: 0.2956\t Train: 77.15%\t Valid: 74.27%\tTest: 63.23%\n",
      "Run: 05\t Epoch: 340\t Loss: 0.2956\t Train: 77.76%\t Valid: 75.14%\tTest: 64.58%\n",
      "Run: 05\t Epoch: 350\t Loss: 0.2941\t Train: 77.90%\t Valid: 75.15%\tTest: 64.43%\n",
      "Run: 05\t Epoch: 360\t Loss: 0.2925\t Train: 78.28%\t Valid: 75.39%\tTest: 64.86%\n",
      "Model saved.\n",
      "Run: 05\t Epoch: 370\t Loss: 0.2968\t Train: 77.25%\t Valid: 73.42%\tTest: 62.71%\n",
      "Run: 05\t Epoch: 380\t Loss: 0.2927\t Train: 78.59%\t Valid: 75.31%\tTest: 65.14%\n",
      "Model saved.\n",
      "Run: 05\t Epoch: 390\t Loss: 0.2903\t Train: 78.58%\t Valid: 75.10%\tTest: 64.95%\n",
      "Run: 05\t Epoch: 400\t Loss: 0.2893\t Train: 79.03%\t Valid: 75.26%\tTest: 65.08%\n",
      "Run: 05\t Epoch: 410\t Loss: 0.2920\t Train: 78.05%\t Valid: 74.07%\tTest: 63.63%\n",
      "Run: 05\t Epoch: 420\t Loss: 0.2875\t Train: 79.33%\t Valid: 75.01%\tTest: 63.90%\n",
      "Run: 05\t Epoch: 430\t Loss: 0.2871\t Train: 79.72%\t Valid: 75.67%\tTest: 65.02%\n",
      "Run: 05\t Epoch: 440\t Loss: 0.2866\t Train: 79.71%\t Valid: 75.57%\tTest: 64.48%\n",
      "Run: 05\t Epoch: 450\t Loss: 0.2842\t Train: 79.88%\t Valid: 75.39%\tTest: 64.08%\n",
      "Run: 05\t Epoch: 460\t Loss: 0.2836\t Train: 80.42%\t Valid: 76.36%\tTest: 65.56%\n",
      "Model saved.\n",
      "Run: 05\t Epoch: 470\t Loss: 0.2838\t Train: 80.34%\t Valid: 76.09%\tTest: 64.56%\n",
      "Run: 05\t Epoch: 480\t Loss: 0.2819\t Train: 79.65%\t Valid: 74.87%\tTest: 63.16%\n",
      "Run: 05\t Epoch: 490\t Loss: 0.2821\t Train: 80.65%\t Valid: 76.52%\tTest: 65.17%\n",
      "Run: 05\t Epoch: 500\t Loss: 0.2805\t Train: 80.90%\t Valid: 76.51%\tTest: 64.71%\n",
      "Run: 05\t Epoch: 510\t Loss: 0.2802\t Train: 80.84%\t Valid: 76.20%\tTest: 63.92%\n",
      "Run: 05\t Epoch: 520\t Loss: 0.2811\t Train: 81.17%\t Valid: 76.88%\tTest: 64.44%\n",
      "Run: 05\t Epoch: 530\t Loss: 0.2783\t Train: 81.00%\t Valid: 76.44%\tTest: 64.28%\n",
      "Run: 05\t Epoch: 540\t Loss: 0.2781\t Train: 81.17%\t Valid: 76.50%\tTest: 64.24%\n",
      "Run: 05\t Epoch: 550\t Loss: 0.2774\t Train: 81.32%\t Valid: 76.75%\tTest: 64.57%\n",
      "Run: 05\t Epoch: 560\t Loss: 0.2775\t Train: 81.36%\t Valid: 76.37%\tTest: 63.47%\n",
      "Run: 05\t Epoch: 570\t Loss: 0.2764\t Train: 81.66%\t Valid: 76.89%\tTest: 64.78%\n",
      "Run: 05\t Epoch: 580\t Loss: 0.2794\t Train: 80.92%\t Valid: 75.15%\tTest: 62.22%\n",
      "Run: 05\t Epoch: 590\t Loss: 0.2763\t Train: 81.74%\t Valid: 77.11%\tTest: 65.29%\n",
      "Run: 05\t Epoch: 600\t Loss: 0.2748\t Train: 81.83%\t Valid: 76.62%\tTest: 62.72%\n",
      "Run: 05\t Epoch: 610\t Loss: 0.2743\t Train: 82.09%\t Valid: 77.04%\tTest: 63.88%\n",
      "Run: 05\t Epoch: 620\t Loss: 0.2738\t Train: 82.22%\t Valid: 77.03%\tTest: 63.68%\n",
      "Run: 05\t Epoch: 630\t Loss: 0.2733\t Train: 82.09%\t Valid: 76.92%\tTest: 63.34%\n",
      "Run: 05\t Epoch: 640\t Loss: 0.2742\t Train: 82.27%\t Valid: 76.92%\tTest: 62.99%\n",
      "Run: 05\t Epoch: 650\t Loss: 0.2724\t Train: 82.24%\t Valid: 77.05%\tTest: 63.36%\n",
      "Run: 05\t Epoch: 660\t Loss: 0.2717\t Train: 82.30%\t Valid: 76.64%\tTest: 62.37%\n",
      "Run: 05\t Epoch: 670\t Loss: 0.2717\t Train: 82.45%\t Valid: 76.98%\tTest: 62.95%\n",
      "Run: 05\t Epoch: 680\t Loss: 0.2714\t Train: 82.63%\t Valid: 77.36%\tTest: 63.71%\n",
      "Run: 05\t Epoch: 690\t Loss: 0.2713\t Train: 82.85%\t Valid: 77.71%\tTest: 64.33%\n",
      "Run: 05\t Epoch: 700\t Loss: 0.2723\t Train: 82.78%\t Valid: 77.50%\tTest: 65.35%\n",
      "Run: 05\t Epoch: 710\t Loss: 0.2723\t Train: 82.81%\t Valid: 77.07%\tTest: 63.01%\n",
      "Run: 05\t Epoch: 720\t Loss: 0.2702\t Train: 82.74%\t Valid: 76.67%\tTest: 62.82%\n",
      "Run: 05\t Epoch: 730\t Loss: 0.2711\t Train: 83.01%\t Valid: 77.84%\tTest: 65.07%\n",
      "Run: 05\t Epoch: 740\t Loss: 0.2696\t Train: 83.11%\t Valid: 77.88%\tTest: 65.37%\n",
      "Run: 05\t Epoch: 750\t Loss: 0.2694\t Train: 83.14%\t Valid: 77.22%\tTest: 63.14%\n",
      "Run: 05\t Epoch: 760\t Loss: 0.2689\t Train: 83.13%\t Valid: 77.30%\tTest: 63.41%\n",
      "Run: 05\t Epoch: 770\t Loss: 0.2707\t Train: 83.20%\t Valid: 77.78%\tTest: 64.12%\n",
      "Run: 05\t Epoch: 780\t Loss: 0.2736\t Train: 82.71%\t Valid: 76.90%\tTest: 64.03%\n",
      "Run: 05\t Epoch: 790\t Loss: 0.2692\t Train: 83.33%\t Valid: 77.67%\tTest: 63.52%\n",
      "Run: 05\t Epoch: 800\t Loss: 0.2681\t Train: 83.35%\t Valid: 77.78%\tTest: 64.76%\n",
      "Run: 05\t Epoch: 810\t Loss: 0.2672\t Train: 83.41%\t Valid: 77.90%\tTest: 65.01%\n",
      "Run: 05\t Epoch: 820\t Loss: 0.2668\t Train: 83.48%\t Valid: 78.11%\tTest: 64.46%\n",
      "Run: 05\t Epoch: 830\t Loss: 0.2669\t Train: 83.58%\t Valid: 78.10%\tTest: 64.99%\n",
      "Run: 05\t Epoch: 840\t Loss: 0.2664\t Train: 83.64%\t Valid: 78.00%\tTest: 64.57%\n",
      "Run: 05\t Epoch: 850\t Loss: 0.2659\t Train: 83.58%\t Valid: 77.67%\tTest: 63.78%\n",
      "Run: 05\t Epoch: 860\t Loss: 0.2664\t Train: 83.57%\t Valid: 77.74%\tTest: 63.84%\n",
      "Run: 05\t Epoch: 870\t Loss: 0.2662\t Train: 83.70%\t Valid: 77.76%\tTest: 64.01%\n",
      "Run: 05\t Epoch: 880\t Loss: 0.2650\t Train: 83.85%\t Valid: 77.75%\tTest: 63.67%\n",
      "Run: 05\t Epoch: 890\t Loss: 0.2650\t Train: 83.79%\t Valid: 78.07%\tTest: 64.67%\n",
      "Run: 05\t Epoch: 900\t Loss: 0.2664\t Train: 83.92%\t Valid: 77.96%\tTest: 63.77%\n",
      "Run: 05\t Epoch: 910\t Loss: 0.2649\t Train: 83.85%\t Valid: 77.93%\tTest: 65.25%\n",
      "Run: 05\t Epoch: 920\t Loss: 0.2646\t Train: 83.97%\t Valid: 78.16%\tTest: 64.39%\n",
      "Run: 05\t Epoch: 930\t Loss: 0.2643\t Train: 83.94%\t Valid: 77.98%\tTest: 64.18%\n",
      "Run: 05\t Epoch: 940\t Loss: 0.2650\t Train: 83.75%\t Valid: 77.35%\tTest: 63.62%\n",
      "Run: 05\t Epoch: 950\t Loss: 0.2644\t Train: 84.03%\t Valid: 77.77%\tTest: 64.15%\n",
      "Run: 05\t Epoch: 960\t Loss: 0.2639\t Train: 84.05%\t Valid: 77.59%\tTest: 63.96%\n",
      "Run: 05\t Epoch: 970\t Loss: 0.2717\t Train: 83.23%\t Valid: 76.87%\tTest: 64.10%\n",
      "Run: 05\t Epoch: 980\t Loss: 0.2648\t Train: 84.02%\t Valid: 78.16%\tTest: 65.95%\n",
      "Model saved.\n",
      "Run: 05\t Epoch: 990\t Loss: 0.2637\t Train: 84.15%\t Valid: 78.14%\tTest: 64.96%\n",
      "Run: 05\t Epoch: 1000\t Loss: 0.2632\t Train: 84.22%\t Valid: 78.00%\tTest: 64.38%\n",
      "Run 05:\n",
      "Highest Train: 84.22\n",
      "Highest Valid: 78.16\n",
      "  Final Train: 83.97\n",
      "   Final Test: 64.39\n",
      "Run: 06\t Epoch: 10\t Loss: 0.4239\t Train: 38.00%\t Valid: 34.83%\tTest: 37.04%\n",
      "Run: 06\t Epoch: 20\t Loss: 0.4169\t Train: 38.42%\t Valid: 33.37%\tTest: 35.50%\n",
      "Run: 06\t Epoch: 30\t Loss: 0.3988\t Train: 42.51%\t Valid: 34.19%\tTest: 35.05%\n",
      "Run: 06\t Epoch: 40\t Loss: 0.3840\t Train: 48.58%\t Valid: 37.98%\tTest: 37.22%\n",
      "Run: 06\t Epoch: 50\t Loss: 0.3721\t Train: 55.39%\t Valid: 45.64%\tTest: 42.26%\n",
      "Run: 06\t Epoch: 60\t Loss: 0.3638\t Train: 57.84%\t Valid: 49.51%\tTest: 44.48%\n",
      "Run: 06\t Epoch: 70\t Loss: 0.3546\t Train: 59.41%\t Valid: 52.35%\tTest: 46.33%\n",
      "Run: 06\t Epoch: 80\t Loss: 0.3437\t Train: 62.86%\t Valid: 56.83%\tTest: 49.38%\n",
      "Run: 06\t Epoch: 90\t Loss: 0.3340\t Train: 66.55%\t Valid: 63.53%\tTest: 53.71%\n",
      "Run: 06\t Epoch: 100\t Loss: 0.3291\t Train: 68.37%\t Valid: 67.04%\tTest: 56.25%\n",
      "Run: 06\t Epoch: 110\t Loss: 0.3259\t Train: 69.22%\t Valid: 68.12%\tTest: 57.09%\n",
      "Run: 06\t Epoch: 120\t Loss: 0.3224\t Train: 69.65%\t Valid: 68.51%\tTest: 56.73%\n",
      "Run: 06\t Epoch: 130\t Loss: 0.3200\t Train: 71.39%\t Valid: 70.73%\tTest: 58.54%\n",
      "Run: 06\t Epoch: 140\t Loss: 0.3178\t Train: 71.57%\t Valid: 70.60%\tTest: 58.46%\n",
      "Run: 06\t Epoch: 150\t Loss: 0.3155\t Train: 71.96%\t Valid: 70.82%\tTest: 58.09%\n",
      "Run: 06\t Epoch: 160\t Loss: 0.3142\t Train: 73.03%\t Valid: 72.10%\tTest: 59.72%\n",
      "Run: 06\t Epoch: 170\t Loss: 0.3115\t Train: 73.56%\t Valid: 72.59%\tTest: 59.90%\n",
      "Run: 06\t Epoch: 180\t Loss: 0.3093\t Train: 73.45%\t Valid: 72.16%\tTest: 59.11%\n",
      "Run: 06\t Epoch: 190\t Loss: 0.3085\t Train: 75.00%\t Valid: 73.97%\tTest: 61.40%\n",
      "Run: 06\t Epoch: 200\t Loss: 0.3063\t Train: 73.83%\t Valid: 72.04%\tTest: 58.80%\n",
      "Run: 06\t Epoch: 210\t Loss: 0.3047\t Train: 75.29%\t Valid: 73.95%\tTest: 60.75%\n",
      "Run: 06\t Epoch: 220\t Loss: 0.3029\t Train: 74.65%\t Valid: 72.56%\tTest: 59.35%\n",
      "Run: 06\t Epoch: 230\t Loss: 0.3013\t Train: 75.97%\t Valid: 74.17%\tTest: 60.46%\n",
      "Run: 06\t Epoch: 240\t Loss: 0.2996\t Train: 76.28%\t Valid: 74.00%\tTest: 59.88%\n",
      "Run: 06\t Epoch: 250\t Loss: 0.3002\t Train: 77.22%\t Valid: 75.21%\tTest: 61.55%\n",
      "Run: 06\t Epoch: 260\t Loss: 0.2986\t Train: 76.48%\t Valid: 73.23%\tTest: 59.03%\n",
      "Run: 06\t Epoch: 270\t Loss: 0.2963\t Train: 77.12%\t Valid: 74.47%\tTest: 60.44%\n",
      "Run: 06\t Epoch: 280\t Loss: 0.2954\t Train: 77.35%\t Valid: 74.35%\tTest: 59.65%\n",
      "Run: 06\t Epoch: 290\t Loss: 0.2943\t Train: 78.22%\t Valid: 74.96%\tTest: 60.17%\n",
      "Run: 06\t Epoch: 300\t Loss: 0.2928\t Train: 78.41%\t Valid: 75.42%\tTest: 60.73%\n",
      "Run: 06\t Epoch: 310\t Loss: 0.2922\t Train: 78.38%\t Valid: 74.88%\tTest: 59.55%\n",
      "Run: 06\t Epoch: 320\t Loss: 0.2915\t Train: 78.35%\t Valid: 74.40%\tTest: 58.32%\n",
      "Run: 06\t Epoch: 330\t Loss: 0.2916\t Train: 77.65%\t Valid: 74.41%\tTest: 58.64%\n",
      "Run: 06\t Epoch: 340\t Loss: 0.2903\t Train: 78.77%\t Valid: 75.82%\tTest: 61.27%\n",
      "Run: 06\t Epoch: 350\t Loss: 0.2886\t Train: 79.19%\t Valid: 75.54%\tTest: 59.60%\n",
      "Run: 06\t Epoch: 360\t Loss: 0.2871\t Train: 79.29%\t Valid: 75.10%\tTest: 58.18%\n",
      "Run: 06\t Epoch: 370\t Loss: 0.2859\t Train: 79.59%\t Valid: 75.85%\tTest: 59.30%\n",
      "Run: 06\t Epoch: 380\t Loss: 0.2859\t Train: 79.48%\t Valid: 75.03%\tTest: 57.77%\n",
      "Run: 06\t Epoch: 390\t Loss: 0.2869\t Train: 80.22%\t Valid: 76.41%\tTest: 59.43%\n",
      "Run: 06\t Epoch: 400\t Loss: 0.2837\t Train: 80.12%\t Valid: 75.82%\tTest: 58.81%\n",
      "Run: 06\t Epoch: 410\t Loss: 0.2860\t Train: 79.27%\t Valid: 74.79%\tTest: 56.74%\n",
      "Run: 06\t Epoch: 420\t Loss: 0.2830\t Train: 80.72%\t Valid: 76.65%\tTest: 59.15%\n",
      "Run: 06\t Epoch: 430\t Loss: 0.2826\t Train: 80.38%\t Valid: 75.85%\tTest: 57.54%\n",
      "Run: 06\t Epoch: 440\t Loss: 0.2810\t Train: 80.48%\t Valid: 75.93%\tTest: 57.96%\n",
      "Run: 06\t Epoch: 450\t Loss: 0.2806\t Train: 80.61%\t Valid: 76.02%\tTest: 58.43%\n",
      "Run: 06\t Epoch: 460\t Loss: 0.2794\t Train: 80.95%\t Valid: 76.24%\tTest: 58.09%\n",
      "Run: 06\t Epoch: 470\t Loss: 0.2801\t Train: 81.38%\t Valid: 76.70%\tTest: 58.97%\n",
      "Run: 06\t Epoch: 480\t Loss: 0.2802\t Train: 81.22%\t Valid: 76.67%\tTest: 58.95%\n",
      "Run: 06\t Epoch: 490\t Loss: 0.2783\t Train: 81.15%\t Valid: 76.36%\tTest: 58.01%\n",
      "Run: 06\t Epoch: 500\t Loss: 0.2789\t Train: 80.96%\t Valid: 76.14%\tTest: 58.04%\n",
      "Run: 06\t Epoch: 510\t Loss: 0.2775\t Train: 81.30%\t Valid: 76.10%\tTest: 57.48%\n",
      "Run: 06\t Epoch: 520\t Loss: 0.2821\t Train: 81.04%\t Valid: 77.02%\tTest: 60.14%\n",
      "Run: 06\t Epoch: 530\t Loss: 0.2778\t Train: 81.49%\t Valid: 77.31%\tTest: 59.97%\n",
      "Run: 06\t Epoch: 540\t Loss: 0.2790\t Train: 81.94%\t Valid: 77.46%\tTest: 61.21%\n",
      "Run: 06\t Epoch: 550\t Loss: 0.2763\t Train: 81.67%\t Valid: 76.80%\tTest: 58.82%\n",
      "Run: 06\t Epoch: 560\t Loss: 0.2748\t Train: 81.93%\t Valid: 76.84%\tTest: 58.98%\n",
      "Run: 06\t Epoch: 570\t Loss: 0.2749\t Train: 82.20%\t Valid: 77.39%\tTest: 59.97%\n",
      "Run: 06\t Epoch: 580\t Loss: 0.2744\t Train: 82.25%\t Valid: 77.32%\tTest: 59.76%\n",
      "Run: 06\t Epoch: 590\t Loss: 0.2742\t Train: 82.26%\t Valid: 76.79%\tTest: 58.84%\n",
      "Run: 06\t Epoch: 600\t Loss: 0.2731\t Train: 82.43%\t Valid: 77.07%\tTest: 58.78%\n",
      "Run: 06\t Epoch: 610\t Loss: 0.2745\t Train: 82.50%\t Valid: 77.79%\tTest: 60.70%\n",
      "Run: 06\t Epoch: 620\t Loss: 0.2735\t Train: 82.46%\t Valid: 77.90%\tTest: 61.08%\n",
      "Run: 06\t Epoch: 630\t Loss: 0.2728\t Train: 82.60%\t Valid: 77.85%\tTest: 60.50%\n",
      "Run: 06\t Epoch: 640\t Loss: 0.2751\t Train: 82.77%\t Valid: 77.74%\tTest: 60.03%\n",
      "Run: 06\t Epoch: 650\t Loss: 0.2725\t Train: 82.25%\t Valid: 76.78%\tTest: 59.38%\n",
      "Run: 06\t Epoch: 660\t Loss: 0.2723\t Train: 82.93%\t Valid: 77.12%\tTest: 58.90%\n",
      "Run: 06\t Epoch: 670\t Loss: 0.2712\t Train: 82.57%\t Valid: 76.82%\tTest: 58.62%\n",
      "Run: 06\t Epoch: 680\t Loss: 0.2702\t Train: 82.94%\t Valid: 77.48%\tTest: 59.27%\n",
      "Run: 06\t Epoch: 690\t Loss: 0.2704\t Train: 82.74%\t Valid: 77.47%\tTest: 59.49%\n",
      "Run: 06\t Epoch: 700\t Loss: 0.2707\t Train: 82.90%\t Valid: 77.65%\tTest: 59.93%\n",
      "Run: 06\t Epoch: 710\t Loss: 0.2695\t Train: 83.05%\t Valid: 77.37%\tTest: 59.28%\n",
      "Run: 06\t Epoch: 720\t Loss: 0.2710\t Train: 82.88%\t Valid: 77.17%\tTest: 58.61%\n",
      "Run: 06\t Epoch: 730\t Loss: 0.2688\t Train: 83.17%\t Valid: 77.55%\tTest: 58.90%\n",
      "Run: 06\t Epoch: 740\t Loss: 0.2678\t Train: 83.24%\t Valid: 77.49%\tTest: 59.22%\n",
      "Run: 06\t Epoch: 750\t Loss: 0.2692\t Train: 83.41%\t Valid: 77.78%\tTest: 59.15%\n",
      "Run: 06\t Epoch: 760\t Loss: 0.2674\t Train: 83.35%\t Valid: 77.88%\tTest: 60.16%\n",
      "Run: 06\t Epoch: 770\t Loss: 0.2684\t Train: 83.15%\t Valid: 77.28%\tTest: 58.97%\n",
      "Run: 06\t Epoch: 780\t Loss: 0.2688\t Train: 83.27%\t Valid: 78.13%\tTest: 60.65%\n",
      "Run: 06\t Epoch: 790\t Loss: 0.2669\t Train: 83.67%\t Valid: 77.90%\tTest: 59.62%\n",
      "Run: 06\t Epoch: 800\t Loss: 0.2678\t Train: 83.60%\t Valid: 78.36%\tTest: 60.79%\n",
      "Run: 06\t Epoch: 810\t Loss: 0.2671\t Train: 83.71%\t Valid: 78.04%\tTest: 59.84%\n",
      "Run: 06\t Epoch: 820\t Loss: 0.2658\t Train: 83.73%\t Valid: 78.22%\tTest: 60.24%\n",
      "Run: 06\t Epoch: 830\t Loss: 0.2659\t Train: 83.75%\t Valid: 77.97%\tTest: 59.87%\n",
      "Run: 06\t Epoch: 840\t Loss: 0.2659\t Train: 83.60%\t Valid: 77.44%\tTest: 59.01%\n",
      "Run: 06\t Epoch: 850\t Loss: 0.2697\t Train: 83.44%\t Valid: 77.24%\tTest: 58.94%\n",
      "Run: 06\t Epoch: 860\t Loss: 0.2659\t Train: 83.89%\t Valid: 77.83%\tTest: 60.42%\n",
      "Run: 06\t Epoch: 870\t Loss: 0.2658\t Train: 84.02%\t Valid: 78.25%\tTest: 61.03%\n",
      "Run: 06\t Epoch: 880\t Loss: 0.2647\t Train: 84.00%\t Valid: 77.55%\tTest: 59.26%\n",
      "Run: 06\t Epoch: 890\t Loss: 0.2662\t Train: 83.97%\t Valid: 77.91%\tTest: 59.50%\n",
      "Run: 06\t Epoch: 900\t Loss: 0.2669\t Train: 84.06%\t Valid: 77.85%\tTest: 59.52%\n",
      "Run: 06\t Epoch: 910\t Loss: 0.2649\t Train: 83.96%\t Valid: 78.14%\tTest: 60.67%\n",
      "Run: 06\t Epoch: 920\t Loss: 0.2644\t Train: 84.13%\t Valid: 77.97%\tTest: 60.34%\n",
      "Run: 06\t Epoch: 930\t Loss: 0.2642\t Train: 84.06%\t Valid: 77.09%\tTest: 58.52%\n",
      "Run: 06\t Epoch: 940\t Loss: 0.2635\t Train: 84.11%\t Valid: 77.54%\tTest: 59.08%\n",
      "Run: 06\t Epoch: 950\t Loss: 0.2630\t Train: 84.26%\t Valid: 78.21%\tTest: 60.23%\n",
      "Run: 06\t Epoch: 960\t Loss: 0.2632\t Train: 84.15%\t Valid: 77.74%\tTest: 59.49%\n",
      "Run: 06\t Epoch: 970\t Loss: 0.2628\t Train: 84.33%\t Valid: 77.75%\tTest: 59.84%\n",
      "Run: 06\t Epoch: 980\t Loss: 0.2627\t Train: 84.39%\t Valid: 78.49%\tTest: 60.87%\n",
      "Run: 06\t Epoch: 990\t Loss: 0.2624\t Train: 84.43%\t Valid: 78.40%\tTest: 60.48%\n",
      "Run: 06\t Epoch: 1000\t Loss: 0.2622\t Train: 84.13%\t Valid: 77.60%\tTest: 59.38%\n",
      "Run 06:\n",
      "Highest Train: 84.43\n",
      "Highest Valid: 78.49\n",
      "  Final Train: 84.39\n",
      "   Final Test: 60.87\n",
      "Run: 07\t Epoch: 10\t Loss: 0.4247\t Train: 38.54%\t Valid: 35.42%\tTest: 37.54%\n",
      "Run: 07\t Epoch: 20\t Loss: 0.4157\t Train: 38.71%\t Valid: 33.52%\tTest: 35.65%\n",
      "Run: 07\t Epoch: 30\t Loss: 0.3992\t Train: 42.74%\t Valid: 34.00%\tTest: 34.84%\n",
      "Run: 07\t Epoch: 40\t Loss: 0.3846\t Train: 48.74%\t Valid: 37.00%\tTest: 35.97%\n",
      "Run: 07\t Epoch: 50\t Loss: 0.3726\t Train: 55.04%\t Valid: 44.52%\tTest: 41.19%\n",
      "Run: 07\t Epoch: 60\t Loss: 0.3650\t Train: 57.14%\t Valid: 48.47%\tTest: 43.66%\n",
      "Run: 07\t Epoch: 70\t Loss: 0.3573\t Train: 58.58%\t Valid: 51.04%\tTest: 45.39%\n",
      "Run: 07\t Epoch: 80\t Loss: 0.3481\t Train: 61.74%\t Valid: 54.72%\tTest: 48.12%\n",
      "Run: 07\t Epoch: 90\t Loss: 0.3397\t Train: 64.47%\t Valid: 59.22%\tTest: 51.35%\n",
      "Run: 07\t Epoch: 100\t Loss: 0.3332\t Train: 66.62%\t Valid: 63.59%\tTest: 53.87%\n",
      "Run: 07\t Epoch: 110\t Loss: 0.3295\t Train: 68.59%\t Valid: 66.80%\tTest: 55.71%\n",
      "Run: 07\t Epoch: 120\t Loss: 0.3242\t Train: 69.92%\t Valid: 68.67%\tTest: 57.03%\n",
      "Run: 07\t Epoch: 130\t Loss: 0.3213\t Train: 70.53%\t Valid: 69.43%\tTest: 57.23%\n",
      "Run: 07\t Epoch: 140\t Loss: 0.3226\t Train: 68.29%\t Valid: 65.76%\tTest: 53.56%\n",
      "Run: 07\t Epoch: 150\t Loss: 0.3206\t Train: 71.64%\t Valid: 70.29%\tTest: 58.67%\n",
      "Run: 07\t Epoch: 160\t Loss: 0.3156\t Train: 71.49%\t Valid: 69.89%\tTest: 57.91%\n",
      "Run: 07\t Epoch: 170\t Loss: 0.3138\t Train: 72.67%\t Valid: 71.52%\tTest: 59.49%\n",
      "Run: 07\t Epoch: 180\t Loss: 0.3115\t Train: 73.21%\t Valid: 71.90%\tTest: 59.62%\n",
      "Run: 07\t Epoch: 190\t Loss: 0.3094\t Train: 73.68%\t Valid: 72.23%\tTest: 59.60%\n",
      "Run: 07\t Epoch: 200\t Loss: 0.3082\t Train: 74.83%\t Valid: 73.32%\tTest: 61.48%\n",
      "Run: 07\t Epoch: 210\t Loss: 0.3069\t Train: 74.61%\t Valid: 72.85%\tTest: 60.80%\n",
      "Run: 07\t Epoch: 220\t Loss: 0.3047\t Train: 74.56%\t Valid: 72.52%\tTest: 59.52%\n",
      "Run: 07\t Epoch: 230\t Loss: 0.3036\t Train: 75.50%\t Valid: 73.57%\tTest: 60.60%\n",
      "Run: 07\t Epoch: 240\t Loss: 0.3033\t Train: 74.84%\t Valid: 71.99%\tTest: 58.93%\n",
      "Run: 07\t Epoch: 250\t Loss: 0.3026\t Train: 75.82%\t Valid: 73.41%\tTest: 61.00%\n",
      "Run: 07\t Epoch: 260\t Loss: 0.3028\t Train: 75.68%\t Valid: 72.90%\tTest: 60.39%\n",
      "Run: 07\t Epoch: 270\t Loss: 0.2988\t Train: 75.91%\t Valid: 72.55%\tTest: 59.26%\n",
      "Run: 07\t Epoch: 280\t Loss: 0.2986\t Train: 76.94%\t Valid: 74.49%\tTest: 61.77%\n",
      "Run: 07\t Epoch: 290\t Loss: 0.2965\t Train: 77.06%\t Valid: 74.29%\tTest: 61.08%\n",
      "Run: 07\t Epoch: 300\t Loss: 0.2965\t Train: 76.07%\t Valid: 72.08%\tTest: 59.10%\n",
      "Run: 07\t Epoch: 310\t Loss: 0.2956\t Train: 77.93%\t Valid: 75.01%\tTest: 61.92%\n",
      "Run: 07\t Epoch: 320\t Loss: 0.2939\t Train: 77.98%\t Valid: 74.48%\tTest: 60.76%\n",
      "Run: 07\t Epoch: 330\t Loss: 0.2928\t Train: 77.67%\t Valid: 73.97%\tTest: 59.53%\n",
      "Run: 07\t Epoch: 340\t Loss: 0.2918\t Train: 78.28%\t Valid: 74.92%\tTest: 60.47%\n",
      "Run: 07\t Epoch: 350\t Loss: 0.2907\t Train: 78.19%\t Valid: 74.34%\tTest: 59.53%\n",
      "Run: 07\t Epoch: 360\t Loss: 0.2896\t Train: 78.76%\t Valid: 74.83%\tTest: 60.20%\n",
      "Run: 07\t Epoch: 370\t Loss: 0.2887\t Train: 79.10%\t Valid: 75.39%\tTest: 60.78%\n",
      "Run: 07\t Epoch: 380\t Loss: 0.2888\t Train: 78.92%\t Valid: 74.19%\tTest: 58.06%\n",
      "Run: 07\t Epoch: 390\t Loss: 0.2883\t Train: 79.04%\t Valid: 74.28%\tTest: 57.53%\n",
      "Run: 07\t Epoch: 400\t Loss: 0.2869\t Train: 79.04%\t Valid: 75.05%\tTest: 60.43%\n",
      "Run: 07\t Epoch: 410\t Loss: 0.2869\t Train: 79.35%\t Valid: 75.30%\tTest: 60.21%\n",
      "Run: 07\t Epoch: 420\t Loss: 0.2856\t Train: 79.80%\t Valid: 75.95%\tTest: 60.31%\n",
      "Run: 07\t Epoch: 430\t Loss: 0.2837\t Train: 80.28%\t Valid: 76.38%\tTest: 61.45%\n",
      "Run: 07\t Epoch: 440\t Loss: 0.2825\t Train: 80.36%\t Valid: 76.47%\tTest: 60.91%\n",
      "Run: 07\t Epoch: 450\t Loss: 0.2819\t Train: 80.56%\t Valid: 76.88%\tTest: 62.31%\n",
      "Run: 07\t Epoch: 460\t Loss: 0.2813\t Train: 80.64%\t Valid: 76.84%\tTest: 61.64%\n",
      "Run: 07\t Epoch: 470\t Loss: 0.2808\t Train: 80.58%\t Valid: 76.38%\tTest: 60.73%\n",
      "Run: 07\t Epoch: 480\t Loss: 0.2835\t Train: 80.96%\t Valid: 76.32%\tTest: 60.97%\n",
      "Run: 07\t Epoch: 490\t Loss: 0.2791\t Train: 80.71%\t Valid: 75.19%\tTest: 58.14%\n",
      "Run: 07\t Epoch: 500\t Loss: 0.2809\t Train: 80.70%\t Valid: 75.83%\tTest: 60.51%\n",
      "Run: 07\t Epoch: 510\t Loss: 0.2783\t Train: 81.00%\t Valid: 76.36%\tTest: 59.54%\n",
      "Run: 07\t Epoch: 520\t Loss: 0.2790\t Train: 81.74%\t Valid: 77.27%\tTest: 61.63%\n",
      "Run: 07\t Epoch: 530\t Loss: 0.2817\t Train: 81.49%\t Valid: 76.51%\tTest: 60.05%\n",
      "Run: 07\t Epoch: 540\t Loss: 0.2764\t Train: 81.64%\t Valid: 76.60%\tTest: 59.54%\n",
      "Run: 07\t Epoch: 550\t Loss: 0.2763\t Train: 81.55%\t Valid: 76.33%\tTest: 59.16%\n",
      "Run: 07\t Epoch: 560\t Loss: 0.2763\t Train: 81.68%\t Valid: 76.88%\tTest: 60.16%\n",
      "Run: 07\t Epoch: 570\t Loss: 0.2752\t Train: 82.16%\t Valid: 77.43%\tTest: 61.59%\n",
      "Run: 07\t Epoch: 580\t Loss: 0.2766\t Train: 82.11%\t Valid: 76.83%\tTest: 59.86%\n",
      "Run: 07\t Epoch: 590\t Loss: 0.2743\t Train: 81.79%\t Valid: 76.67%\tTest: 60.34%\n",
      "Run: 07\t Epoch: 600\t Loss: 0.2729\t Train: 82.25%\t Valid: 76.95%\tTest: 59.92%\n",
      "Run: 07\t Epoch: 610\t Loss: 0.2739\t Train: 82.35%\t Valid: 77.27%\tTest: 60.65%\n",
      "Run: 07\t Epoch: 620\t Loss: 0.2733\t Train: 82.07%\t Valid: 76.35%\tTest: 59.30%\n",
      "Run: 07\t Epoch: 630\t Loss: 0.2722\t Train: 82.30%\t Valid: 76.66%\tTest: 60.35%\n",
      "Run: 07\t Epoch: 640\t Loss: 0.2721\t Train: 82.62%\t Valid: 77.46%\tTest: 61.23%\n",
      "Run: 07\t Epoch: 650\t Loss: 0.2731\t Train: 82.73%\t Valid: 77.54%\tTest: 61.35%\n",
      "Run: 07\t Epoch: 660\t Loss: 0.2728\t Train: 82.82%\t Valid: 77.64%\tTest: 62.01%\n",
      "Run: 07\t Epoch: 670\t Loss: 0.2706\t Train: 82.68%\t Valid: 77.25%\tTest: 60.55%\n",
      "Run: 07\t Epoch: 680\t Loss: 0.2730\t Train: 82.33%\t Valid: 77.24%\tTest: 60.83%\n",
      "Run: 07\t Epoch: 690\t Loss: 0.2717\t Train: 82.95%\t Valid: 77.71%\tTest: 62.20%\n",
      "Run: 07\t Epoch: 700\t Loss: 0.2711\t Train: 82.53%\t Valid: 76.66%\tTest: 60.90%\n",
      "Run: 07\t Epoch: 710\t Loss: 0.2706\t Train: 82.93%\t Valid: 76.33%\tTest: 59.79%\n",
      "Run: 07\t Epoch: 720\t Loss: 0.2694\t Train: 82.89%\t Valid: 76.95%\tTest: 59.91%\n",
      "Run: 07\t Epoch: 730\t Loss: 0.2693\t Train: 83.16%\t Valid: 77.58%\tTest: 61.77%\n",
      "Run: 07\t Epoch: 740\t Loss: 0.2685\t Train: 83.16%\t Valid: 77.23%\tTest: 60.25%\n",
      "Run: 07\t Epoch: 750\t Loss: 0.2679\t Train: 83.36%\t Valid: 77.20%\tTest: 61.03%\n",
      "Run: 07\t Epoch: 760\t Loss: 0.2682\t Train: 83.38%\t Valid: 77.40%\tTest: 61.62%\n",
      "Run: 07\t Epoch: 770\t Loss: 0.2685\t Train: 83.45%\t Valid: 77.48%\tTest: 61.05%\n",
      "Run: 07\t Epoch: 780\t Loss: 0.2671\t Train: 83.37%\t Valid: 77.10%\tTest: 60.88%\n",
      "Run: 07\t Epoch: 790\t Loss: 0.2665\t Train: 83.54%\t Valid: 77.59%\tTest: 61.04%\n",
      "Run: 07\t Epoch: 800\t Loss: 0.2668\t Train: 83.62%\t Valid: 78.04%\tTest: 61.47%\n",
      "Run: 07\t Epoch: 810\t Loss: 0.2679\t Train: 83.62%\t Valid: 77.56%\tTest: 61.51%\n",
      "Run: 07\t Epoch: 820\t Loss: 0.2663\t Train: 83.63%\t Valid: 77.36%\tTest: 61.04%\n",
      "Run: 07\t Epoch: 830\t Loss: 0.2654\t Train: 83.64%\t Valid: 77.46%\tTest: 60.85%\n",
      "Run: 07\t Epoch: 840\t Loss: 0.2654\t Train: 83.81%\t Valid: 77.61%\tTest: 61.13%\n",
      "Run: 07\t Epoch: 850\t Loss: 0.2653\t Train: 83.82%\t Valid: 78.07%\tTest: 61.50%\n",
      "Run: 07\t Epoch: 860\t Loss: 0.2671\t Train: 83.57%\t Valid: 77.44%\tTest: 60.60%\n",
      "Run: 07\t Epoch: 870\t Loss: 0.2657\t Train: 83.70%\t Valid: 77.35%\tTest: 60.63%\n",
      "Run: 07\t Epoch: 880\t Loss: 0.2650\t Train: 83.46%\t Valid: 77.26%\tTest: 61.24%\n",
      "Run: 07\t Epoch: 890\t Loss: 0.2654\t Train: 83.79%\t Valid: 76.97%\tTest: 61.34%\n",
      "Run: 07\t Epoch: 900\t Loss: 0.2646\t Train: 84.02%\t Valid: 77.63%\tTest: 60.84%\n",
      "Run: 07\t Epoch: 910\t Loss: 0.2640\t Train: 83.99%\t Valid: 77.65%\tTest: 60.97%\n",
      "Run: 07\t Epoch: 920\t Loss: 0.2639\t Train: 83.95%\t Valid: 77.55%\tTest: 60.36%\n",
      "Run: 07\t Epoch: 930\t Loss: 0.2650\t Train: 83.96%\t Valid: 77.68%\tTest: 60.44%\n",
      "Run: 07\t Epoch: 940\t Loss: 0.2639\t Train: 84.13%\t Valid: 77.33%\tTest: 60.44%\n",
      "Run: 07\t Epoch: 950\t Loss: 0.2650\t Train: 83.87%\t Valid: 76.73%\tTest: 60.04%\n",
      "Run: 07\t Epoch: 960\t Loss: 0.2644\t Train: 84.27%\t Valid: 77.17%\tTest: 60.30%\n",
      "Run: 07\t Epoch: 970\t Loss: 0.2637\t Train: 84.32%\t Valid: 77.43%\tTest: 61.62%\n",
      "Run: 07\t Epoch: 980\t Loss: 0.2630\t Train: 84.21%\t Valid: 77.60%\tTest: 60.45%\n",
      "Run: 07\t Epoch: 990\t Loss: 0.2619\t Train: 84.32%\t Valid: 77.97%\tTest: 61.20%\n",
      "Run: 07\t Epoch: 1000\t Loss: 0.2616\t Train: 84.44%\t Valid: 77.94%\tTest: 61.14%\n",
      "Run 07:\n",
      "Highest Train: 84.44\n",
      "Highest Valid: 78.07\n",
      "  Final Train: 83.82\n",
      "   Final Test: 61.50\n",
      "Run: 08\t Epoch: 10\t Loss: 0.4259\t Train: 37.42%\t Valid: 35.16%\tTest: 37.78%\n",
      "Run: 08\t Epoch: 20\t Loss: 0.4195\t Train: 37.78%\t Valid: 33.67%\tTest: 36.17%\n",
      "Run: 08\t Epoch: 30\t Loss: 0.4002\t Train: 41.33%\t Valid: 33.61%\tTest: 34.97%\n",
      "Run: 08\t Epoch: 40\t Loss: 0.3872\t Train: 46.69%\t Valid: 35.35%\tTest: 34.97%\n",
      "Run: 08\t Epoch: 50\t Loss: 0.3757\t Train: 52.93%\t Valid: 41.12%\tTest: 38.57%\n",
      "Run: 08\t Epoch: 60\t Loss: 0.3667\t Train: 55.84%\t Valid: 46.34%\tTest: 42.27%\n",
      "Run: 08\t Epoch: 70\t Loss: 0.3597\t Train: 57.55%\t Valid: 49.56%\tTest: 44.57%\n",
      "Run: 08\t Epoch: 80\t Loss: 0.3521\t Train: 59.18%\t Valid: 51.66%\tTest: 46.30%\n",
      "Run: 08\t Epoch: 90\t Loss: 0.3427\t Train: 63.09%\t Valid: 56.53%\tTest: 49.50%\n",
      "Run: 08\t Epoch: 100\t Loss: 0.3366\t Train: 65.20%\t Valid: 60.87%\tTest: 52.18%\n",
      "Run: 08\t Epoch: 110\t Loss: 0.3315\t Train: 67.07%\t Valid: 63.92%\tTest: 53.98%\n",
      "Run: 08\t Epoch: 120\t Loss: 0.3305\t Train: 68.73%\t Valid: 66.45%\tTest: 55.60%\n",
      "Run: 08\t Epoch: 130\t Loss: 0.3258\t Train: 69.83%\t Valid: 68.42%\tTest: 56.71%\n",
      "Run: 08\t Epoch: 140\t Loss: 0.3221\t Train: 70.23%\t Valid: 68.75%\tTest: 56.65%\n",
      "Run: 08\t Epoch: 150\t Loss: 0.3197\t Train: 70.46%\t Valid: 68.93%\tTest: 56.37%\n",
      "Run: 08\t Epoch: 160\t Loss: 0.3240\t Train: 70.04%\t Valid: 68.30%\tTest: 55.84%\n",
      "Run: 08\t Epoch: 170\t Loss: 0.3163\t Train: 71.01%\t Valid: 69.21%\tTest: 56.50%\n",
      "Run: 08\t Epoch: 180\t Loss: 0.3142\t Train: 72.07%\t Valid: 70.79%\tTest: 58.19%\n",
      "Run: 08\t Epoch: 190\t Loss: 0.3121\t Train: 72.66%\t Valid: 71.53%\tTest: 58.60%\n",
      "Run: 08\t Epoch: 200\t Loss: 0.3117\t Train: 74.42%\t Valid: 73.73%\tTest: 61.58%\n",
      "Run: 08\t Epoch: 210\t Loss: 0.3082\t Train: 73.45%\t Valid: 72.08%\tTest: 59.16%\n",
      "Run: 08\t Epoch: 220\t Loss: 0.3070\t Train: 73.99%\t Valid: 72.52%\tTest: 59.46%\n",
      "Run: 08\t Epoch: 230\t Loss: 0.3046\t Train: 74.67%\t Valid: 73.19%\tTest: 59.96%\n",
      "Run: 08\t Epoch: 240\t Loss: 0.3056\t Train: 74.36%\t Valid: 72.25%\tTest: 59.12%\n",
      "Run: 08\t Epoch: 250\t Loss: 0.3016\t Train: 75.33%\t Valid: 73.29%\tTest: 60.16%\n",
      "Run: 08\t Epoch: 260\t Loss: 0.3016\t Train: 75.12%\t Valid: 72.66%\tTest: 59.30%\n",
      "Run: 08\t Epoch: 270\t Loss: 0.3004\t Train: 76.07%\t Valid: 73.66%\tTest: 61.05%\n",
      "Run: 08\t Epoch: 280\t Loss: 0.2983\t Train: 77.41%\t Valid: 75.01%\tTest: 62.00%\n",
      "Run: 08\t Epoch: 290\t Loss: 0.2970\t Train: 77.02%\t Valid: 74.05%\tTest: 60.50%\n",
      "Run: 08\t Epoch: 300\t Loss: 0.2953\t Train: 77.36%\t Valid: 74.72%\tTest: 60.98%\n",
      "Run: 08\t Epoch: 310\t Loss: 0.2934\t Train: 77.72%\t Valid: 74.54%\tTest: 60.21%\n",
      "Run: 08\t Epoch: 320\t Loss: 0.3016\t Train: 78.85%\t Valid: 76.58%\tTest: 64.41%\n",
      "Run: 08\t Epoch: 330\t Loss: 0.2924\t Train: 78.66%\t Valid: 75.93%\tTest: 62.61%\n",
      "Run: 08\t Epoch: 340\t Loss: 0.2912\t Train: 78.68%\t Valid: 75.48%\tTest: 61.46%\n",
      "Run: 08\t Epoch: 350\t Loss: 0.2892\t Train: 78.83%\t Valid: 75.24%\tTest: 60.62%\n",
      "Run: 08\t Epoch: 360\t Loss: 0.2880\t Train: 79.22%\t Valid: 75.76%\tTest: 61.05%\n",
      "Run: 08\t Epoch: 370\t Loss: 0.2870\t Train: 79.47%\t Valid: 75.62%\tTest: 60.80%\n",
      "Run: 08\t Epoch: 380\t Loss: 0.2872\t Train: 79.77%\t Valid: 76.00%\tTest: 61.32%\n",
      "Run: 08\t Epoch: 390\t Loss: 0.2857\t Train: 79.83%\t Valid: 75.92%\tTest: 61.04%\n",
      "Run: 08\t Epoch: 400\t Loss: 0.2849\t Train: 79.71%\t Valid: 75.43%\tTest: 59.60%\n",
      "Run: 08\t Epoch: 410\t Loss: 0.2874\t Train: 80.21%\t Valid: 76.59%\tTest: 62.16%\n",
      "Run: 08\t Epoch: 420\t Loss: 0.2885\t Train: 79.34%\t Valid: 75.58%\tTest: 59.33%\n",
      "Run: 08\t Epoch: 430\t Loss: 0.2872\t Train: 80.09%\t Valid: 76.34%\tTest: 60.73%\n",
      "Run: 08\t Epoch: 440\t Loss: 0.2822\t Train: 80.44%\t Valid: 75.64%\tTest: 59.86%\n",
      "Run: 08\t Epoch: 450\t Loss: 0.2823\t Train: 80.72%\t Valid: 76.61%\tTest: 60.52%\n",
      "Run: 08\t Epoch: 460\t Loss: 0.2806\t Train: 80.80%\t Valid: 76.83%\tTest: 61.49%\n",
      "Run: 08\t Epoch: 470\t Loss: 0.2798\t Train: 80.85%\t Valid: 76.54%\tTest: 60.95%\n",
      "Run: 08\t Epoch: 480\t Loss: 0.2803\t Train: 81.12%\t Valid: 76.94%\tTest: 61.31%\n",
      "Run: 08\t Epoch: 490\t Loss: 0.2794\t Train: 81.34%\t Valid: 76.82%\tTest: 60.64%\n",
      "Run: 08\t Epoch: 500\t Loss: 0.2792\t Train: 81.19%\t Valid: 75.91%\tTest: 59.13%\n",
      "Run: 08\t Epoch: 510\t Loss: 0.2795\t Train: 81.58%\t Valid: 76.84%\tTest: 60.95%\n",
      "Run: 08\t Epoch: 520\t Loss: 0.2783\t Train: 80.88%\t Valid: 75.68%\tTest: 58.32%\n",
      "Run: 08\t Epoch: 530\t Loss: 0.2768\t Train: 81.63%\t Valid: 77.08%\tTest: 60.59%\n",
      "Run: 08\t Epoch: 540\t Loss: 0.2770\t Train: 81.90%\t Valid: 77.24%\tTest: 60.72%\n",
      "Run: 08\t Epoch: 550\t Loss: 0.2753\t Train: 81.64%\t Valid: 76.46%\tTest: 59.95%\n",
      "Run: 08\t Epoch: 560\t Loss: 0.2748\t Train: 81.93%\t Valid: 77.37%\tTest: 61.32%\n",
      "Run: 08\t Epoch: 570\t Loss: 0.2742\t Train: 82.08%\t Valid: 77.11%\tTest: 60.22%\n",
      "Run: 08\t Epoch: 580\t Loss: 0.2753\t Train: 82.36%\t Valid: 77.39%\tTest: 60.98%\n",
      "Run: 08\t Epoch: 590\t Loss: 0.2772\t Train: 82.18%\t Valid: 77.54%\tTest: 62.09%\n",
      "Run: 08\t Epoch: 600\t Loss: 0.2758\t Train: 81.87%\t Valid: 77.63%\tTest: 61.06%\n",
      "Run: 08\t Epoch: 610\t Loss: 0.2731\t Train: 82.46%\t Valid: 78.02%\tTest: 61.98%\n",
      "Run: 08\t Epoch: 620\t Loss: 0.2722\t Train: 82.39%\t Valid: 77.71%\tTest: 61.61%\n",
      "Run: 08\t Epoch: 630\t Loss: 0.2723\t Train: 82.59%\t Valid: 77.72%\tTest: 61.16%\n",
      "Run: 08\t Epoch: 640\t Loss: 0.2713\t Train: 82.36%\t Valid: 77.24%\tTest: 60.69%\n",
      "Run: 08\t Epoch: 650\t Loss: 0.2713\t Train: 82.40%\t Valid: 77.11%\tTest: 60.41%\n",
      "Run: 08\t Epoch: 660\t Loss: 0.2711\t Train: 82.57%\t Valid: 77.37%\tTest: 61.20%\n",
      "Run: 08\t Epoch: 670\t Loss: 0.2704\t Train: 82.62%\t Valid: 77.26%\tTest: 60.16%\n",
      "Run: 08\t Epoch: 680\t Loss: 0.2698\t Train: 82.81%\t Valid: 77.70%\tTest: 61.89%\n",
      "Run: 08\t Epoch: 690\t Loss: 0.2698\t Train: 82.85%\t Valid: 77.18%\tTest: 60.23%\n",
      "Run: 08\t Epoch: 700\t Loss: 0.2686\t Train: 83.12%\t Valid: 77.61%\tTest: 61.53%\n",
      "Run: 08\t Epoch: 710\t Loss: 0.2706\t Train: 83.20%\t Valid: 77.40%\tTest: 60.99%\n",
      "Run: 08\t Epoch: 720\t Loss: 0.2681\t Train: 83.04%\t Valid: 77.21%\tTest: 59.64%\n",
      "Run: 08\t Epoch: 730\t Loss: 0.2691\t Train: 82.91%\t Valid: 77.24%\tTest: 60.14%\n",
      "Run: 08\t Epoch: 740\t Loss: 0.2672\t Train: 83.39%\t Valid: 77.64%\tTest: 61.34%\n",
      "Run: 08\t Epoch: 750\t Loss: 0.2681\t Train: 83.41%\t Valid: 77.30%\tTest: 60.45%\n",
      "Run: 08\t Epoch: 760\t Loss: 0.2673\t Train: 83.43%\t Valid: 77.57%\tTest: 60.83%\n",
      "Run: 08\t Epoch: 770\t Loss: 0.2662\t Train: 83.50%\t Valid: 77.52%\tTest: 60.82%\n",
      "Run: 08\t Epoch: 780\t Loss: 0.2672\t Train: 83.39%\t Valid: 77.23%\tTest: 60.64%\n",
      "Run: 08\t Epoch: 790\t Loss: 0.2698\t Train: 83.26%\t Valid: 77.62%\tTest: 61.17%\n",
      "Run: 08\t Epoch: 800\t Loss: 0.2677\t Train: 83.64%\t Valid: 77.04%\tTest: 60.59%\n",
      "Run: 08\t Epoch: 810\t Loss: 0.2655\t Train: 83.46%\t Valid: 76.36%\tTest: 59.38%\n",
      "Run: 08\t Epoch: 820\t Loss: 0.2656\t Train: 83.65%\t Valid: 77.00%\tTest: 60.07%\n",
      "Run: 08\t Epoch: 830\t Loss: 0.2655\t Train: 83.55%\t Valid: 77.21%\tTest: 61.06%\n",
      "Run: 08\t Epoch: 840\t Loss: 0.2645\t Train: 83.77%\t Valid: 76.78%\tTest: 59.69%\n",
      "Run: 08\t Epoch: 850\t Loss: 0.2645\t Train: 83.91%\t Valid: 77.55%\tTest: 61.00%\n",
      "Run: 08\t Epoch: 860\t Loss: 0.2635\t Train: 83.96%\t Valid: 77.31%\tTest: 60.93%\n",
      "Run: 08\t Epoch: 870\t Loss: 0.2654\t Train: 84.07%\t Valid: 77.57%\tTest: 61.81%\n",
      "Run: 08\t Epoch: 880\t Loss: 0.2644\t Train: 83.94%\t Valid: 77.56%\tTest: 60.87%\n",
      "Run: 08\t Epoch: 890\t Loss: 0.2640\t Train: 84.05%\t Valid: 77.43%\tTest: 60.28%\n",
      "Run: 08\t Epoch: 900\t Loss: 0.2625\t Train: 84.02%\t Valid: 77.06%\tTest: 60.22%\n",
      "Run: 08\t Epoch: 910\t Loss: 0.2634\t Train: 84.22%\t Valid: 77.81%\tTest: 61.41%\n",
      "Run: 08\t Epoch: 920\t Loss: 0.2632\t Train: 84.19%\t Valid: 77.36%\tTest: 60.55%\n",
      "Run: 08\t Epoch: 930\t Loss: 0.2628\t Train: 84.24%\t Valid: 77.69%\tTest: 60.85%\n",
      "Run: 08\t Epoch: 940\t Loss: 0.2624\t Train: 84.23%\t Valid: 77.66%\tTest: 61.19%\n",
      "Run: 08\t Epoch: 950\t Loss: 0.2616\t Train: 84.25%\t Valid: 77.44%\tTest: 60.24%\n",
      "Run: 08\t Epoch: 960\t Loss: 0.2637\t Train: 84.32%\t Valid: 77.21%\tTest: 60.05%\n",
      "Run: 08\t Epoch: 970\t Loss: 0.2629\t Train: 84.29%\t Valid: 77.00%\tTest: 59.50%\n",
      "Run: 08\t Epoch: 980\t Loss: 0.2620\t Train: 84.42%\t Valid: 77.93%\tTest: 61.26%\n",
      "Run: 08\t Epoch: 990\t Loss: 0.2626\t Train: 84.46%\t Valid: 77.62%\tTest: 61.01%\n",
      "Run: 08\t Epoch: 1000\t Loss: 0.2614\t Train: 84.47%\t Valid: 77.88%\tTest: 60.68%\n",
      "Run 08:\n",
      "Highest Train: 84.47\n",
      "Highest Valid: 78.02\n",
      "  Final Train: 82.46\n",
      "   Final Test: 61.98\n",
      "Run: 09\t Epoch: 10\t Loss: 0.4251\t Train: 38.39%\t Valid: 35.88%\tTest: 38.06%\n",
      "Run: 09\t Epoch: 20\t Loss: 0.4156\t Train: 38.73%\t Valid: 33.76%\tTest: 35.95%\n",
      "Run: 09\t Epoch: 30\t Loss: 0.3978\t Train: 43.69%\t Valid: 34.54%\tTest: 35.14%\n",
      "Run: 09\t Epoch: 40\t Loss: 0.3803\t Train: 51.69%\t Valid: 40.37%\tTest: 38.60%\n",
      "Run: 09\t Epoch: 50\t Loss: 0.3703\t Train: 54.99%\t Valid: 46.00%\tTest: 42.12%\n",
      "Run: 09\t Epoch: 60\t Loss: 0.3623\t Train: 56.94%\t Valid: 48.92%\tTest: 43.79%\n",
      "Run: 09\t Epoch: 70\t Loss: 0.3538\t Train: 58.87%\t Valid: 51.64%\tTest: 45.91%\n",
      "Run: 09\t Epoch: 80\t Loss: 0.3443\t Train: 62.45%\t Valid: 56.33%\tTest: 49.15%\n",
      "Run: 09\t Epoch: 90\t Loss: 0.3357\t Train: 65.93%\t Valid: 62.46%\tTest: 53.10%\n",
      "Run: 09\t Epoch: 100\t Loss: 0.3304\t Train: 67.53%\t Valid: 65.67%\tTest: 54.89%\n",
      "Run: 09\t Epoch: 110\t Loss: 0.3278\t Train: 69.48%\t Valid: 68.64%\tTest: 57.43%\n",
      "Run: 09\t Epoch: 120\t Loss: 0.3244\t Train: 68.85%\t Valid: 67.15%\tTest: 55.58%\n",
      "Run: 09\t Epoch: 130\t Loss: 0.3216\t Train: 70.02%\t Valid: 68.76%\tTest: 56.82%\n",
      "Run: 09\t Epoch: 140\t Loss: 0.3192\t Train: 70.55%\t Valid: 69.13%\tTest: 56.64%\n",
      "Run: 09\t Epoch: 150\t Loss: 0.3167\t Train: 71.44%\t Valid: 70.20%\tTest: 57.65%\n",
      "Run: 09\t Epoch: 160\t Loss: 0.3149\t Train: 71.56%\t Valid: 69.91%\tTest: 57.49%\n",
      "Run: 09\t Epoch: 170\t Loss: 0.3135\t Train: 73.24%\t Valid: 72.37%\tTest: 60.45%\n",
      "Run: 09\t Epoch: 180\t Loss: 0.3117\t Train: 72.78%\t Valid: 71.27%\tTest: 59.04%\n",
      "Run: 09\t Epoch: 190\t Loss: 0.3098\t Train: 73.22%\t Valid: 71.56%\tTest: 59.22%\n",
      "Run: 09\t Epoch: 200\t Loss: 0.3095\t Train: 74.52%\t Valid: 73.39%\tTest: 61.77%\n",
      "Run: 09\t Epoch: 210\t Loss: 0.3068\t Train: 73.79%\t Valid: 71.67%\tTest: 59.53%\n",
      "Run: 09\t Epoch: 220\t Loss: 0.3051\t Train: 74.92%\t Valid: 73.26%\tTest: 61.35%\n",
      "Run: 09\t Epoch: 230\t Loss: 0.3034\t Train: 75.60%\t Valid: 73.61%\tTest: 61.25%\n",
      "Run: 09\t Epoch: 240\t Loss: 0.3061\t Train: 75.80%\t Valid: 73.80%\tTest: 62.24%\n",
      "Run: 09\t Epoch: 250\t Loss: 0.3024\t Train: 76.05%\t Valid: 73.74%\tTest: 61.99%\n",
      "Run: 09\t Epoch: 260\t Loss: 0.2993\t Train: 76.39%\t Valid: 74.24%\tTest: 62.30%\n",
      "Run: 09\t Epoch: 270\t Loss: 0.2979\t Train: 77.02%\t Valid: 74.27%\tTest: 60.90%\n",
      "Run: 09\t Epoch: 280\t Loss: 0.2989\t Train: 77.00%\t Valid: 74.19%\tTest: 60.84%\n",
      "Run: 09\t Epoch: 290\t Loss: 0.2962\t Train: 76.94%\t Valid: 74.01%\tTest: 60.09%\n",
      "Run: 09\t Epoch: 300\t Loss: 0.2944\t Train: 77.38%\t Valid: 74.45%\tTest: 60.74%\n",
      "Run: 09\t Epoch: 310\t Loss: 0.2942\t Train: 78.28%\t Valid: 75.32%\tTest: 61.46%\n",
      "Run: 09\t Epoch: 320\t Loss: 0.2927\t Train: 78.61%\t Valid: 75.52%\tTest: 61.41%\n",
      "Run: 09\t Epoch: 330\t Loss: 0.2912\t Train: 78.51%\t Valid: 75.12%\tTest: 60.27%\n",
      "Run: 09\t Epoch: 340\t Loss: 0.2899\t Train: 79.02%\t Valid: 75.55%\tTest: 60.61%\n",
      "Run: 09\t Epoch: 350\t Loss: 0.2938\t Train: 78.41%\t Valid: 74.42%\tTest: 58.67%\n",
      "Run: 09\t Epoch: 360\t Loss: 0.2907\t Train: 78.73%\t Valid: 74.65%\tTest: 58.92%\n",
      "Run: 09\t Epoch: 370\t Loss: 0.2889\t Train: 79.36%\t Valid: 75.77%\tTest: 60.61%\n",
      "Run: 09\t Epoch: 380\t Loss: 0.2867\t Train: 79.60%\t Valid: 76.19%\tTest: 61.48%\n",
      "Run: 09\t Epoch: 390\t Loss: 0.2875\t Train: 79.29%\t Valid: 75.47%\tTest: 59.69%\n",
      "Run: 09\t Epoch: 400\t Loss: 0.2849\t Train: 79.83%\t Valid: 75.65%\tTest: 59.88%\n",
      "Run: 09\t Epoch: 410\t Loss: 0.2881\t Train: 79.43%\t Valid: 75.56%\tTest: 60.07%\n",
      "Run: 09\t Epoch: 420\t Loss: 0.2837\t Train: 80.44%\t Valid: 76.61%\tTest: 61.81%\n",
      "Run: 09\t Epoch: 430\t Loss: 0.2844\t Train: 80.36%\t Valid: 76.40%\tTest: 60.61%\n",
      "Run: 09\t Epoch: 440\t Loss: 0.2835\t Train: 80.16%\t Valid: 75.83%\tTest: 59.37%\n",
      "Run: 09\t Epoch: 450\t Loss: 0.2826\t Train: 80.41%\t Valid: 75.91%\tTest: 59.00%\n",
      "Run: 09\t Epoch: 460\t Loss: 0.2819\t Train: 80.33%\t Valid: 75.84%\tTest: 58.72%\n",
      "Run: 09\t Epoch: 470\t Loss: 0.2799\t Train: 80.97%\t Valid: 76.49%\tTest: 59.34%\n",
      "Run: 09\t Epoch: 480\t Loss: 0.2815\t Train: 81.20%\t Valid: 76.31%\tTest: 59.11%\n",
      "Run: 09\t Epoch: 490\t Loss: 0.2789\t Train: 81.19%\t Valid: 76.48%\tTest: 59.42%\n",
      "Run: 09\t Epoch: 500\t Loss: 0.2781\t Train: 81.14%\t Valid: 76.42%\tTest: 59.30%\n",
      "Run: 09\t Epoch: 510\t Loss: 0.2788\t Train: 80.97%\t Valid: 75.89%\tTest: 58.35%\n",
      "Run: 09\t Epoch: 520\t Loss: 0.2814\t Train: 81.58%\t Valid: 76.94%\tTest: 60.37%\n",
      "Run: 09\t Epoch: 530\t Loss: 0.2770\t Train: 81.39%\t Valid: 76.26%\tTest: 58.07%\n",
      "Run: 09\t Epoch: 540\t Loss: 0.2760\t Train: 81.48%\t Valid: 76.26%\tTest: 58.38%\n",
      "Run: 09\t Epoch: 550\t Loss: 0.2755\t Train: 81.77%\t Valid: 76.96%\tTest: 60.00%\n",
      "Run: 09\t Epoch: 560\t Loss: 0.2758\t Train: 81.92%\t Valid: 76.61%\tTest: 58.72%\n",
      "Run: 09\t Epoch: 570\t Loss: 0.2744\t Train: 82.04%\t Valid: 77.26%\tTest: 60.04%\n",
      "Run: 09\t Epoch: 580\t Loss: 0.2737\t Train: 82.12%\t Valid: 76.79%\tTest: 58.71%\n",
      "Run: 09\t Epoch: 590\t Loss: 0.2753\t Train: 81.36%\t Valid: 75.40%\tTest: 57.41%\n",
      "Run: 09\t Epoch: 600\t Loss: 0.2747\t Train: 82.06%\t Valid: 76.93%\tTest: 60.51%\n",
      "Run: 09\t Epoch: 610\t Loss: 0.2725\t Train: 82.41%\t Valid: 76.89%\tTest: 58.99%\n",
      "Run: 09\t Epoch: 620\t Loss: 0.2721\t Train: 82.50%\t Valid: 77.36%\tTest: 60.05%\n",
      "Run: 09\t Epoch: 630\t Loss: 0.2720\t Train: 82.63%\t Valid: 77.32%\tTest: 60.05%\n",
      "Run: 09\t Epoch: 640\t Loss: 0.2708\t Train: 82.67%\t Valid: 77.18%\tTest: 59.32%\n",
      "Run: 09\t Epoch: 650\t Loss: 0.2705\t Train: 82.61%\t Valid: 76.69%\tTest: 58.66%\n",
      "Run: 09\t Epoch: 660\t Loss: 0.2708\t Train: 82.93%\t Valid: 77.41%\tTest: 59.56%\n",
      "Run: 09\t Epoch: 670\t Loss: 0.2718\t Train: 82.74%\t Valid: 77.17%\tTest: 59.50%\n",
      "Run: 09\t Epoch: 680\t Loss: 0.2699\t Train: 83.00%\t Valid: 77.71%\tTest: 59.67%\n",
      "Run: 09\t Epoch: 690\t Loss: 0.2702\t Train: 82.93%\t Valid: 76.75%\tTest: 58.73%\n",
      "Run: 09\t Epoch: 700\t Loss: 0.2699\t Train: 82.89%\t Valid: 77.17%\tTest: 59.97%\n",
      "Run: 09\t Epoch: 710\t Loss: 0.2687\t Train: 83.02%\t Valid: 77.46%\tTest: 59.11%\n",
      "Run: 09\t Epoch: 720\t Loss: 0.2683\t Train: 83.20%\t Valid: 77.47%\tTest: 59.55%\n",
      "Run: 09\t Epoch: 730\t Loss: 0.2676\t Train: 83.15%\t Valid: 77.08%\tTest: 59.14%\n",
      "Run: 09\t Epoch: 740\t Loss: 0.2691\t Train: 83.37%\t Valid: 77.37%\tTest: 59.19%\n",
      "Run: 09\t Epoch: 750\t Loss: 0.2694\t Train: 82.76%\t Valid: 77.17%\tTest: 58.35%\n",
      "Run: 09\t Epoch: 760\t Loss: 0.2694\t Train: 83.33%\t Valid: 78.34%\tTest: 61.46%\n",
      "Run: 09\t Epoch: 770\t Loss: 0.2681\t Train: 83.42%\t Valid: 77.14%\tTest: 59.57%\n",
      "Run: 09\t Epoch: 780\t Loss: 0.2667\t Train: 83.42%\t Valid: 77.18%\tTest: 59.35%\n",
      "Run: 09\t Epoch: 790\t Loss: 0.2673\t Train: 83.61%\t Valid: 77.96%\tTest: 59.95%\n",
      "Run: 09\t Epoch: 800\t Loss: 0.2660\t Train: 83.58%\t Valid: 77.83%\tTest: 59.81%\n",
      "Run: 09\t Epoch: 810\t Loss: 0.2654\t Train: 83.76%\t Valid: 77.72%\tTest: 59.00%\n",
      "Run: 09\t Epoch: 820\t Loss: 0.2653\t Train: 83.85%\t Valid: 78.07%\tTest: 60.35%\n",
      "Run: 09\t Epoch: 830\t Loss: 0.2649\t Train: 83.77%\t Valid: 77.79%\tTest: 59.35%\n",
      "Run: 09\t Epoch: 840\t Loss: 0.2652\t Train: 83.63%\t Valid: 77.99%\tTest: 59.14%\n",
      "Run: 09\t Epoch: 850\t Loss: 0.2646\t Train: 83.96%\t Valid: 77.87%\tTest: 59.40%\n",
      "Run: 09\t Epoch: 860\t Loss: 0.2650\t Train: 84.03%\t Valid: 77.67%\tTest: 58.93%\n",
      "Run: 09\t Epoch: 870\t Loss: 0.2649\t Train: 83.89%\t Valid: 77.68%\tTest: 59.99%\n",
      "Run: 09\t Epoch: 880\t Loss: 0.2640\t Train: 84.03%\t Valid: 77.91%\tTest: 59.69%\n",
      "Run: 09\t Epoch: 890\t Loss: 0.2635\t Train: 84.17%\t Valid: 78.08%\tTest: 59.59%\n",
      "Run: 09\t Epoch: 900\t Loss: 0.2629\t Train: 84.13%\t Valid: 77.76%\tTest: 59.34%\n",
      "Run: 09\t Epoch: 910\t Loss: 0.2643\t Train: 84.14%\t Valid: 78.22%\tTest: 59.87%\n",
      "Run: 09\t Epoch: 920\t Loss: 0.2637\t Train: 84.04%\t Valid: 77.95%\tTest: 59.21%\n",
      "Run: 09\t Epoch: 930\t Loss: 0.2639\t Train: 84.12%\t Valid: 78.17%\tTest: 59.70%\n",
      "Run: 09\t Epoch: 940\t Loss: 0.2630\t Train: 84.31%\t Valid: 77.79%\tTest: 59.17%\n",
      "Run: 09\t Epoch: 950\t Loss: 0.2620\t Train: 84.29%\t Valid: 77.92%\tTest: 59.16%\n",
      "Run: 09\t Epoch: 960\t Loss: 0.2632\t Train: 84.44%\t Valid: 78.02%\tTest: 59.38%\n",
      "Run: 09\t Epoch: 970\t Loss: 0.2617\t Train: 84.42%\t Valid: 78.04%\tTest: 60.02%\n",
      "Run: 09\t Epoch: 980\t Loss: 0.2627\t Train: 84.48%\t Valid: 77.85%\tTest: 59.38%\n",
      "Run: 09\t Epoch: 990\t Loss: 0.2618\t Train: 84.53%\t Valid: 77.92%\tTest: 59.28%\n",
      "Run: 09\t Epoch: 1000\t Loss: 0.2622\t Train: 84.48%\t Valid: 77.81%\tTest: 58.97%\n",
      "Run 09:\n",
      "Highest Train: 84.53\n",
      "Highest Valid: 78.34\n",
      "  Final Train: 83.33\n",
      "   Final Test: 61.46\n",
      "Run: 10\t Epoch: 10\t Loss: 0.4261\t Train: 38.07%\t Valid: 35.59%\tTest: 37.82%\n",
      "Run: 10\t Epoch: 20\t Loss: 0.4161\t Train: 38.74%\t Valid: 33.85%\tTest: 35.92%\n",
      "Run: 10\t Epoch: 30\t Loss: 0.3997\t Train: 42.64%\t Valid: 34.06%\tTest: 34.94%\n",
      "Run: 10\t Epoch: 40\t Loss: 0.3837\t Train: 49.05%\t Valid: 38.04%\tTest: 37.18%\n",
      "Run: 10\t Epoch: 50\t Loss: 0.3717\t Train: 55.36%\t Valid: 45.82%\tTest: 42.55%\n",
      "Run: 10\t Epoch: 60\t Loss: 0.3628\t Train: 57.93%\t Valid: 50.21%\tTest: 45.23%\n",
      "Run: 10\t Epoch: 70\t Loss: 0.3539\t Train: 59.61%\t Valid: 52.81%\tTest: 47.13%\n",
      "Run: 10\t Epoch: 80\t Loss: 0.3447\t Train: 62.23%\t Valid: 56.36%\tTest: 49.56%\n",
      "Run: 10\t Epoch: 90\t Loss: 0.3364\t Train: 65.51%\t Valid: 62.23%\tTest: 53.28%\n",
      "Run: 10\t Epoch: 100\t Loss: 0.3315\t Train: 67.60%\t Valid: 66.58%\tTest: 56.20%\n",
      "Run: 10\t Epoch: 110\t Loss: 0.3283\t Train: 67.37%\t Valid: 66.07%\tTest: 55.96%\n",
      "Run: 10\t Epoch: 120\t Loss: 0.3253\t Train: 68.53%\t Valid: 67.59%\tTest: 56.98%\n",
      "Run: 10\t Epoch: 130\t Loss: 0.3230\t Train: 69.82%\t Valid: 69.04%\tTest: 58.06%\n",
      "Run: 10\t Epoch: 140\t Loss: 0.3199\t Train: 70.81%\t Valid: 70.20%\tTest: 58.80%\n",
      "Run: 10\t Epoch: 150\t Loss: 0.3174\t Train: 72.01%\t Valid: 71.53%\tTest: 59.89%\n",
      "Run: 10\t Epoch: 160\t Loss: 0.3150\t Train: 72.81%\t Valid: 72.33%\tTest: 60.55%\n",
      "Run: 10\t Epoch: 170\t Loss: 0.3136\t Train: 71.51%\t Valid: 69.75%\tTest: 57.99%\n",
      "Run: 10\t Epoch: 180\t Loss: 0.3116\t Train: 72.94%\t Valid: 71.71%\tTest: 59.83%\n",
      "Run: 10\t Epoch: 190\t Loss: 0.3097\t Train: 73.27%\t Valid: 71.85%\tTest: 60.05%\n",
      "Run: 10\t Epoch: 200\t Loss: 0.3119\t Train: 72.79%\t Valid: 71.07%\tTest: 59.73%\n",
      "Run: 10\t Epoch: 210\t Loss: 0.3071\t Train: 74.97%\t Valid: 73.80%\tTest: 62.20%\n",
      "Run: 10\t Epoch: 220\t Loss: 0.3051\t Train: 74.39%\t Valid: 72.39%\tTest: 60.85%\n",
      "Run: 10\t Epoch: 230\t Loss: 0.3047\t Train: 74.68%\t Valid: 72.47%\tTest: 61.05%\n",
      "Run: 10\t Epoch: 240\t Loss: 0.3031\t Train: 74.92%\t Valid: 72.53%\tTest: 60.95%\n",
      "Run: 10\t Epoch: 250\t Loss: 0.3011\t Train: 75.56%\t Valid: 73.26%\tTest: 61.53%\n",
      "Run: 10\t Epoch: 260\t Loss: 0.3009\t Train: 75.74%\t Valid: 73.37%\tTest: 61.69%\n",
      "Run: 10\t Epoch: 270\t Loss: 0.2998\t Train: 75.76%\t Valid: 73.10%\tTest: 61.54%\n",
      "Run: 10\t Epoch: 280\t Loss: 0.2978\t Train: 76.34%\t Valid: 73.92%\tTest: 62.78%\n",
      "Run: 10\t Epoch: 290\t Loss: 0.2978\t Train: 76.35%\t Valid: 73.56%\tTest: 61.93%\n",
      "Run: 10\t Epoch: 300\t Loss: 0.2961\t Train: 77.04%\t Valid: 74.37%\tTest: 62.45%\n",
      "Run: 10\t Epoch: 310\t Loss: 0.2961\t Train: 77.62%\t Valid: 75.38%\tTest: 63.80%\n",
      "Run: 10\t Epoch: 320\t Loss: 0.2974\t Train: 77.07%\t Valid: 73.97%\tTest: 62.31%\n",
      "Run: 10\t Epoch: 330\t Loss: 0.2930\t Train: 78.19%\t Valid: 75.14%\tTest: 63.46%\n",
      "Run: 10\t Epoch: 340\t Loss: 0.2934\t Train: 78.47%\t Valid: 75.34%\tTest: 63.29%\n",
      "Run: 10\t Epoch: 350\t Loss: 0.2911\t Train: 78.34%\t Valid: 74.64%\tTest: 62.73%\n",
      "Run: 10\t Epoch: 360\t Loss: 0.2898\t Train: 78.49%\t Valid: 74.38%\tTest: 62.17%\n",
      "Run: 10\t Epoch: 370\t Loss: 0.2902\t Train: 79.30%\t Valid: 75.79%\tTest: 64.01%\n",
      "Run: 10\t Epoch: 380\t Loss: 0.2877\t Train: 79.36%\t Valid: 75.30%\tTest: 63.21%\n",
      "Run: 10\t Epoch: 390\t Loss: 0.2908\t Train: 79.40%\t Valid: 75.64%\tTest: 65.48%\n",
      "Run: 10\t Epoch: 400\t Loss: 0.2864\t Train: 79.22%\t Valid: 74.33%\tTest: 61.33%\n",
      "Run: 10\t Epoch: 410\t Loss: 0.2859\t Train: 79.66%\t Valid: 75.43%\tTest: 63.54%\n",
      "Run: 10\t Epoch: 420\t Loss: 0.2868\t Train: 79.27%\t Valid: 73.94%\tTest: 60.30%\n",
      "Run: 10\t Epoch: 430\t Loss: 0.2835\t Train: 80.14%\t Valid: 75.58%\tTest: 63.01%\n",
      "Run: 10\t Epoch: 440\t Loss: 0.2848\t Train: 79.21%\t Valid: 73.75%\tTest: 60.88%\n",
      "Run: 10\t Epoch: 450\t Loss: 0.2836\t Train: 79.78%\t Valid: 74.87%\tTest: 62.18%\n",
      "Run: 10\t Epoch: 460\t Loss: 0.2834\t Train: 80.11%\t Valid: 74.62%\tTest: 60.21%\n",
      "Run: 10\t Epoch: 470\t Loss: 0.2840\t Train: 80.54%\t Valid: 75.38%\tTest: 61.04%\n",
      "Run: 10\t Epoch: 480\t Loss: 0.2813\t Train: 81.12%\t Valid: 76.14%\tTest: 62.02%\n",
      "Run: 10\t Epoch: 490\t Loss: 0.2795\t Train: 80.57%\t Valid: 75.19%\tTest: 61.02%\n",
      "Run: 10\t Epoch: 500\t Loss: 0.2789\t Train: 81.03%\t Valid: 75.77%\tTest: 61.52%\n",
      "Run: 10\t Epoch: 510\t Loss: 0.2783\t Train: 81.14%\t Valid: 75.76%\tTest: 61.10%\n",
      "Run: 10\t Epoch: 520\t Loss: 0.2784\t Train: 81.36%\t Valid: 75.89%\tTest: 61.23%\n",
      "Run: 10\t Epoch: 530\t Loss: 0.2784\t Train: 80.84%\t Valid: 75.23%\tTest: 60.73%\n",
      "Run: 10\t Epoch: 540\t Loss: 0.2800\t Train: 80.85%\t Valid: 75.37%\tTest: 61.02%\n",
      "Run: 10\t Epoch: 550\t Loss: 0.2773\t Train: 81.61%\t Valid: 76.06%\tTest: 60.97%\n",
      "Run: 10\t Epoch: 560\t Loss: 0.2754\t Train: 81.88%\t Valid: 76.85%\tTest: 62.91%\n",
      "Run: 10\t Epoch: 570\t Loss: 0.2758\t Train: 81.79%\t Valid: 76.45%\tTest: 61.85%\n",
      "Run: 10\t Epoch: 580\t Loss: 0.2745\t Train: 81.95%\t Valid: 76.36%\tTest: 61.30%\n",
      "Run: 10\t Epoch: 590\t Loss: 0.2752\t Train: 82.14%\t Valid: 76.73%\tTest: 61.99%\n",
      "Run: 10\t Epoch: 600\t Loss: 0.2746\t Train: 81.78%\t Valid: 76.22%\tTest: 61.08%\n",
      "Run: 10\t Epoch: 610\t Loss: 0.2735\t Train: 82.10%\t Valid: 76.75%\tTest: 62.14%\n",
      "Run: 10\t Epoch: 620\t Loss: 0.2785\t Train: 82.38%\t Valid: 77.23%\tTest: 62.47%\n",
      "Run: 10\t Epoch: 630\t Loss: 0.2753\t Train: 82.23%\t Valid: 76.83%\tTest: 61.89%\n",
      "Run: 10\t Epoch: 640\t Loss: 0.2731\t Train: 82.26%\t Valid: 76.04%\tTest: 60.16%\n",
      "Run: 10\t Epoch: 650\t Loss: 0.2720\t Train: 82.57%\t Valid: 76.86%\tTest: 61.24%\n",
      "Run: 10\t Epoch: 660\t Loss: 0.2729\t Train: 82.61%\t Valid: 76.56%\tTest: 60.74%\n",
      "Run: 10\t Epoch: 670\t Loss: 0.2721\t Train: 82.69%\t Valid: 77.01%\tTest: 62.38%\n",
      "Run: 10\t Epoch: 680\t Loss: 0.2717\t Train: 82.30%\t Valid: 75.99%\tTest: 60.65%\n",
      "Run: 10\t Epoch: 690\t Loss: 0.2717\t Train: 82.93%\t Valid: 76.95%\tTest: 61.61%\n",
      "Run: 10\t Epoch: 700\t Loss: 0.2720\t Train: 82.83%\t Valid: 76.94%\tTest: 61.64%\n",
      "Run: 10\t Epoch: 710\t Loss: 0.2702\t Train: 82.91%\t Valid: 76.71%\tTest: 60.64%\n",
      "Run: 10\t Epoch: 720\t Loss: 0.2710\t Train: 83.05%\t Valid: 77.26%\tTest: 61.66%\n",
      "Run: 10\t Epoch: 730\t Loss: 0.2702\t Train: 82.62%\t Valid: 76.05%\tTest: 59.94%\n",
      "Run: 10\t Epoch: 740\t Loss: 0.2691\t Train: 83.19%\t Valid: 76.48%\tTest: 60.70%\n",
      "Run: 10\t Epoch: 750\t Loss: 0.2689\t Train: 83.15%\t Valid: 76.68%\tTest: 60.46%\n",
      "Run: 10\t Epoch: 760\t Loss: 0.2682\t Train: 83.10%\t Valid: 76.90%\tTest: 60.52%\n",
      "Run: 10\t Epoch: 770\t Loss: 0.2682\t Train: 83.30%\t Valid: 76.92%\tTest: 60.33%\n",
      "Run: 10\t Epoch: 780\t Loss: 0.2681\t Train: 83.36%\t Valid: 76.98%\tTest: 60.63%\n",
      "Run: 10\t Epoch: 790\t Loss: 0.2682\t Train: 83.18%\t Valid: 76.50%\tTest: 60.59%\n",
      "Run: 10\t Epoch: 800\t Loss: 0.2682\t Train: 83.54%\t Valid: 77.43%\tTest: 61.37%\n",
      "Run: 10\t Epoch: 810\t Loss: 0.2669\t Train: 83.26%\t Valid: 76.48%\tTest: 59.52%\n",
      "Run: 10\t Epoch: 820\t Loss: 0.2677\t Train: 83.62%\t Valid: 76.91%\tTest: 59.85%\n",
      "Run: 10\t Epoch: 830\t Loss: 0.2702\t Train: 83.63%\t Valid: 77.14%\tTest: 60.57%\n",
      "Run: 10\t Epoch: 840\t Loss: 0.2665\t Train: 83.53%\t Valid: 77.23%\tTest: 61.25%\n",
      "Run: 10\t Epoch: 850\t Loss: 0.2667\t Train: 83.61%\t Valid: 77.21%\tTest: 60.34%\n",
      "Run: 10\t Epoch: 860\t Loss: 0.2657\t Train: 83.67%\t Valid: 76.58%\tTest: 59.61%\n",
      "Run: 10\t Epoch: 870\t Loss: 0.2654\t Train: 83.71%\t Valid: 77.17%\tTest: 60.53%\n",
      "Run: 10\t Epoch: 880\t Loss: 0.2654\t Train: 83.82%\t Valid: 77.44%\tTest: 60.28%\n",
      "Run: 10\t Epoch: 890\t Loss: 0.2656\t Train: 83.89%\t Valid: 76.82%\tTest: 60.02%\n",
      "Run: 10\t Epoch: 900\t Loss: 0.2651\t Train: 83.92%\t Valid: 76.66%\tTest: 59.66%\n",
      "Run: 10\t Epoch: 910\t Loss: 0.2653\t Train: 83.95%\t Valid: 77.51%\tTest: 60.51%\n",
      "Run: 10\t Epoch: 920\t Loss: 0.2664\t Train: 83.54%\t Valid: 75.97%\tTest: 59.01%\n",
      "Run: 10\t Epoch: 930\t Loss: 0.2660\t Train: 84.03%\t Valid: 77.25%\tTest: 60.18%\n",
      "Run: 10\t Epoch: 940\t Loss: 0.2641\t Train: 84.14%\t Valid: 77.23%\tTest: 60.78%\n",
      "Run: 10\t Epoch: 950\t Loss: 0.2668\t Train: 84.21%\t Valid: 77.29%\tTest: 61.17%\n",
      "Run: 10\t Epoch: 960\t Loss: 0.2637\t Train: 83.94%\t Valid: 76.91%\tTest: 60.04%\n",
      "Run: 10\t Epoch: 970\t Loss: 0.2632\t Train: 84.19%\t Valid: 77.14%\tTest: 60.48%\n",
      "Run: 10\t Epoch: 980\t Loss: 0.2638\t Train: 84.28%\t Valid: 77.44%\tTest: 61.05%\n",
      "Run: 10\t Epoch: 990\t Loss: 0.2634\t Train: 84.07%\t Valid: 76.30%\tTest: 59.34%\n",
      "Run: 10\t Epoch: 1000\t Loss: 0.2625\t Train: 84.37%\t Valid: 77.53%\tTest: 60.78%\n",
      "Run 10:\n",
      "Highest Train: 84.37\n",
      "Highest Valid: 77.53\n",
      "  Final Train: 84.37\n",
      "   Final Test: 60.78\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute GCN normalization.\n",
    "adj_t = data.adj_t.set_diag()\n",
    "deg = adj_t.sum(dim=1).to(torch.float)\n",
    "deg_inv_sqrt = deg.pow(-0.5)\n",
    "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
    "data.adj_t = adj_t\n",
    "    \n",
    "evaluator = Evaluator(name='ogbn-proteins')\n",
    "logger = Logger(args.runs, args)\n",
    "best_test_score = 0\n",
    "\n",
    "\n",
    "print(\"Seed = \",seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    \n",
    "model = GCN(data.num_features, args.hidden_channels, 112, args.num_layers, args.dropout).to(device)\n",
    "\n",
    "for run in range(args.runs):        \n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    for epoch in range(1, 1 + args.epochs):\n",
    "\n",
    "        loss = train(model, data, train_idx, optimizer)\n",
    "\n",
    "        if epoch % args.eval_steps == 0:\n",
    "            result = test(model, data, split_idx, evaluator)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:                \n",
    "                train_rocauc, valid_rocauc, test_rocauc = result\n",
    "                print(f'Run: {run + 1:02d}\\t '\n",
    "                      f'Epoch: {epoch:02d}\\t '\n",
    "                      f'Loss: {loss:.4f}\\t '\n",
    "                      f'Train: {100 * train_rocauc:.2f}%\\t '\n",
    "                      f'Valid: {100 * valid_rocauc:.2f}%\\t'\n",
    "                      f'Test: {100 * test_rocauc:.2f}%')\n",
    "                if(test_rocauc > best_test_score):\n",
    "                    best_test_score = test_rocauc\n",
    "                    save_path = \"gcn.pth\"\n",
    "                    torch.save(model, save_path)\n",
    "                    print(\"Model saved.\")\n",
    "    logger.print_statistics(run)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.95441577668223\n"
     ]
    }
   ],
   "source": [
    "print(best_test_score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 hours\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model, optimizer\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
